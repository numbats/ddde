---
title: "ETC5521 Diving Deeper into Data Exploration: Assignment 1"
subtitle: "As per Monash's integrity rules, these solutions are not to be shared beyond this class."
author: "Prof. Di Cook"
date: "Jul 28, 2025"
quarto-required: ">=1.3.0"
format:
    unilur-html: 
        output-file: assign01.html
        css: "assignment.css"
        embed-resources: true
    unilur-html+solution:
        output-file: assign01sol.html
        css: "assignment.css"
        embed-resources: true
        show-solution: true
---

```{r}
#| echo: false
# Set up chunk for all slides
knitr::opts_chunk$set(
  fig.width = 6,
  fig.height = 4,
  fig.align = "center",
  out.width = "100%",
  code.line.numbers = FALSE,
  fig.retina = 4,
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  dev.args = list(pointsize = 11)
)
```

```{r}
#| echo: false
# Load libraries
library(tidyverse)
library(conflicted)

conflicts_prefer(dplyr::filter)
conflicts_prefer(dplyr::select)
conflicts_prefer(dplyr::slice)

# Plot options and themes
options(
  digits = 2,
  width = 60,
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  ggplot2.discrete.colour = c("#D55E00", "#0072B2", "#009E73", "#CC79A7", "#E69F00", "#56B4E9", "#F0E442"),
  ggplot2.discrete.fill = c("#D55E00", "#0072B2", "#009E73", "#CC79A7", "#E69F00", "#56B4E9", "#F0E442")
)

theme_set(theme_bw(base_size = 14) +
   theme(
     #aspect.ratio = 1,
     plot.background = element_rect(fill = 'transparent', colour = NA),
     plot.title.position = "plot",
     plot.title = element_text(size = 24),
     panel.background = element_rect(fill = 'transparent', colour = NA),
     legend.background = element_rect(fill = 'transparent', colour = NA),
     legend.key = element_rect(fill = 'transparent', colour = NA)
   )
)
```

## üéØ Goal

The assignment is designed to assess your knowledge of data wrangling and GitHub is at a level to be able to successfully follow the content of this class.  The assignment represents 15\% of your final grade for ETC5521. This is an **individual** assignment.

## üìå Guidelines 

1. Accept the GitHub Classroom Assignment provided in Moodle using a GitHub Classroom compatible web browser. This should generate a **private** GitHub repository that can be found at https://github.com/etc5521-2025. Your GitHub assignment 1 repo should contain the file `assign01.html`, `README.md`, `assign01-submission.qmd`, `assignment.css`, `etc5521-assignment1.Rproj` and `.gitignore`.

2. Answer each question in the `assign01-submission.qmd` in the repo.

3. For the final submission knit `assign01-submission.qmd` which will contain your answers. Make sure to **provide the link to the script of Generative AI conversation** you employed in arriving at your solution. *Note that marks are allocated for overall grammar and structure of your final report.*

4. Leave all of your files in your GitHub repo for marking. We will check your git commit history. You should have contributions to the repo with consistent commits over time. (*Note: nothing needs to be submitted to Moodle.*)

5. You are expected to **develop your solutions by yourself**, without discussing any details with other class members or other friends or contacts. You can ask for clarifications from the teaching team and we encourage you to attend consultations to get assistance as needed.  As a Monash student you are expected to adhere to **[Monash's academic integrity policy](https://publicpolicydms.monash.edu/Monash/documents/1935748).** and the details on use of Generative AI as detailed on this unit's Moodle assessment overview. Failure to adhere to this policy may result in a ZERO for this assignment, followed by an academic integrity breach report. The chief examiner reserves the right to question you about any part of your solution.

6. The primary sources for methods needed for this assignment are [R for Data Science (2e)](https://r4ds.hadley.nz) and (Text Mining with R: A Tidy Approach)[https://www.tidytextmining.com). 

7. We expect that this assignment takes about 10 hours of time to complete. You should work on this analysis steadily over the time period between release and due date. Spend a couple of hours soon after the assignment is released getting started, and several hours each of the following two weeks to refine your analysis. Your GitHub `commit` history should reflect this working pattern.

Deadlines: 

```{r create-deadline-table, echo = FALSE}
due_date <- c("11:45pm Mon Aug 4", "11:45pm Mon Aug 18")

turn_in <- c("Assignment 1 Repo on GitHub has been created by due date, and commit history shows steady effort on analysis.", "Final solutions available on repo.")

points <- c(3, 12)

library(tibble)
df_project <- tibble(`Due date` = due_date,
                     `Turn in` = turn_in,
                     `Points` = points)

```

```{r display-table, echo = FALSE}
library(kableExtra)
kable(df_project, table.attr = 'data-quarto-disable-processing="true"') |>
  column_spec(1, width = "3cm", bold = T, border_right = T) |>
  column_spec(2, width = "5cm", border_right = T) |>
  column_spec(3, width = "0.3cm") |>
  kable_styling(full_width = FALSE) 
```


## Marks


```{r create-marks-table, echo = FALSE}
part <- c("GitHub Repo", "Q1-Q4 each worth", "References and AI", "Formatting, spelling,  grammar, code and reproducibility")

points <- c(3, 3, -3, -3)

library(tibble)
df_marks <- tibble(`Part` = part,
                     `Points` = points)

```


```{r display-marks, echo = FALSE}
library(kableExtra)
kable(df_marks, table.attr = 'data-quarto-disable-processing="true"') |>
  kable_styling(full_width = FALSE) |>
  column_spec(1, width = "5cm", border_right = T) |>
  column_spec(2, width = "0.3cm") 

```

Appropriate use of GitHub is an important collaborative analysis skill, and demonstrating this counts towards marks. 

Spelling and grammar mistakes and lack of nice formatting, detract from the score because it makes it harder to mark, and harder to read. It is expected that we can reproduce your report, so all code needs to be included, and the code needs to be readable. 

Using GAI well is an emerging skill. Inadequate use or over-use without fully processing the responses detracts from a data analysis. 

## üõ†Ô∏è Exercises

The data to use is available in the [orcas](https://jadeynryan.github.io/orcas/) R package on GitHub. The package can be installed using

`remotes::install_github("jadeynryan/orcas")`

to access the data. The descriptions of the variables can be found [here](https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-10-15/readme.md).

Why are we interested in orca encounters? Whale watching is a major tourism business in many parts of the world. Monitoring the population of whales is important for the sustainability of many businesses and for the health of the planet. Beyond this, orcas are cool! They are intelligent, social, beautiful and one of the top predators in the ocean. 

An interesting fact is that orcas routinely helped whalers hunt baleen whales in Two Fold Bay near Eden, Australia, in the 1800s. They would herd the whales into the bay for the whalers. When the whale was killed, the whalers rewarded them with the whales' tongue and lips. Actually, this is quite gruesome üò≥ üò¨. 

::: unilur-solution

```{r}
library(orcas)
glimpse(cwr_tidy)
```

:::

## Question 1 

Summarise the temporal patterns in this data. For example, 

- what is the time frame of the data collection?
- are there any seasonal patterns in the measurements?
- what is the usual length of the encounters?

::: unilur-solution

#### Overall time

Examining the year variable to get an overview of the timing of measurements. 

```{r}
#| label: year-summary
#| code-fold: true
yr_smry <- cwr_tidy |> count(year) 
yr_smry
```

We can see that there are a similar number of encounters every year between `r min(yr_smry$year, na.rm=TRUE)` and `r max(yr_smry$year, na.rm=TRUE)`. We also notice that there are some missing values on `year`, and these observations might need removing for later analysis.

#### Seasonality

Next extract the month and examine if there are any seasonal trends. 

```{r}
#| label: fig-month-smry
#| fig-width: 12
#| fig-height: 8
#| code-fold: true
library(lubridate)
cwr_tidy <- cwr_tidy |>
  mutate(month = month(date, label=TRUE))
cwr_tidy |>
  filter(!is.na(month)) |>
  ggplot(aes(x=month)) +
  geom_bar() + 
  facet_wrap(~year, ncol=4) +
  xlab("")
```

@fig-month-smry doesn't show any strong seasonal patterns. There is some hint of more encounters Jun-Oct, and less in Nov-Dec, but the variation from month to month is large. This suggests that orcas are around this area all year long. Note: If you don't facet, you will find that the number per month is reasonably constant, for Jan-Oct, and less in Nov-Dec. However, this is something that you should be careful doing. You should always examine on strata and smaller time scale, which reveals the month to month variability for this data.

#### Duration of encounters

```{r}
#| label: fig-duration
#| code-fold: true
#| out-width: 60%
cwr_tidy <- cwr_tidy |>
  mutate(duration_num = str_remove_all(duration, "\\([^)]*\\)")) |>
  mutate(duration_num = as.numeric(str_remove_all(duration_num, "[A-Za-z\\-~\\s]")))
ggplot(cwr_tidy, aes(duration_num)) +
  geom_histogram() +
  xlab("Duration (sec)")
```

From @fig-duration the distribution is bimodal. We can see that most encounters were around 1.5 hours, which is the mode of the first peak. The second mode has a peak around 9-10 hours!

:::

## Question 2

Summarise the spatial patterns, for example

- where are these encounters happening?
- are the boats following the whales based on the tracks of the encounters?

::: unilur-solution

#### Spatial location

Plotting the longitude and latitude, particularly overlaid on a map, can help work out where the encounters are happening.

```{r}
#| code-fold: true
library(leaflet)
cwr_tidy |>
  leaflet() |>
  addTiles() |>
  addCircleMarkers(
    radius=1, 
    opacity = 0.5, 
    color = "hotpink", 
    label = ~date,
    lat = ~begin_latitude, lng = ~begin_longitude) 
```

These encounters are all in the Puget Sound area of northwest USA, and western Canada.

#### Encounter tracks

```{r}
#| label: fig-enc-l-yr
#| fig-width: 12
#| fig-height: 7
#| fig-cap: "Encounters with segments marking start and end, and circles indicating the start. Most are short. There is little difference between years."
#| code-fold: true
library(ggthemes)
cwr_tidy |>
  filter(!is.na(year)) |>
  ggplot() +
    geom_segment(aes(x=begin_longitude, 
                     y=begin_latitude,
                     xend=end_longitude,
                     yend=end_latitude)) +
    geom_point(aes(x=begin_longitude, 
                     y=begin_latitude), shape = 1) +
    facet_wrap(~month, ncol=4) +
    coord_map() +
    theme_map() +
    theme(panel.border = element_rect(colour = "black", fill = NA))
```

Most of the encounters are short distance, seen by the small or non-existent lengths, as seen in @fig-enc-l-yr.

```{r}
#| label: fig-enc-l-mth
#| fig-width: 12
#| fig-height: 9
#| fig-cap: "Encounters with segments marking start and end, and circles indicating the start. Most are short. September is when there seems to be more varied locations of encounters."
#| code-fold: true
library(ggthemes)
cwr_tidy |>
  filter(!is.na(month)) |>
  ggplot() +
    geom_segment(aes(x=begin_longitude, 
                     y=begin_latitude,
                     xend=end_longitude,
                     yend=end_latitude)) +
    geom_point(aes(x=begin_longitude, 
                     y=begin_latitude), shape = 1) +
    facet_wrap(~year, ncol=4) +
    coord_map() +
    theme_map() +
    theme(panel.border = element_rect(colour = "black", fill = NA))
```

:::

## Question 3

Summarise the encounters by the vessels and observers. Are they the same whales that are frequently seen?

- Are there some especially frequent observers or active vessels?
- Are the long encounters made by special vessels?
- Do some vessels make multiple encounters?

::: unilur-solution

```{r}
#| code-fold: true
vessel_cnt <- cwr_tidy |>
  count(vessel, sort=TRUE) 
vessel_cnt |>
  filter(n > 9)
```

There are two especially active vessels `r vessel_cnt$vessel[1]` and `r vessel_cnt$vessel[2]`.

```{r}
#| code-fold: true
cwr_tidy |> 
  filter(duration_num > 30000) |>
  count(vessel, sort=TRUE) 
```

It is the same two vessels also are the ones that made long encounters. Some error in the data in that some vessel names and observer names are mashed together.

```{r}
#| code-fold: true
cwr_tidy |> 
  count(observers, sort=TRUE) 
```

XXX Maybe need to split names, and recount. Are these always associated with the same boats?

```{r}
#| code-fold: true
cwr_tidy |> 
  count(pods_or_ecotype, sort=TRUE) 
```

XXX This information is a bit inconsistent. It needs more processing.

:::

## Question 4

Each encounter has a text description. Summarise the common words used for the encounters.

::: unilur-solution

```{r}
#| label: fig-text
#| fig-cap: "Word cloud of top 100 words."
#| code-fold: true
library(tidytext)
library(ggwordcloud)
cwr_text <- cwr_tidy |>
  unnest_tokens(word, encounter_summary) |>
  select(word) |>
  anti_join(stop_words)

cwr_text_top <- cwr_text |> 
  filter(!is.na(word)) |>
  count(word, sort=TRUE) |>
  slice_head(n=100) 
cwr_text_top |>
  ggplot() +
  geom_text_wordcloud_area(aes(label = word, size=n)) +
  scale_size_area(max_size = 30) + 
  theme_minimal()

```

The text descriptions are not especially interesting word-wise (@fig-text), because the top words are `r paste(cwr_text_top$word[1:10], collapse=", ")`.  A few of the top 100 words are interesting, "dave", "mike" and "joe" feature. The words "calf", "seal", "fish" suggest the nature of the encounters are more interesting, and "milling", "foraging", suggest activities of the whales. More processing of the text could be done to remove uninteresting words, and also look for two or three words used together, n-grams. 

:::


## Resources

In this part, you should cite major resources used, including R packages, and actively discuss how generative AI helped with your answers to the assignment questions, and where or how it was mistaken or misleading. 

You need to provide links to the full script of your conversations generative AI tools. You should not use a paid service, as the freely available systems will sufficiently helpful.

For example, the `citation()` function in R can give R package details:

Wickham H, Averick M, Bryan J, Chang W, McGowan LD, Fran√ßois R,
  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL,
  Miller E, Bache SM, M√ºller K, Ooms J, Robinson D, Seidel DP, Spinu
  V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019).
  "Welcome to the tidyverse." _Journal of Open Source Software_,
  *4*(43), 1686. [doi:10.21105/joss.01686](https://doi.org/10.21105/joss.01686)..
  
  
The links to my use of ChatGPT for help on this assignment are:

- [https://chatgpt.com/share/68857a22-debc-8001-bfcc-10552befba4c](https://chatgpt.com/share/68857a22-debc-8001-bfcc-10552befba4c) helped to construct the regex to process the duration variable.

## Rubric

To help you complete in your report, below is a rubric to guide you  to what we are expecting:

```{r, echo = FALSE, message=FALSE}
rubric <- readr::read_tsv("rubric_assign01.tsv") 

rubric |>
  replace_na(list(`Very good (D)` = "", `Good (C)` = "")) |>
  kable() |>
  kable_styling(full_width = FALSE) |> 
  column_spec(1, bold = TRUE, width = "10em", border_right = TRUE) |> 
  column_spec(2, width = "30em") |> 
  column_spec(3, width = "20em") |> 
  column_spec(4, width = "20em") |> 
  column_spec(5, width = "20em") |> 
  column_spec(6, width = "20em") 
```

