[
  {
    "objectID": "week9/index.html",
    "href": "week9/index.html",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "",
    "text": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R"
  },
  {
    "objectID": "week9/index.html#main-reference",
    "href": "week9/index.html#main-reference",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "",
    "text": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R"
  },
  {
    "objectID": "week9/index.html#what-you-will-learn-this-week",
    "href": "week9/index.html#what-you-will-learn-this-week",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nWhat is temporal data?\nWhat is exploratory temporal data analysis?\nUsing temporal objects in R: tsibble\nData wrangling: aggregation, creating temporal components, missing values\nPlotting conventions: connect the dots; aspect ratio, landscape or portrait\nCalendar plots: arranging daily records into a calendar format\nVisual inference for temporal data\ntignostics: cognostics for temporal data\nInteractive graphics for temporal data\nExploring longitudinal data, with the brolgar package"
  },
  {
    "objectID": "week9/index.html#lecture-slides",
    "href": "week9/index.html#lecture-slides",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week9/index.html#tutorial-instructions",
    "href": "week9/index.html#tutorial-instructions",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week9/index.html#assignments",
    "href": "week9/index.html#assignments",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Assignments",
    "text": "Assignments\n\nExercises 3 is due on Monday 22 September.\nQuiz 8 is due on Thursday 25 September.\nQuiz 9 is due on Thursday 09 October.\nProject Part 1 is due on Monday 13 October.\nQuiz 10 is due on Thursday 16 October.\nQuiz 11 is due on Thursday 23 October.\nProject Part 2 is due on Monday 03 November."
  },
  {
    "objectID": "week7/index.html",
    "href": "week7/index.html",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "",
    "text": "Wilke (2019) Ch 9, 10.2-4, 11.2"
  },
  {
    "objectID": "week7/index.html#main-reference",
    "href": "week7/index.html#main-reference",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "",
    "text": "Wilke (2019) Ch 9, 10.2-4, 11.2"
  },
  {
    "objectID": "week7/index.html#what-you-will-learn-this-week",
    "href": "week7/index.html#what-you-will-learn-this-week",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nWhy making comparisons is important\nHow to decide on what comparison to make\nComparing between strata, relative to a baseline\nComparing the same data, in many ways\nUsing normalising to compare different distributions\nInference using bootstrap and lineups"
  },
  {
    "objectID": "week7/index.html#lecture-slides",
    "href": "week7/index.html#lecture-slides",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week7/index.html#tutorial-instructions",
    "href": "week7/index.html#tutorial-instructions",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week7/index.html#assignments",
    "href": "week7/index.html#assignments",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 6 is due on Thursday 11 September.\nQuiz 7 is due on Thursday 18 September.\nExercises 3 is due on Monday 22 September.\nQuiz 8 is due on Thursday 25 September."
  },
  {
    "objectID": "week5/index.html",
    "href": "week5/index.html",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "",
    "text": "Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions"
  },
  {
    "objectID": "week5/index.html#main-reference",
    "href": "week5/index.html#main-reference",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "",
    "text": "Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions"
  },
  {
    "objectID": "week5/index.html#what-you-will-learn-this-week",
    "href": "week5/index.html#what-you-will-learn-this-week",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nNumeric and visual summaries for a single variable\nCommon features to discover\nTools for inference for a single variable\nImputing missings on a single variable"
  },
  {
    "objectID": "week5/index.html#lecture-slides",
    "href": "week5/index.html#lecture-slides",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week5/index.html#tutorial-instructions",
    "href": "week5/index.html#tutorial-instructions",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week5/index.html#assignments",
    "href": "week5/index.html#assignments",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 4 is due on Thursday 28 August.\nExercises 2 is due on Monday 01 September.\nQuiz 5 is due on Thursday 04 September.\nQuiz 6 is due on Thursday 11 September."
  },
  {
    "objectID": "week3/index.html",
    "href": "week3/index.html",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "",
    "text": "The initial examination of data"
  },
  {
    "objectID": "week3/index.html#main-reference",
    "href": "week3/index.html#main-reference",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "",
    "text": "The initial examination of data"
  },
  {
    "objectID": "week3/index.html#what-you-will-learn-this-week",
    "href": "week3/index.html#what-you-will-learn-this-week",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nRole of IDA\nTechniques for\n\ndata screening\ndata cleaning\nimputation\nvalidation\n\nChecking assumptions for hypothesis testing and fitting linear models"
  },
  {
    "objectID": "week3/index.html#lecture-slides",
    "href": "week3/index.html#lecture-slides",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week3/index.html#tutorial-instructions",
    "href": "week3/index.html#tutorial-instructions",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week3/index.html#assignments",
    "href": "week3/index.html#assignments",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week3/index.html#assignments-1",
    "href": "week3/index.html#assignments-1",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 2 is due on Thursday 14 August.\nExercises 1 is due on Monday 18 August.\nQuiz 3 is due on Thursday 21 August.\nQuiz 4 is due on Thursday 28 August."
  },
  {
    "objectID": "week2/tutorial.html",
    "href": "week2/tutorial.html",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week2/tutorial.html#objectives",
    "href": "week2/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week2/tutorial.html#preparation",
    "href": "week2/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 2",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is EDA Case Study: Bay area blues. It is authored by Hadley Wickham, Deborah F. Swayne, and David Poole. It appeared in the book “Beautiful Data” edited by Jeff Hammerbacher and Toby Segaran. Not all the chapters in the book are good examples of data analysis, though.\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"forcats\", \"patchwork\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week2/tutorial.html#exercises",
    "href": "week2/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 2",
    "section": "📥 Exercises",
    "text": "📥 Exercises\nPoint your web browser to the github site for the analysis, https://github.com/hadley/sfhousing. The main data file is house-sales.csv. Read this data into your R session. (🛑 ARE YOU USING A PROJECT FOR THIS UNIT? IF NOT, STOP and OPEN IT NOW.)\nYou can read the data in directly from the web site using this code:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(forcats)\nsales &lt;- read_csv(\"https://raw.githubusercontent.com/hadley/sfhousing/master/house-sales.csv\")\n\n\n1. What’s in the data?\n\nIs the data in tidy form?\nOf the variables in the data, which are\n\nnumeric?\ncategorical?\ntemporal?\n\nWhat would be an appropriate plot to make to examine the\n\nnumeric variables?\ncategorical variables?\na categorical and numeric variable?\na temporal variable and a numeric variable?\n\n\n\n\n2. Time series plots\nReproduce the time series plots of weekly average price and volume of sales.\n\n\n\n\n\n\n\n\n\n\n\n3. Correlation between series\nIt looks like volume goes down as price goes up. There is a better plot to make to examine this. What is it? Make the plot. After making the plot, report what you learn about the apparent correlation.\n\n\n4. Geographic differences\nThink about potential plots you might make for examining differences by geographic region (as measured by zip, county or city). Make a plot, and report what you learn.\n\n\n5. The Rich Get Richer and the Poor Get Poorer\nIn the section “The Rich Get Richer and the Poor Get Poorer” there are some interesting transformations of the data, and unusual types of plots. Explain why looking at proportional change in value refines the view of price movement in different higher vs lower priced properties.\n\n\n6. Anything surprising?\nWere there any findings that surprised the authors? Or would surprise you?\n\n\n7. Additional resources\nSome of the findings were compared against information gathered from external sources. Can you point to an example of this, and how the other information was used to support or question the finding?"
  },
  {
    "objectID": "week2/tutorial.html#finishing-up",
    "href": "week2/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 2",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week2/tutorialsol.html",
    "href": "week2/tutorialsol.html",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week2/tutorialsol.html#objectives",
    "href": "week2/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week2/tutorialsol.html#preparation",
    "href": "week2/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 2",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is EDA Case Study: Bay area blues. It is authored by Hadley Wickham, Deborah F. Swayne, and David Poole. It appeared in the book “Beautiful Data” edited by Jeff Hammerbacher and Toby Segaran. Not all the chapters in the book are good examples of data analysis, though.\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"forcats\", \"patchwork\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week2/tutorialsol.html#exercises",
    "href": "week2/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 2",
    "section": "📥 Exercises",
    "text": "📥 Exercises\nPoint your web browser to the github site for the analysis, https://github.com/hadley/sfhousing. The main data file is house-sales.csv. Read this data into your R session. (🛑 ARE YOU USING A PROJECT FOR THIS UNIT? IF NOT, STOP and OPEN IT NOW.)\nYou can read the data in directly from the web site using this code:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(forcats)\nsales &lt;- read_csv(\"https://raw.githubusercontent.com/hadley/sfhousing/master/house-sales.csv\")\n\n\n1. What’s in the data?\n\nIs the data in tidy form?\nOf the variables in the data, which are\n\nnumeric?\ncategorical?\ntemporal?\n\nWhat would be an appropriate plot to make to examine the\n\nnumeric variables?\ncategorical variables?\na categorical and numeric variable?\na temporal variable and a numeric variable?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nYes\n\nprice, br, lsqft, bsqft\ncounty, city, zip, street\nyear, date, datesold\n\n\nscatterplots\nbar charts, pie charts, mosaic\nfacet by the categorical variable. could be boxplots, or density plots, or facetted scatterplots to look at multiple numeric variables\n\ntime series plot, connect lines to indicate time, maybe need to aggregate over time to get one value per time point\n\n\n\n\n\n\n\n\n2. Time series plots\nReproduce the time series plots of weekly average price and volume of sales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNote, stacking time series plots helps compare the series relative to the time point.\n\nsales_weekly &lt;- sales |&gt;\n  group_by(date) |&gt;\n  summarise(av_price = mean(price, na.rm=TRUE),\n            volume = n())\np1 &lt;- ggplot(sales_weekly, aes(x=date,\n                               y=av_price)) +\n  geom_line() +\n  scale_y_continuous(\"Average price (millions)\", \n              breaks = seq(500000, 800000, 50000), \n              labels = c(\"0.50\", \"0.55\", \"0.60\", \"0.65\",\n                         \"0.70\", \"0.75\", \"0.80\")) +\n  scale_x_date(\"\", date_breaks = \"1 years\", \n               minor_breaks = NULL, \n               date_labels = \"%Y\") +\n  theme(aspect.ratio = 0.5)\np2 &lt;- ggplot(sales_weekly, aes(x=date, y=volume)) + geom_line() +\n  scale_y_continuous(\"Number of sales\", \n              breaks = seq(500,3000,500), \n              labels = c(\"500\", \"1,000\", \"1,500\", \"2,000\",\n                         \"2,500\", \"3,000\")) +\n  scale_x_date(\"\", date_breaks = \"1 years\", \n               minor_breaks = NULL, \n               date_labels = \"%Y\") +\n  theme(aspect.ratio = 0.5)\np1 + p2 + plot_layout(ncol=1)\n\n\n\n\n\n\n\n3. Correlation between series\nIt looks like volume goes down as price goes up. There is a better plot to make to examine this. What is it? Make the plot. After making the plot, report what you learn about the apparent correlation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nggplot(sales_weekly, aes(x=av_price, y=volume)) +\n  geom_point() +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\nAny correlation is very weak, and negative.\n\n\n\n\n\n\n4. Geographic differences\nThink about potential plots you might make for examining differences by geographic region (as measured by zip, county or city). Make a plot, and report what you learn.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nggplot(sales, \n       aes(x = fct_reorder(county, \n                  price, na.rm=TRUE), \n           y = price)) +\n         geom_boxplot() + \n  scale_y_log10() +\n  xlab(\"\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nMarin County has the highest prices on average, and San Joaquin the lowest. The lowest priced house was sold in Sonoma County. The highest priced properties and lowest priced are pretty similar from one county to another - that is, the variability within county is large.\n\n\n\n\n\n\n5. The Rich Get Richer and the Poor Get Poorer\nIn the section “The Rich Get Richer and the Poor Get Poorer” there are some interesting transformations of the data, and unusual types of plots. Explain why looking at proportional change in value refines the view of price movement in different higher vs lower priced properties.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe transformation makes changes relative to the initial average price at the start of the time period. All curves produced will start from the same point. This means that we only need to compare the end points of each line, saving us from calculating differences between lines relative to the difference at the beginning.\n\n\n\n\n\n\n6. Anything surprising?\nWere there any findings that surprised the authors? Or would surprise you?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nI found it interesting that Mountain View had no decline in housing prices. This city has the headquarters of many of the world’s largest technology companies are in the city, including Google, Mozilla Foundation, Symantec, and Intuit.\n\n\n\n\n\n\n7. Additional resources\nSome of the findings were compared against information gathered from external sources. Can you point to an example of this, and how the other information was used to support or question the finding?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAll of this is consistent with what we have learned about subprime mortgages since the housing bust hit the headlines.\nSubprime mortgages were offered on little collateral which meant they were quite risky, and they tended to be on the lower end of the housing market. This information was in all the news headlines at the time, and the analysis that these authors have done was checked against the common reporting at the time. The data was consistent with these reports."
  },
  {
    "objectID": "week2/tutorialsol.html#finishing-up",
    "href": "week2/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 2",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week2/slides.html#birth-of-eda",
    "href": "week2/slides.html#birth-of-eda",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Birth of EDA",
    "text": "Birth of EDA\n\nThe field of exploratory data analysis came of age when this book appeared in 1977.\n\nTukey held that too much emphasis in statistics was placed on statistical hypothesis testing (confirmatory data analysis); more emphasis needed to be placed on using data to suggest hypotheses to test."
  },
  {
    "objectID": "week2/slides.html#john-w.-tukey",
    "href": "week2/slides.html#john-w.-tukey",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "John W. Tukey",
    "text": "John W. Tukey\n\n\n\n\n\n\n Image source: wikimedia.org\n\n\nBorn in 1915, in New Bedford, Massachusetts.\nMum was a private tutor who home-schooled John. Dad was a Latin teacher.\nBA and MSc in Chemistry, and PhD in Mathematics\nAwarded the National Medal of Science in 1973, by President Nixon\nBy some reports, his home-schooling was unorthodox and contributed to his thinking and working differently."
  },
  {
    "objectID": "week2/slides.html#taking-a-glimpse-back-in-time",
    "href": "week2/slides.html#taking-a-glimpse-back-in-time",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Taking a glimpse back in time",
    "text": "Taking a glimpse back in time\nis possible with the American Statistical Association video lending library.\n We’re going to watch John Tukey talking about exploring high-dimensional data with an amazing new computer in 1973, four years before the EDA book.\n\nLook out for these things:\nTukey’s expertise is described as for trial and error learning and the computing equipment.\n\n\nFirst 4.25 minutes"
  },
  {
    "objectID": "week2/slides.html#setting-the-frame-of-mind",
    "href": "week2/slides.html#setting-the-frame-of-mind",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Setting the frame of mind",
    "text": "Setting the frame of mind\nExcerpt from the introduction\n\nThis book is based on an important principle.\n It is important to understand what you CAN DO before you learn to measure how WELL you seem to have DONE it.\n Learning first what you can do will help you to work more easily and effectively.\n This book is about exploratory data analysis, about looking at data to see what it seems to say. It concentrates on simple arithmetic and easy-to-draw pictures. It regards whatever appearances we have recognized as partial descriptions, and tries to look beneath them for new insights. Its concern is with appearance, not with confirmation.\n Examples, NOT case histories\n The book does not exist to make the case that exploratory data analysis is useful. Rather it exists to expose its readers and users to a considerable variety of techniques for looking more effectively at one’s data. The examples are not intended to be complete case histories. Rather they should isolated techniques in action on real data. The emphasis is on general techniques, rather than specific problems. \nA basic problem about any body of data is to make it more easily and effectively handleable by minds – our minds, her mind, his mind. To this general end:\n\nanything that make a simpler description possible makes the description more easily handleable.\nanything that looks below the previously described surface makes the description more effective.\n\n\nSo we shall always be glad (a) to simplify description and (b) to describe one layer deeper. In particular,\n\nto be able to say that we looked one layer deeper, and found nothing, is a definite step forward – though not as far as to be able to say that we looked deeper and found thus-and-such.\nto be able to say that “if we change our point of view in the following way … things are simpler” is always a gain–though not quite so much as to be able to say “if we don’t bother to change out point of view (some other) things are equally simple.”\n\n …\n Consistent with this view, we believe, is a clear demand that pictures based on exploration of data should force their messages upon us. Pictures that emphasize what we already know–“security blankets” to reassure us–are frequently not worth the space they take. Pictures that have to be gone over with a reading glass to see the main point are wasteful of time and inadequate of effect. The greatest value of a picture is when it forces us to notice what we never expected to see.\n\n\nConfirmation\n\n\nThe principles and procedures of what we call confirmatory data analysis are both widely used and one of the great intellectual products of our century. In their simplest form, these principles and procedures look at a sample–and at what that sample has told us about the population from which it came–and assess the precision with which our inference from sample to population is made. We can no longer get along without confirmatory data analysis. But we need not start with it.\n\nThe best way to understand what CAN be done is not longer–if it ever was–to ask what things could, in the current state of our skill techniques, be confirmed (positively or negatively). Even more understanding is lost if we consider each thing we can do to data only in terms of some set of very restrictive assumptions under which that thing is best possible–assumptions we know we CANNOT check in practice.\n\nExploration AND confirmation\n\nOnce upon a time, statisticians only explored. Then they learned to confirm exactly–to confirm a few things exactly, each under very specific circumstances. As they emphasized exact confirmation, their techniques inevitably became less flexible. The connection of the most used techniques with past insights was weakened. Anything to which confirmatory procedure was not explicitly attached was decried as “mere descriptive statistics”, no matter how much we learned from it.\n\nToday, the flexibility of (approximate) confirmation by the jacknife makes it relatively easy to ask, for almost any clearly specified exploration, “How far is it confirmed?”\n\nToday, exploratory and confirmatory can–and should–proceed side by side. This book, of course, considers only exploratory techniques, leaving confirmatory techniques to other accounts.\n\n\n About the problems \n\n The teacher needs to be careful about assigning problems. Not too many, please. They are likely to take longer than you think. The number supplied is to accommodate diversity of interest, not to keep everybody busy.\n Besides the length of our problems, both teacher and student need to realise that many problems do not have a single “right answer”. There can be many ways to approach a body of data. Not all are equally good. For some bodies of data this may be clear, but for others we may not be able to tell from a single body of data which approach is preferred. Even several bodies of data about very similar situations may not be enough to show which approach should be preferred. Accordingly, it will often be quite reasonable for different analysts to reach somewhat different analyses.\n Yet more–to unlock the analysis of a body of day, to find the good way to approach it, may require a key, whose finding is a creative act. Not everyone can be expected to create the key to any one situation. And to continue to paraphrase Barnum, no one can be expected to create a key to each situation he or she meets.\n To learn about data analysis, it is right that each of us try many things that do not work–that we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed–at least until we were told about it–an opportunity to learn more. Each teacher needs to recognize this in grading and commenting on problems.\n\n\n Precision\n\nThe teacher who heeds these words and admits that there need be no one correct approach may, I regret to contemplate, still want whatever is done to be digit perfect. (Under such a requirement, the write should still be able to pass the course, but it is not clear whether she would get an “A”.) One does, from time to time, have to produce digit-perfect, carefully checked results, but forgiving techniques that are not too distributed by unusual data are also, usually, little disturbed by SMALL arithmetic errors. The techniques we discuss here have been chosen to be forgiving. It is hoped, then, that small arithmetic errors will take little off the problem’s grades, leaving severe penalties for larger errors, either of arithmetic or concept."
  },
  {
    "objectID": "week2/slides.html#outline",
    "href": "week2/slides.html#outline",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Outline",
    "text": "Outline\n\n\n\nScratching down numbers\nSchematic summary\nEasy re-expression\nEffective comparison\nPlots of relationship\nStraightening out plots (using three points)\nSmoothing sequences\nParallel and wandering schematic plots\nDelineations of batches of points\nUsing two-way analyses\n\n\n\n\n\nMaking two-way analyses\nAdvanced fits\nThree way fits\nLooking in two or more ways at batched of points\nCounted fractions\nBetter smoothing\nCounts in bin after bin\nProduct-ratio plots\nShapes of distributions\nMathematical distributions"
  },
  {
    "objectID": "week2/slides.html#looking-at-numbers-with-tukey",
    "href": "week2/slides.html#looking-at-numbers-with-tukey",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Looking at numbers with Tukey",
    "text": "Looking at numbers with Tukey"
  },
  {
    "objectID": "week2/slides.html#scratching-down-numbers",
    "href": "week2/slides.html#scratching-down-numbers",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scratching down numbers",
    "text": "Scratching down numbers\n\n\nPrices of Chevrolet in the local used car newspaper ads of 1968.\n\noptions(width=20)\nchevrolets &lt;- tibble(\n  prices = c(250, 150, 795, 895, 695, \n               1699, 1499, 1099, 1693, 1166,\n               688, 1333, 895, 1775, 895,\n               1895, 795))\n#chevrolets$prices\n\n\nStem-and-leaf plot: still seen in introductory statistics texts"
  },
  {
    "objectID": "week2/slides.html#section-1",
    "href": "week2/slides.html#section-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "First stem-and-leaf, first digit on stem, second digit on leaf\n\n\nOrder any leaves which need it, eg stem 6\n\n\n\nA benefit is that the numbers can be read off the plot, but the focus is still on the pattern. Also quantiles like the median, can be computed easily."
  },
  {
    "objectID": "week2/slides.html#section-2",
    "href": "week2/slides.html#section-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Shrink the stem\n\n\nShrink the stem more"
  },
  {
    "objectID": "week2/slides.html#and-in-r",
    "href": "week2/slides.html#and-in-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "And, in R …",
    "text": "And, in R …\n\nchevrolets$prices\n\n [1]  250  150  795\n [4]  895  695 1699\n [7] 1499 1099 1693\n[10] 1166  688 1333\n[13]  895 1775  895\n[16] 1895  795\n\nstem(chevrolets$prices)\n\n\n  The decimal point is 3 digit(s) to the right of the |\n\n  0 | 23\n  0 | 7788999\n  1 | 123\n  1 | 57789"
  },
  {
    "objectID": "week2/slides.html#remember-the-tips-data",
    "href": "week2/slides.html#remember-the-tips-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "🔖 Remember the tips data",
    "text": "🔖 Remember the tips data\n\n\n [1] 1.01 1.66 3.50 3.31 3.61 4.71 2.00 3.12 1.96 3.23 1.71 5.00 1.57 3.00 3.02\n[16] 3.92 1.67 3.71 3.50 3.35 4.08 2.75 2.23 7.58 3.18 2.34 2.00 2.00 4.30 3.00\n[31] 1.45 2.50 3.00 2.45 3.27 3.60 2.00 3.07 2.31 5.00 2.24 2.54 3.06 1.32 5.60\n[46] 3.00 5.00 6.00 2.05 3.00\n\n\n\nstem(tips$tip, scale=0.5, width=120)\n\n\n  The decimal point is at the |\n\n   1 | 000001233334445555555555556666667777788889\n   2 | 000000000000000000000000000000000000000001122222223333555555555555556666677788899\n   3 | 00000000000000000000000011111112222222333344445555555555555666778889\n   4 | 0000000000001112233335777\n   5 | 00000000001122226799\n   6 | 05577\n   7 | 6\n   8 | \n   9 | 0\n  10 | 0"
  },
  {
    "objectID": "week2/slides.html#refining-the-size",
    "href": "week2/slides.html#refining-the-size",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Refining the size",
    "text": "Refining the size\n\n\nFive digits per stem\n\n\n\nWhat is the number in parentheses? And why might this be useful?\n\n\n\nTwo digits per stem\n\n\n\n\n\nstem(tips$tip, scale=2)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n   10 | 0000107\n   12 | 55526\n   14 | 44578000000000678\n   16 | 1346781356\n   18 | 032678\n   20 | 00000000000000000000000000000000011233598\n   22 | 0033440114\n   24 | 5700000000002456\n   26 | 01412455\n   28 | 382\n   30 | 00000000000000000000000267891245688\n   32 | 133557159\n   34 | 0188800000000015\n   36 | 0181566\n   38 | 2\n   40 | 0000000000006889\n   42 | 09004\n   44 | 0\n   46 | 713\n   48 | \n   50 | 000000000074567\n   52 | 0\n   54 | \n   56 | 05\n   58 | 52\n   60 | 0\n   62 | \n   64 | 00\n   66 | 03\n   68 | \n   70 | \n   72 | \n   74 | 8\n   76 | \n   78 | \n   80 | \n   82 | \n   84 | \n   86 | \n   88 | \n   90 | 0\n   92 | \n   94 | \n   96 | \n   98 | \n  100 | 0\n\n\n\n\nWhy no number in parentheses?\n\n\n\nmedian(tips$tip)\n\n[1] 2.9"
  },
  {
    "objectID": "week2/slides.html#summary",
    "href": "week2/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\nStem-and-leaf plots are similar information to the histogram.\nGenerally it is possible to also read off the numbers, and to then easily calculate median or Q1 or Q3.\nIt’s great for small data sets, when you only have pencil and paper.\nAlternatives are a histogram, (jittered) dotplot, density plot, box plot, violin plot, letter value plot."
  },
  {
    "objectID": "week2/slides.html#a-different-style-of-number-scratching",
    "href": "week2/slides.html#a-different-style-of-number-scratching",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "a different style of number scratching",
    "text": "a different style of number scratching\nfor categorical variables\n\n\nWe know about\n\nbut its too easy to\n\nmake a mistake\n\nIs this easier?\n\n\nor harder"
  },
  {
    "objectID": "week2/slides.html#count-this-data-using-the-squares-approach.",
    "href": "week2/slides.html#count-this-data-using-the-squares-approach.",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Count this data using the squares approach.",
    "text": "Count this data using the squares approach.\n\n\n\n\n [1] \"F\" \"M\" \"M\" \"M\" \"F\" \"M\"\n [7] \"M\" \"M\" \"M\" \"M\" \"M\" \"F\"\n[13] \"M\" \"M\" \"F\" \"M\" \"F\" \"M\"\n[19] \"F\" \"M\" \"M\" \"F\" \"F\" \"M\"\n[25] \"M\" \"M\" \"M\" \"M\" \"M\" \"F\"\n[31] \"M\" \"M\" \"F\" \"F\" \"M\" \"M\"\n[37] \"M\" \"F\" \"M\" \"M\" \"M\" \"M\"\n[43] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n[49] \"M\" \"M\" \"M\" \"F\" \"F\" \"M\"\n[55] \"M\" \"M\" \"M\" \"F\" \"M\" \"M\"\n[61] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n[67] \"F\" \"F\" \"M\" \"M\" \"M\" \"F\""
  },
  {
    "objectID": "week2/slides.html#what-does-it-mean-to-feel-what-the-data-are-like",
    "href": "week2/slides.html#what-does-it-mean-to-feel-what-the-data-are-like",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What does it mean to “feel what the data are like?”",
    "text": "What does it mean to “feel what the data are like?”"
  },
  {
    "objectID": "week2/slides.html#section-3",
    "href": "week2/slides.html#section-3",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "This is a stem and leaf of the height of the highest peak in each of the 50 US states.\n\nThe states roughly fall into three groups.\n\nIt’s not really surprising, but we can imagine this grouping. Alaska is in a group of its own, with a much higher high peak. Then the Rocky Mountain states, California, Washington and Hawaii also have high peaks, and the rest of the states lump together."
  },
  {
    "objectID": "week2/slides.html#section-4",
    "href": "week2/slides.html#section-4",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "More summaries of numerical values"
  },
  {
    "objectID": "week2/slides.html#hinges-and-5-number-summaries",
    "href": "week2/slides.html#hinges-and-5-number-summaries",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hinges and 5-number summaries",
    "text": "Hinges and 5-number summaries\n\n\n\n\n [1] -3.2 -1.7 -0.4  0.1\n [5]  0.3  1.2  1.5  1.8\n [9]  2.4  3.0  4.3  6.4\n[13]  9.8\n\n\nYou know the median is the middle number. What’s a hinge?\nThere are 13 data values here, provided already sorted. We are going to write them into a Tukey named down-up-down-up pattern, evenly.\nMedian will be 7th, hinge will be 4th from each end."
  },
  {
    "objectID": "week2/slides.html#hinges-and-5-number-summary",
    "href": "week2/slides.html#hinges-and-5-number-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hinges and 5-number summary",
    "text": "Hinges and 5-number summary\n\n\n\n\n\nHinges are almost always the same as Q1 and Q3"
  },
  {
    "objectID": "week2/slides.html#box-and-whisker-display",
    "href": "week2/slides.html#box-and-whisker-display",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "box-and-whisker display",
    "text": "box-and-whisker display\n\n\n\n\nStarting with a 5-number summary"
  },
  {
    "objectID": "week2/slides.html#box-and-whisker-display-1",
    "href": "week2/slides.html#box-and-whisker-display-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "box-and-whisker display",
    "text": "box-and-whisker display\n\n\nStarting with a 5-number summary"
  },
  {
    "objectID": "week2/slides.html#identified-end-values",
    "href": "week2/slides.html#identified-end-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Identified end values",
    "text": "Identified end values\n\n\n\nWhy are some individual points singled out?\n\n\nRules for this one may be clearer?"
  },
  {
    "objectID": "week2/slides.html#section-5",
    "href": "week2/slides.html#section-5",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Isn’t this imposing a belief?"
  },
  {
    "objectID": "week2/slides.html#section-6",
    "href": "week2/slides.html#section-6",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "There is no excuse for failing to plot and look\nAnother Tukey wisdom drop"
  },
  {
    "objectID": "week2/slides.html#fences-and-outside-values",
    "href": "week2/slides.html#fences-and-outside-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Fences and outside values",
    "text": "Fences and outside values\n\n\nH-spread: difference between the hinges (we would call this Inter-Quartile Range)\nstep: 1.5 times H-spread\ninner fences: 1 step outside the hinges\nouter fences: 2 steps outside the hinges\nthe value at each end closest to, but still inside the inner fence are “adjacent”\nvalues between an inner fence and its neighbouring outer fence are “outside”\nvalues beyond outer fences are “far out”\nthese rules produce a SCHEMATIC PLOT"
  },
  {
    "objectID": "week2/slides.html#new-statistics-trimeans",
    "href": "week2/slides.html#new-statistics-trimeans",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "New statistics: trimeans",
    "text": "New statistics: trimeans\nThe number that comes closest to\n\\[\\frac{\\text{lower hinge} + 2\\times \\text{median} + \\text{upper hinge}}{4}\\] is the trimean.\n \nThink about trimmed means, where we might drop the highest and lowest 5% of observations."
  },
  {
    "objectID": "week2/slides.html#letter-value-plots-todays-solution",
    "href": "week2/slides.html#letter-value-plots-todays-solution",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Letter value plots: today’s solution",
    "text": "Letter value plots: today’s solution\n\n\nWhy break the data into quarters? Why not eighths, sixteenths? k-number summaries?\nWhat does a 7-number summary look like?\n\nHow would you make an 11-number summary?\n\n\nlibrary(lvplot)\np &lt;- ggplot(mpg, \n            aes(class, hwy))\np + geom_lv(aes(fill=..LV..)) + \n  scale_fill_brewer() + \n  coord_flip() + \n  xlab(\"\")"
  },
  {
    "objectID": "week2/slides.html#box-plots-are-ubiquitous-in-use-today.",
    "href": "week2/slides.html#box-plots-are-ubiquitous-in-use-today.",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Box plots are ubiquitous in use today.",
    "text": "Box plots are ubiquitous in use today.\n - 🐈🐩 Mostly used to compare distributions, multiple subsets of the data.\n\nPuts the emphasis on the middle 50% of observations, although variations can put emphasis on other aspects."
  },
  {
    "objectID": "week2/slides.html#easy-re-expression",
    "href": "week2/slides.html#easy-re-expression",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Easy re-expression",
    "text": "Easy re-expression"
  },
  {
    "objectID": "week2/slides.html#logs-square-roots-reciprocals",
    "href": "week2/slides.html#logs-square-roots-reciprocals",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Logs, square roots, reciprocals",
    "text": "Logs, square roots, reciprocals\n\n\nWhat you need to know about logs?\n\nhow to find good enough logs fast and easily\nthat equal differences in logs correspond to equal ratios of raw values.\n\n(This means that wherever you find people using products or ratios– even in such things as price indexes–using logs–thus converting producers to sums and ratios to differences–is likely to help.)\n\n\nThe most common transformations are logs, sqrt root, reciprocals, reciprocals of square roots\n\n-1, -1/2, +1/2, +1\n\nWhat happened to ZERO?\n\n\nIt turns out that the role of a zero power, is for the purposes of re-expression, neatly solved by the logarithm."
  },
  {
    "objectID": "week2/slides.html#re-express-to-symmetrize-the-distribution",
    "href": "week2/slides.html#re-express-to-symmetrize-the-distribution",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Re-express to symmetrize the distribution",
    "text": "Re-express to symmetrize the distribution"
  },
  {
    "objectID": "week2/slides.html#power-ladder",
    "href": "week2/slides.html#power-ladder",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Power ladder",
    "text": "Power ladder\n \n⬅️ fix RIGHT-skewed values  \n-2, -1, -1/2, 0 (log), 1/3, 1/2, 1, 2, 3, 4\n\nfix LEFT-skewed values ➡️"
  },
  {
    "objectID": "week2/slides.html#section-7",
    "href": "week2/slides.html#section-7",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "We now regard re-expression as a tool, something to let us do a better job of grasping. The grasping is done with the eye and the better job is through a more symmetric appearance.\nAnother Tukey wisdom drop"
  },
  {
    "objectID": "week2/slides.html#linearising-bivariate-relationships",
    "href": "week2/slides.html#linearising-bivariate-relationships",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linearising bivariate relationships",
    "text": "Linearising bivariate relationships\n  \n\nSurprising observation: The small fluctuations in later years.\nWhat might be possible reasons?"
  },
  {
    "objectID": "week2/slides.html#linearising-bivariate-relationships-1",
    "href": "week2/slides.html#linearising-bivariate-relationships-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linearising bivariate relationships",
    "text": "Linearising bivariate relationships\n  \n\nSee some fluctuations in the early years, too. Note that the log transformation couldn’t linearise."
  },
  {
    "objectID": "week2/slides.html#rules-and-advice",
    "href": "week2/slides.html#rules-and-advice",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Rules and advice",
    "text": "Rules and advice\n\n\n\nGraphics are friendly.\nArithmetic often exists to make graphs possible.\nGraphs force us to notice the unexpected; nothing could be more important.\nDifferent graphs show us quite different aspects of the same data.\nThere is no more reason to expect one graph to “tell all” than to expect one number to do the same.\n“Plotting \\(y\\) against \\(x\\)” involves significant choices–how we express one or both variables can be crucial.\n\n\n\n\n\nThe first step in penetrating plotting is to straighten out the dependence or point scatter as much as reasonable.\nPlotting \\(y^2\\), \\(\\sqrt{y}\\), \\(log(y)\\), \\(-1/y\\) or the like instead of \\(y\\) is one plausible step to take in search of straightness.\nPlotting \\(x^2\\), \\(\\sqrt{x}\\), \\(log(x)\\), \\(-1/x\\) or the like instead of \\(x\\) is another.\nOnce the plot is straightened, we can usually gain much by flattening it, usually by plotting residuals.\nWhen plotting scatters, we may need to be careful about how we express \\(x\\) and \\(y\\) in order to avoid concealment by crowding."
  },
  {
    "objectID": "week2/slides.html#section-8",
    "href": "week2/slides.html#section-8",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The book is a digest of 🌟 tricks and treats 🌟 of massaging numbers and drafting displays.\nMany of the tools have made it into today’s analyses in various ways. Many have not.\nNotice the word developments too: froots, fences. Tukey brought you the word “software”\nThe temperament of the book is an inspiration for the mind-set for this unit. There is such delight in working with numbers!\n“We love data!”"
  },
  {
    "objectID": "week2/slides.html#take-aways",
    "href": "week2/slides.html#take-aways",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Take-aways",
    "text": "Take-aways\n\nTukey’s approach was a reaction to many years of formalising data analysis using statistical hypothesis testing.\nMethodology development in statistical testing was a reaction to the ad-hoc nature of data analysis.\nComplex machine learning models like neural networks are in reaction to the inability of statistical models to capture highly non-linear relationships, and depend heavily on the data provided.\nExploring data today is in reaction to the need to explain complex models, to support organisations against legal challenges to decisions made from the model\nIt is much easier to accomplish computers.\n“Exploratory data analysis” as commonly used today term is unfortunately synonymous with “descriptive statistics”, but it is truly much more. Understanding its history from Tukey’s advocation helps you see it is the tooling to discover what you don’t know."
  },
  {
    "objectID": "week2/slides.html#resources",
    "href": "week2/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nwikipedia\nJohn W. Tukey (1977) Exploratory data analysis\nData coding using tidyverse suite of R packages\nSketching canvases made using fabricerin"
  },
  {
    "objectID": "week12/index.html#assignments",
    "href": "week12/index.html#assignments",
    "title": "Week 12: Long help session",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 11 is due on Thursday 23 October.\nProject Part 2 is due on Monday 03 November."
  },
  {
    "objectID": "week10/index.html",
    "href": "week10/index.html",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "",
    "text": "cubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data"
  },
  {
    "objectID": "week10/index.html#main-reference",
    "href": "week10/index.html#main-reference",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "",
    "text": "cubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data"
  },
  {
    "objectID": "week10/index.html#what-you-will-learn-this-week",
    "href": "week10/index.html#what-you-will-learn-this-week",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nBreaking up data by time, and by space\nChanging focus:\n\nMaps of space over time\nExploring time over space with glyph maps\n\nInference for spatial trends\nA flash back to the 1970s: Tukey’s median polish\nWorking with spatial polygon data\n\nMaking a choropleth map\nBending the choropleth into a cartogram\nTiling spatial regions"
  },
  {
    "objectID": "week10/index.html#lecture-slides",
    "href": "week10/index.html#lecture-slides",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week10/index.html#tutorial-instructions",
    "href": "week10/index.html#tutorial-instructions",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week10/index.html#assignments",
    "href": "week10/index.html#assignments",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 9 is due on Thursday 09 October.\nProject Part 1 is due on Monday 13 October.\nQuiz 10 is due on Thursday 16 October.\nQuiz 11 is due on Thursday 23 October.\nProject Part 2 is due on Monday 03 November."
  },
  {
    "objectID": "week1/tutorial.html",
    "href": "week1/tutorial.html",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you’ve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorial.html#objectives",
    "href": "week1/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you’ve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorial.html#preparation",
    "href": "week1/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 1",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nHave git installed on your laptop so that you can access the GitHub classroom.\nHave the latest versions of RStudio and R installed on your laptop.\nInstall this list of R packages:\n\n\nCreate an RStudio Project for this unit, called ETC5521. All your work in the tutorials should be conducted in this project. Ideally, your project is organised into folders, one for data, one for tutorial_XX, … Each week when you begin your tutorial, open the project."
  },
  {
    "objectID": "week1/tutorial.html#exercises",
    "href": "week1/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 1",
    "section": "📥 Exercises",
    "text": "📥 Exercises"
  },
  {
    "objectID": "week1/tutorial.html#how-good-are-your-detective-skills",
    "href": "week1/tutorial.html#how-good-are-your-detective-skills",
    "title": "ETC5521 Tutorial 1",
    "section": "1. How good are your detective skills?",
    "text": "1. How good are your detective skills?\nBeing good at noticing something unexpected or unusual is an important skills for exploratory data analysis. This exercise is designed to practice your detective skills.\nPlay the game alzheimer_test from the fun package by running this code:\nYou will be given 6 tasks to complete. Each one is to find a specific letter hidden among a \\(10\\times 30\\) grid of letters. When you are finished, answer these questions:\n\nWhich task did you THINK was the most difficult?\nWhich task does the DATA say was most difficult based, based on the time taken to answer, tm1.1.j. in your results data?\nSave the dataset to an .rda file.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n           char1.1.j. char2.1.j.  tm1.1.j.\nans.user.2          M          N 30.839718\nans.user.3          I          T 19.695932\nans.user.5          D          O 17.189302\nans.user.1          O          C 16.534676\nans.user.4          F          E  4.424869\nans.user            9          6  3.812386"
  },
  {
    "objectID": "week1/tutorial.html#get-started-using-github-classroom",
    "href": "week1/tutorial.html#get-started-using-github-classroom",
    "title": "ETC5521 Tutorial 1",
    "section": "3. Get started using GitHub Classroom",
    "text": "3. Get started using GitHub Classroom\n\nIn Moodle go to the Assignment 1 instructions to find the invitation to a GitHub Classroom. Accept this invitation.\nClone the assignment repo to your computer.\nOpen the assign01.html instructions.\nMake a start on loading the data into R."
  },
  {
    "objectID": "week1/tutorial.html#finishing-up",
    "href": "week1/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 1",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week1/tutorialsol.html",
    "href": "week1/tutorialsol.html",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you’ve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorialsol.html#objectives",
    "href": "week1/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you’ve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorialsol.html#preparation",
    "href": "week1/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 1",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nHave git installed on your laptop so that you can access the GitHub classroom.\nHave the latest versions of RStudio and R installed on your laptop.\nInstall this list of R packages:\n\n\nCreate an RStudio Project for this unit, called ETC5521. All your work in the tutorials should be conducted in this project. Ideally, your project is organised into folders, one for data, one for tutorial_XX, … Each week when you begin your tutorial, open the project."
  },
  {
    "objectID": "week1/tutorialsol.html#exercises",
    "href": "week1/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 1",
    "section": "📥 Exercises",
    "text": "📥 Exercises"
  },
  {
    "objectID": "week1/tutorialsol.html#how-good-are-your-detective-skills",
    "href": "week1/tutorialsol.html#how-good-are-your-detective-skills",
    "title": "ETC5521 Tutorial 1",
    "section": "1. How good are your detective skills?",
    "text": "1. How good are your detective skills?\nBeing good at noticing something unexpected or unusual is an important skills for exploratory data analysis. This exercise is designed to practice your detective skills.\nPlay the game alzheimer_test from the fun package by running this code:\nYou will be given 6 tasks to complete. Each one is to find a specific letter hidden among a \\(10\\times 30\\) grid of letters. When you are finished, answer these questions:\n\nWhich task did you THINK was the most difficult?\nWhich task does the DATA say was most difficult based, based on the time taken to answer, tm1.1.j. in your results data?\nSave the dataset to an .rda file.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n           char1.1.j. char2.1.j.  tm1.1.j.\nans.user.2          M          N 30.839718\nans.user.3          I          T 19.695932\nans.user.5          D          O 17.189302\nans.user.1          O          C 16.534676\nans.user.4          F          E  4.424869\nans.user            9          6  3.812386"
  },
  {
    "objectID": "week1/tutorialsol.html#get-started-using-github-classroom",
    "href": "week1/tutorialsol.html#get-started-using-github-classroom",
    "title": "ETC5521 Tutorial 1",
    "section": "3. Get started using GitHub Classroom",
    "text": "3. Get started using GitHub Classroom\n\nIn Moodle go to the Assignment 1 instructions to find the invitation to a GitHub Classroom. Accept this invitation.\nClone the assignment repo to your computer.\nOpen the assign01.html instructions.\nMake a start on loading the data into R."
  },
  {
    "objectID": "week1/tutorialsol.html#finishing-up",
    "href": "week1/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 1",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week1/index.html",
    "href": "week1/index.html",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "",
    "text": "The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week1/index.html#reading",
    "href": "week1/index.html#reading",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "",
    "text": "The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week1/index.html#what-you-will-learn-this-week",
    "href": "week1/index.html#what-you-will-learn-this-week",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nHow exploring data is different from a confirmatory analysis\nGet up and running with GitHub Classroom"
  },
  {
    "objectID": "week1/index.html#lecture-slides",
    "href": "week1/index.html#lecture-slides",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week1/index.html#worksheet",
    "href": "week1/index.html#worksheet",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Worksheet",
    "text": "Worksheet\n\nqmd\nhtml"
  },
  {
    "objectID": "week1/index.html#tutorial-instructions",
    "href": "week1/index.html#tutorial-instructions",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week1/index.html#assignments",
    "href": "week1/index.html#assignments",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 1 is due on Thursday 07 August.\nQuiz 2 is due on Thursday 14 August."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Professor Di Cook\n\nEmail: etc5521.clayton-x@monash.edu\nConsultation: Fridays 11-1 Clayton: Education Blg, its blg 6, 29 Ancora Imparo way, Room 352 and on zoom (see link in moodle)"
  },
  {
    "objectID": "index.html#lecturerchief-examiner",
    "href": "index.html#lecturerchief-examiner",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Professor Di Cook\n\nEmail: etc5521.clayton-x@monash.edu\nConsultation: Fridays 11-1 Clayton: Education Blg, its blg 6, 29 Ancora Imparo way, Room 352 and on zoom (see link in moodle)"
  },
  {
    "objectID": "index.html#tutors",
    "href": "index.html#tutors",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Tutors",
    "text": "Tutors\n\nKrisanat Anukarnsakulchularp\n\nTutorials: Thu 9am (CL_LTB_188), 10am (CL_LTB_188), 3pm (CL_LTB_387)\nConsultation: Thursdays 4-6pm Clayton: Education Blg, its blg 6, 29 Ancora Imparo way, Room 232A"
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Weekly schedule",
    "text": "Weekly schedule\n\nLecture+workshop: Tues 2-5pm on zoom (link in Moodle)\nTutorial: 1 hour\nWeekly learning quizzes due each Thursday 9am, from week 2\n\n\n\n\nWeek\nTopic\nReference\nAssessments\n\n\n\n\n29 Jul\nOverview. Why this course? What is EDA?\nThe Landscape of R Packages for Automated Exploratory Data Analysis\n\n\n\n05 Aug\nLearning from history\nEDA Case Study: Bay area blues\nQuiz 1\n\n\n12 Aug\nInitial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA\nThe initial examination of data\nQuiz 2\n\n\n19 Aug\nUsing computational tools to determine whether what is seen in the data can be assumed to apply more broadly\nWickham et al. (2010) Graphical inference for Infovis\nExercises 1\n\n\n19 Aug\nUsing computational tools to determine whether what is seen in the data can be assumed to apply more broadly\nWickham et al. (2010) Graphical inference for Infovis\nQuiz 3\n\n\n26 Aug\nWorking with a single variable, making transformations, detecting outliers, using robust statistics\nWilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions\nQuiz 4\n\n\n02 Sep\nBivariate dependencies and relationships, transformations to linearise\nWilke (2019) Ch 12 Visualising associations\nExercises 2\n\n\n02 Sep\nBivariate dependencies and relationships, transformations to linearise\nWilke (2019) Ch 12 Visualising associations\nQuiz 5\n\n\n09 Sep\nMaking comparisons between groups and strata\nWilke (2019) Ch 9, 10.2-4, 11.2\nQuiz 6\n\n\n16 Sep\nGoing beyond two variables, exploring high dimensions\nCook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1\nQuiz 7\n\n\n23 Sep\nExploring data having a space and time context Part I\nbrolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R\nExercises 3\n\n\n23 Sep\nExploring data having a space and time context Part I\nbrolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R\nQuiz 8\n\n\n30 Sep\nMid-semester break\n\n\n\n\n07 Oct\nExploring data having a space and time context Part II\ncubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data\nQuiz 9\n\n\n14 Oct\nSculpting data using models, checking assumptions, co-dependency and performing diagnostics\nHow to use a tour to check if your model suffers from multicollinearity\nProject Part 1\n\n\n14 Oct\nSculpting data using models, checking assumptions, co-dependency and performing diagnostics\nHow to use a tour to check if your model suffers from multicollinearity\nQuiz 10\n\n\n21 Oct\nHelp session\n\nQuiz 11\n\n\n04 Nov\n\n\nProject Part 2"
  },
  {
    "objectID": "index.html#assessments",
    "href": "index.html#assessments",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Assessments",
    "text": "Assessments\n\nWeekly learning quizzes: 5% (Due each week by Thu 9am, weeks 2-12)\nExercises 1: Instructions (15%) (Due Monday 11:55pm)\nExercises 2: Instructions (20%) (Due Monday 11:55pm)\nExercises 3: Instructions (20%) (Due Monday 11:55pm)\nProject part 1: Instructions (20%) (Due Monday 11:55pm)\nProject part 2: Instructions (20%) (Due Monday 11:55pm)"
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Software",
    "text": "Software\nWe will be using the latest versions of R and RStudio.\nHere is the code to install (most of) the R packages we will be using in this unit.\ninstall.packages(c(\"tidyr\", \"dplyr\", \"readr\", \"readxl\", \"readabs\", \"forcats\", \"tsibble\", \"cubble\", \"lubridate\", \"ggplot2\", \"GGally\", \"ggthemes\", \"sugrrants\", \"ggbeeswarm\", \"plotly\", \"gganimate\", \"tourr\", \"sugarbag\", \"tsibbletalk\", \"visdat\", \"inspectdf\", \"naniar\", \"validate\", \"vcd\", \"mvtnorm\", \"nullabor\", \"visage\", \"forecast\", \"cassowaryr\", \"brolgar\", \"palmerpenguins\", \"housingData\",  \"broom\", \"kableExtra\", \"lvplot\", \"colorspace\", \"patchwork\"), dependencies=TRUE)\nFrom GitHub, install\nremotes::install_github(\"casperhart/detourr\")\nIf you are relatively new to R, working through the materials at https://startr.numbat.space is an excellent way to up-skill. You are epsecially encouraged to work through Chapter 3, on Troubleshooting and asking for help, because at some point you will need help with your coding, and how you go about this matters and impacts the ability of others to help you.\nThese materials are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "ETC5521 Resources",
    "section": "",
    "text": "Books\n\nUnwin (2015) Graphical Data Analysis with R\nWilke (2019) Fundamentals of Data Visualization\nCook and Swayne (2007) Interactive and Dynamic Graphics for Data Analysis, Introduction\nvan der Loo and de Jonge (2018). Statistical Data Cleaning with Applications in R, John Wiley and Sons Ltd.\nCleveland (1993) Visualizing Data, Hobart Press.\nCox & Snell (1981) Applied Statistics, London: Chapman and Hall.\nMoraga (2019) Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny\nSievert (2019) Interactive web-based data visualization with R, plotly, and shiny\n\n\n\nWebsites\n\nJosse et al (2022) R-miss-tastic\nFriendly and Denis Milestones in History of Thematic Cartography, Statistical Graphics and Data Visualisation available at http://www.datavis.ca/milestones/\nWang, Cook, Hyndman, O’Hara-Wild (2019) tsibble\ncubble: A Vector Spatio-Temporal Data Structure for Data Analysis\nTierney, Cook, Prvan (2020) Browse Over Longitudinal Data Graphically and Analytically in R\nsf: Simple Features for R\nVisualising spatial data using R\nCook and Laa (2023) Interactively exploring high-dimensional data and models in R\nMason, Lee, Laa, and Cook (2022). cassowaryr: Compute Scagnostics on Pairs of Numeric Variables in a Data Set\n\n\n\nArticles\n\nDonoho (2017) 50 Years of Data Science\nStaniak and Biecek (2019) The Landscape of R Packages for Automated Exploratory Data Analysis\nHuebner et al (2018) A Contemporary Conceptual Framework for Initial Data Analysis\nHuebner et al (2020) Hidden analyses\nChatfield (1985) The Initial Examination of Data, Journal of the Royal Statistical Society, Series A (General) 148(3):214–231\nHyndman (2014) Explaining the ABS unemployment fluctuations\nBuja et al. (2009). Statistical Inference for Exploratory Data Analysis and Model Diagnostics. Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences 367 (1906): 4361–83.\nWickham et al (2010) Graphical Inference for Infovis. IEEE Transactions on Visualization and Computer Graphics 16 (6): 973–79.\nHofmann et al (2012) Graphical Tests for Power Comparison of Competing Designs. IEEE Transactions on Visualization and Computer Graphics 18 (12): 2441–48.\nMajumder et al (2013) Validation of Visual Statistical Inference, Applied to Linear Models. Journal of the American Statistical Association 108 (503): 942–56.\nTierney et al (2023) Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.\nBecker, R. A., Cleveland, W. S., & Shyu, M. J. (1996). “The Visual Design and Control of Trellis Display.” Journal of Computational and Graphical Statistics, 5(2), 123-155.\nWang, Cook, Hyndman (2019) A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data\nKobakian et al Hexagon tile map\nWickham et al (2011). tourr: An R Package for Exploring Multivariate Data with Projections"
  },
  {
    "objectID": "week1/slides.html#about-this-unit",
    "href": "week1/slides.html#about-this-unit",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "About this unit",
    "text": "About this unit"
  },
  {
    "objectID": "week1/slides.html#teaching-team-12",
    "href": "week1/slides.html#teaching-team-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Teaching team 1/2",
    "text": "Teaching team 1/2\n\n\n\nDi Cook  Distinguished Professor  Monash University \n\n🌐 https://dicook.org/\n✉️ ETC5521.Clayton-x@monash.edu\n🦣 @visnut@aus.social  @visnut.bsky.social\n\n\nI have a PhD from Rutgers University, NJ, and a Bachelor of Science from University of New England\nI am a Fellow of the American Statistical Association, elected member of the the R Foundation and International Statistical Institute, Past-Editor of the Journal of Computational and Graphical Statistics, and the R Journal.\nMy research is in data visualisation, statistical graphics and computing, with application to sports, ecology and bioinformatics. I likes to develop new methodology and software.\nMy students work on methods and software that is generally useful for the world. They have been responsible for bringing you the tidyverse suite, knitr, plotly, and many other R packages we regularly use."
  },
  {
    "objectID": "week1/slides.html#teaching-team-22",
    "href": "week1/slides.html#teaching-team-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Teaching team 2/2",
    "text": "Teaching team 2/2\n\n\n\nKrisanat Anukarnsakulchularp  Master of Business Analytics  Monash University \n\n🌐 https://github.com/KrisanatA\n✉️ ETC5521.Clayton-x@monash.edu\n\n\nHe has a Bachelor of Actuarial Science, Monash University, 2018 - 2021\nand a Master of Business Analytics, Monash University | 2022 - 2023.\nHe has published the R package animbook\nand is a first year PhD student at Monash, working on data structures, visualisation and models for spatiotemporal networks.\nThis is his fourth semester tutoring at Monash, and the only unit working on this semester."
  },
  {
    "objectID": "week1/slides.html#got-a-question-or-a-comment",
    "href": "week1/slides.html#got-a-question-or-a-comment",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Got a question, or a comment?",
    "text": "Got a question, or a comment?\n\n✋ 🔡 You can ask directly by unmuting yourself, or typing in the chat, of the live lecture.\n\n💻 If watching the recording, please post questions in the discussion (ED) forum.\n\nI hope you have many questions! 🙋🏻👣"
  },
  {
    "objectID": "week1/slides.html#welcome",
    "href": "week1/slides.html#welcome",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Welcome!",
    "text": "Welcome!\n\n\nSynopsis\nBeyond modelling and prediction, data might have many more stories to tell. Exploring data to uncover patterns and structures, involves both numerical and visual techniques designed to reveal interesting information that may be unexpected. However, an analyst must be cautious not to over-interpret apparent patterns, and to use randomisation tools to assess whether the patterns are real or spurious.\n\n\n\nLearning objectives\n\nlearn to use modern data exploration tools with many different types of contemporary data to uncover interesting structures, unusual relationships and anomalies.\nunderstand how to map out appropriate analyses for a given data set and description, define what we would expect to see in the data, and whether what we see is contrary to expectations.\nbe able to compute null samples in order to test apparent patterns, and to interpret the results using computational methods for statistical inference.\ncritically assess the strength and adequacy of data analysis."
  },
  {
    "objectID": "week1/slides.html#unit-structure",
    "href": "week1/slides.html#unit-structure",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "📅 Unit Structure",
    "text": "📅 Unit Structure\n\n\n2 hour lecture 👩‍🏫 Tue 2.00 - 4:00pm, on zoom (see moodle for the link) Class is more fun if you can attend live!\n1 hour workshop Tue 4:00 - 5:00pm, on same zoom link. This is based on material during lecture.\n1 hour on-campus tutorial 🛠️ Thu 9:00-10:00am, 10:00-11am and 3:00-4:00pm CL_Anc-19.LTB_188 Attendance is expected - this is the chance to practice and get help with assignments from your tutor’s."
  },
  {
    "objectID": "week1/slides.html#resources",
    "href": "week1/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "📚 Resources",
    "text": "📚 Resources\n🏡 Course homepage: this is where you find the course materials  (lecture slides, tutorials and tutorial solutions) https://ddde.numbat.space/\n\n🈴 Moodle: this is where you find discussion forum, zoom links, and marks https://learning.monash.edu/course/view.php?id=34784\n\n🧰 GitHub classroom: this is where you will find assignments, but links to each will be available in moodle. https://classroom.github.com/"
  },
  {
    "objectID": "week1/slides.html#assessment-part-12",
    "href": "week1/slides.html#assessment-part-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "💯 Assessment Part 1/2",
    "text": "💯 Assessment Part 1/2\n\nWeekly quizzes (5%) There will be a weekly quiz starting week 2 provided through Moodle. These are a great chance to check your knowledge, and help you prepare for the tutorial and to keep up to date with the weekly course material. Your best 10 scores will be used for your final quiz total. \nExercises 1 (15%), through GitHub classroom, Due: Aug 18, 11:55pm. This is an individual assessment. \nExercises 2 (20%), through GitHub classroom, Due: Sep 1, 11:55pm. This is an individual assessment. \nExercises 3 (20%): through GitHub classroom, Due: Sep 22, 11:55pm. This is an individual assessment. \nProject, parts 1 and 2 (20% each), through GitHub classroom, Due: Oct 13, 11:55pm and Nov 3, 11:55pm."
  },
  {
    "objectID": "week1/slides.html#github-classroom",
    "href": "week1/slides.html#github-classroom",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nWe are going to use GitHub Classroom to distribute assignment templates and keep track of your assignment progress.\n\n\nClone the first assignment by clicking on the link given in Moodle.\nOnce you have accepted it, you will get a cloned copy on your own GitHub account. It is a private repo, which means you and the teaching staff will be the only people with access.\nIf you need some help getting started, check this information.\nThe week 1 tutorial will help you get started."
  },
  {
    "objectID": "week1/slides.html#what-does-it-mean-to-explore-data",
    "href": "week1/slides.html#what-does-it-mean-to-explore-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What does it mean to explore data?",
    "text": "What does it mean to explore data?\n\n\n\n\nhttps://www.gocomics.com/calvinandhobbes/2015/08/26"
  },
  {
    "objectID": "week1/slides.html#a-simple-example-to-illustrate-exploratory-data-analysis-contrasted-with-a-confirmatory-data-analysis",
    "href": "week1/slides.html#a-simple-example-to-illustrate-exploratory-data-analysis-contrasted-with-a-confirmatory-data-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "A simple example to illustrate “exploratory data analysis” contrasted with a “confirmatory data analysis”",
    "text": "A simple example to illustrate “exploratory data analysis” contrasted with a “confirmatory data analysis”"
  },
  {
    "objectID": "week1/slides.html#what-are-the-factors-that-affect-tipping-behaviour",
    "href": "week1/slides.html#what-are-the-factors-that-affect-tipping-behaviour",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What are the factors that affect tipping behaviour?",
    "text": "What are the factors that affect tipping behaviour?\n\n\nIn one restaurant, a food server recorded the following data on all customers they served during an interval of two and a half months in early 1990.\nFood servers’ tips in restaurants may be influenced by many factors, including the nature of the restaurant, size of the party, and table locations in the restaurant. Restaurant managers need to know which factors matter when they assign tables to food servers.\n\n\n\nlibrary(tidyverse)\ntips &lt;- read_csv(\"http://ggobi.org/book/data/tips.csv\")"
  },
  {
    "objectID": "week1/slides.html#what-is-tipping",
    "href": "week1/slides.html#what-is-tipping",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is tipping?",
    "text": "What is tipping?\n\nWhen you’re dining at a full-service restaurant\n\nTip 20 percent of your full bill.\n\nWhen you grab a cup of coffee\n\nRound up or add a dollar if you’re a regular or ordered a complicated drink.\n\nWhen you have lunch at a food truck\n\nDrop a few dollars into the tip jar, but a little less than you would at a dine-in spot.\n\nWhen you use a gift card\n\nTip on the total value of the meal, not just what you paid out of pocket.\n\n\n\n\nThe basic rules of tipping that everyone should know about"
  },
  {
    "objectID": "week1/slides.html#recommended-procedure-in-the-book",
    "href": "week1/slides.html#recommended-procedure-in-the-book",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Recommended procedure in the book",
    "text": "Recommended procedure in the book\n\nStep 1: Develop a model\n\nShould the response be tip alone and use the total bill as a predictor?\nShould you create a new variable tip rate and use this as the response?\n\nStep 2: Fit the full model with sex, smoker, day, time and size as predictors\nStep 3: Refine model: Should some variables should be dropped?\nStep 4: Check distribution of residuals\nStep 5: Summarise the model, if X=something, what would be the expected tip"
  },
  {
    "objectID": "week1/slides.html#step-1",
    "href": "week1/slides.html#step-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 1",
    "text": "Step 1\nCalculate tip % as tip/total bill \\(\\times\\) 100\n  \n\ntips &lt;- tips %&gt;%\n  mutate(tip_pct = tip/totbill * 100) \n\n\n\nNote: Creating new variables (sometimes called feature engineering), is a common step in any data analysis."
  },
  {
    "objectID": "week1/slides.html#step-2-fit",
    "href": "week1/slides.html#step-2-fit",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 2 Fit",
    "text": "Step 2 Fit\nFit the full model with all variables\n \n\ntips_lm &lt;- tips %&gt;%\n  select(tip_pct, sex, smoker, day, time, size) %&gt;%\n  lm(tip_pct ~ ., data=.)"
  },
  {
    "objectID": "week1/slides.html#step-2-model-summary",
    "href": "week1/slides.html#step-2-model-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 2 Model summary",
    "text": "Step 2 Model summary\n\n\n\nlibrary(broom)\nlibrary(kableExtra)\ntidy(tips_lm) %&gt;% \n  kable(digits=2) %&gt;% \n  kable_styling() \n\n\nglance(tips_lm) %&gt;% \n  select(r.squared, statistic, \n         p.value) %&gt;% \n  kable(digits=3)\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n20.66\n2.49\n8.29\n0.00\n\n\nsexM\n-0.85\n0.83\n-1.02\n0.31\n\n\nsmokerYes\n0.36\n0.85\n0.43\n0.67\n\n\ndaySat\n-0.18\n1.83\n-0.10\n0.92\n\n\ndaySun\n1.67\n1.90\n0.88\n0.38\n\n\ndayThu\n-1.82\n2.32\n-0.78\n0.43\n\n\ntimeNight\n-2.34\n2.61\n-0.89\n0.37\n\n\nsize\n-0.96\n0.42\n-2.28\n0.02\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nstatistic\np.value\n\n\n\n\n0.042\n1.5\n0.17\n\n\n\n\n\n🤔 Which variable(s) would be considered important for predicting tip %?"
  },
  {
    "objectID": "week1/slides.html#step-3-refine-model",
    "href": "week1/slides.html#step-3-refine-model",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 3: Refine model",
    "text": "Step 3: Refine model\n\n\n\ntips_lm &lt;- tips %&gt;%\n  select(tip_pct, size) %&gt;% \n  lm(tip_pct ~ ., data=.) \ntidy(tips_lm) %&gt;% \n  kable(digits=2) %&gt;% \n  kable_styling() \n\n\nglance(tips_lm) %&gt;% \n  select(r.squared, statistic, p.value) %&gt;% \n  kable(digits=3)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n18.44\n1.12\n16.5\n0.00\n\n\nsize\n-0.92\n0.41\n-2.2\n0.03\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nstatistic\np.value\n\n\n\n\n0.02\n5\n0.026"
  },
  {
    "objectID": "week1/slides.html#model-summary",
    "href": "week1/slides.html#model-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model summary",
    "text": "Model summary\n \n\\[\\widehat{tip %} = 18.44 - 0.92 \\times size\\]\n\n \nAs the size of the dining party increases by one person the tip decreases by approximately 1%."
  },
  {
    "objectID": "week1/slides.html#model-assessment",
    "href": "week1/slides.html#model-assessment",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model assessment",
    "text": "Model assessment\n   \\(R^2 = 0.02\\).\n\n  This dropped by half from the full model, even though no other variables contributed significantly to the model. It might be a good step to examine interaction terms.\nWhat does \\(R^2 = 0.02\\) mean?"
  },
  {
    "objectID": "week1/slides.html#model-assessment-1",
    "href": "week1/slides.html#model-assessment-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model assessment",
    "text": "Model assessment\n\\(R^2 = 0.02\\) means that size explains just 2% of the variance in tip %. This is a very weak model.\n\nAnd \\(R^2 = 0.04\\) is also a very weak model.\nWhat do the \\(F\\) statistic and \\(p\\)-value mean?\nWhat do the \\(t\\) statistics and \\(p\\)-value associated with model coefficients mean?"
  },
  {
    "objectID": "week1/slides.html#overall-model-significance",
    "href": "week1/slides.html#overall-model-significance",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Overall model significance",
    "text": "Overall model significance\nAssume that we have a random sample from a population. Assume that the model for the population is\n\\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed\n\\[ \\widehat{tip %} = b_0 + b_1  sexM + ... + b_7 size \\] The \\(F\\) statistic refers to\n\\[ H_o: \\beta_1 = ... = \\beta_7 = 0 ~~ vs ~~ H_a: \\text{at least one is not 0}\\] The \\(p\\)-value is the probability that we observe the given \\(F\\) value or larger, computed assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week1/slides.html#term-significance",
    "href": "week1/slides.html#term-significance",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Term significance",
    "text": "Term significance\nAssume that we have a random sample from a population. Assume that the model for the population is\n\\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed\n\\[ \\widehat{tip %} = b_0 + b_1  sexM + ... + b_7 size \\]\nThe \\(t\\) statistics in the coefficient summary refer to\n\\[ H_o: \\beta_k = 0 ~~ vs ~~ H_a: \\beta_k \\neq 0 \\] The \\(p\\)-value is the probability that we observe the given \\(t\\) value or more extreme, computed assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week1/slides.html#model-diagnostics-md",
    "href": "week1/slides.html#model-diagnostics-md",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model diagnostics (MD)",
    "text": "Model diagnostics (MD)\nNormally, the final model summary would be accompanied diagnostic plots\n\nobserved vs fitted values to check strength and appropriateness of the fit\nunivariate plot, and normal probability plot, of residuals to check for normality\nin the simple final model like this, the observed vs predictor, with model overlaid would be advised to assess the model relative to the variability around the model\nwhen the final model has more terms, using a partial dependence plot to check the relative relationship between the response and predictors would be recommended."
  },
  {
    "objectID": "week1/slides.html#residual-plots",
    "href": "week1/slides.html#residual-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plots",
    "text": "Residual plots\n\n\n\ntips_aug &lt;- augment(tips_lm)\nggplot(tips_aug, \n    aes(x=.resid)) + \n  geom_histogram() +\n  xlab(\"residuals\")"
  },
  {
    "objectID": "week1/slides.html#residual-normal-probability-plots",
    "href": "week1/slides.html#residual-normal-probability-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual normal probability plots",
    "text": "Residual normal probability plots\n\n\n\nggplot(tips_aug, \n    aes(sample=.resid)) + \n  stat_qq() +\n  stat_qq_line() +\n  xlab(\"residuals\") +\n  theme(aspect.ratio=1)"
  },
  {
    "objectID": "week1/slides.html#fitted-vs-observed",
    "href": "week1/slides.html#fitted-vs-observed",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Fitted vs observed",
    "text": "Fitted vs observed\n\n\n\nggplot(tips_aug, \n    aes(x=.fitted, y=tip_pct)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  xlab(\"observed\") +\n  ylab(\"fitted\")"
  },
  {
    "objectID": "week1/slides.html#model-in-the-data-space",
    "href": "week1/slides.html#model-in-the-data-space",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "“Model-in-the-data-space”",
    "text": "“Model-in-the-data-space”\n\n\n\nggplot(tips_aug, \n    aes(x=size, y=tip_pct)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  ylab(\"tip %\")\n\n\nThe fitted model is overlaid on a plot of the data. This is called “model-in-the-data-space” (Wickham et al, 2015).\n\nAll the plots on the previous three slides: histogram of residuals, normal probability plot, fitted vs residuals are considered to be “data-in-the-model-space”. Stay tuned for more discussion on this later."
  },
  {
    "objectID": "week1/slides.html#section",
    "href": "week1/slides.html#section",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The result of this work might leave us with\n\na model that could be used to impose a dining/tipping policy in restaurants (see here)\n\n\nbut it should also leave us with an unease that this policy is based on weak support."
  },
  {
    "objectID": "week1/slides.html#summary",
    "href": "week1/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\n\n\nPlots as we have just seen, associated with pursuit of an answer to a specific question may be best grouped into the category of “model diagnostics (MD)”.\n\nThere are additional categories of plots for data analysis that include initial data analysis (IDA), descriptive statistics. Stay tuned for more on these.\n\n\nA separate and big area for plots of data is for communication, where we already know what is in the data and we want to communicate the information as best possible.\n\nWhen exploring data, we are using data plots to discover things we didn’t already know."
  },
  {
    "objectID": "week1/slides.html#what-did-this-analysis-miss",
    "href": "week1/slides.html#what-did-this-analysis-miss",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What did this analysis miss?",
    "text": "What did this analysis miss?"
  },
  {
    "objectID": "week1/slides.html#general-strategy-for-exploring-data",
    "href": "week1/slides.html#general-strategy-for-exploring-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "General strategy for EXPLORING DATA",
    "text": "General strategy for EXPLORING DATA\n\n\nIt’s a good idea to examine the data description, the explanation of the variables, and how the data was collected.\nYou need to know what type of variables are in the data in order to decide appropriate choice of plots, and calculations to make.\nData description should have information about data collection methods, so that the extent of what we learn from the data might apply to new data.\n\n\nWhat does that look like here?\n\nglimpse(tips)\n\nRows: 244\nColumns: 9\n$ obs     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1…\n$ totbill &lt;dbl&gt; 17.0, 10.3, 21.0, 23.7, 24.6, 25…\n$ tip     &lt;dbl&gt; 1.0, 1.7, 3.5, 3.3, 3.6, 4.7, 2.…\n$ sex     &lt;chr&gt; \"F\", \"M\", \"M\", \"M\", \"F\", \"M\", \"M…\n$ smoker  &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ day     &lt;chr&gt; \"Sun\", \"Sun\", \"Sun\", \"Sun\", \"Sun…\n$ time    &lt;chr&gt; \"Night\", \"Night\", \"Night\", \"Nigh…\n$ size    &lt;dbl&gt; 2, 3, 3, 2, 4, 4, 2, 4, 2, 2, 2,…\n$ tip_pct &lt;dbl&gt; 5.9, 16.1, 16.7, 14.0, 14.7, 18.…\n\n\n\nLook at the distribution of quantitative variables tips, total bill.\n\n\n\nExamine the distributions across categorical variables.\n\n\nExamine quantitative variables relative to categorical variables"
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips",
    "href": "week1/slides.html#distributions-of-tips",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) + \n  geom_histogram(\n    colour=\"white\")"
  },
  {
    "objectID": "week1/slides.html#because-one-binwidth-is-never-enough",
    "href": "week1/slides.html#because-one-binwidth-is-never-enough",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Because, one binwidth is never enough …",
    "text": "Because, one binwidth is never enough …"
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips-1",
    "href": "week1/slides.html#distributions-of-tips-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) +\n  geom_histogram(\n    breaks=seq(0.5,10.5,1),  \n    colour=\"white\") + \n  scale_x_continuous(\n    breaks=seq(0,11,1))\n\nBig fat bins. Tips are skewed, which means most tips are relatively small."
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips-2",
    "href": "week1/slides.html#distributions-of-tips-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) + \n  geom_histogram(\n    breaks=seq(0.5,10.5,0.1), \n    colour=\"white\") +\n  scale_x_continuous(\n    breaks=seq(0,11,1))\n\nSkinny bins. Tips are multimodal, and occurring at the full dollar and 50c amounts."
  },
  {
    "objectID": "week1/slides.html#we-could-also-look-at-total-bill-this-way",
    "href": "week1/slides.html#we-could-also-look-at-total-bill-this-way",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "We could also look at total bill this way",
    "text": "We could also look at total bill this way\nbut I’ve already done this, and we don’t learn anything more about the multiple peaks than waht is learned by plotting tips."
  },
  {
    "objectID": "week1/slides.html#relationship-between-tip-and-total",
    "href": "week1/slides.html#relationship-between-tip-and-total",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Relationship between tip and total",
    "text": "Relationship between tip and total\n\n\n\np &lt;- ggplot(tips, \n    aes(x= totbill, y=tip)) + \n  geom_point() + \n  scale_y_continuous(\n    breaks=seq(0,11,1))\np\n\nWhy is total on the x axis? \nShould we add a guideline?"
  },
  {
    "objectID": "week1/slides.html#add-a-guideline-indicating-common-practice",
    "href": "week1/slides.html#add-a-guideline-indicating-common-practice",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Add a guideline indicating common practice",
    "text": "Add a guideline indicating common practice\n\n\n\np &lt;- p + geom_abline(intercept=0, \n              slope=0.2) + \n  annotate(\"text\", x=45, y=10, \n           label=\"20% tip\") \np\n\n\n\n\nMost tips less than 20%: Skin flints vs generous diners\nA couple of big tips\nBanding horizontally is the rounding seen previously"
  },
  {
    "objectID": "week1/slides.html#we-should-examine-bar-charts-and-mosaic-plots-of-the-categorical-variables-next",
    "href": "week1/slides.html#we-should-examine-bar-charts-and-mosaic-plots-of-the-categorical-variables-next",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "We should examine bar charts and mosaic plots of the categorical variables next",
    "text": "We should examine bar charts and mosaic plots of the categorical variables next\nbut I’ve already done that, and there’s not too much of interest there."
  },
  {
    "objectID": "week1/slides.html#relative-to-categorical-variables",
    "href": "week1/slides.html#relative-to-categorical-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Relative to categorical variables",
    "text": "Relative to categorical variables\n\n\n\np + facet_grid(smoker~sex) \n\n\n\n\n\n\n\n\n\n\n\n\nThe bigger bills tend to be paid by men (and females that smoke).\nExcept for three diners, female non-smokers are very consistent tippers, probably around 15-18% though.\nThe variability in the smokers is much higher than for the non-smokers."
  },
  {
    "objectID": "week1/slides.html#isnt-this-interesting",
    "href": "week1/slides.html#isnt-this-interesting",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Isn’t this interesting?",
    "text": "Isn’t this interesting?"
  },
  {
    "objectID": "week1/slides.html#procedure-of-eda",
    "href": "week1/slides.html#procedure-of-eda",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Procedure of EDA",
    "text": "Procedure of EDA\n\nWe gained a wealth of insight in a short time.\nUsing nothing but graphical methods we investigated univariate, bivariate, and multivariate relationships.\nWe found both global features and local detail. We saw that\n\ntips were rounded; then we saw the obvious\n\ncorrelation between the tip and the size of the bill, noting the scarcity of generous tippers; finally we\ndiscovered differences in the tipping behavior of male and female smokers and non-smokers.\n\n\nThese are unexpected insights were missed from the analysis that focused solely on the primary question."
  },
  {
    "objectID": "week1/slides.html#what-can-go-wrong",
    "href": "week1/slides.html#what-can-go-wrong",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What can go wrong?",
    "text": "What can go wrong?"
  },
  {
    "objectID": "week1/slides.html#how-was-data-collected",
    "href": "week1/slides.html#how-was-data-collected",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How was data collected?",
    "text": "How was data collected?\n\n\nIn one restaurant, a food server recorded the following data on all customers they served during an interval of two and a half months in early 1990.\n\nHow much can you infer about tipping more broadly?\n\n\nTip has a weak but significant relationship with total bill?\nTips have a skewed distribution? (More small tips and fewer large tips?)\nTips tend to be made in nice round numbers.\nPeople generally under-tip?\nSmokers are less reliable tippers."
  },
  {
    "objectID": "week1/slides.html#ways-to-verify-support-or-refute-generalisations",
    "href": "week1/slides.html#ways-to-verify-support-or-refute-generalisations",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Ways to verify, support or refute generalisations",
    "text": "Ways to verify, support or refute generalisations\n\n\n\nexternal information\nother studies/samples\ngood choice of calculations and plots\nall the permutations and subsets of measured variables\ncomputational re-sampling methods (we’ll see these soon)\n\n\n\nPoor data collection methods affects every analysis, including statistical or computational modeling.\n\n\nFor this waiter and the restaurant manager, there is some useful information. Like what?\n\n\nService fee for smokers to ensure consistency?\nAssign waiter to variety of party sizes and composition.\nShifts on different days or time of day (not shown)."
  },
  {
    "objectID": "week1/slides.html#words-of-wisdom",
    "href": "week1/slides.html#words-of-wisdom",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Words of wisdom",
    "text": "Words of wisdom\nFalse discovery is the lesser danger when compared to non-discovery. Non-discovery is the failure to identify meaningful structure, and it may result in false or incomplete modeling. In a healthy scientific enterprise, the fear of non-discovery should be at least as great as the fear of false discovery."
  },
  {
    "objectID": "week1/slides.html#guide",
    "href": "week1/slides.html#guide",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Guide",
    "text": "Guide\n\n\n\nRead the data description to understand the context, and the extent of the data collection.\nUnderstand the types of variables that have been measured.\nBrainstorm a set of questions that might be interesting to answer with this data.\nFor each of your question, write down what you EXPECT to find.\nMap out the possible plots and numerical summaries to make, that could be made, but particularly, what needs to be computed in order to answer the questions.\n\n\n\nWhat potential errors might be in the data? Missing values, not recorded data, errors in coding, and think about the strategy to deal with them.\nStart on your analysis. Think about the results suggested and whether they match or contradict what you expected.\nUse randomisation methods to learn whether what you have observed is possibly spurious.\nCommunicate findings."
  },
  {
    "objectID": "week1/slides.html#topics-covered",
    "href": "week1/slides.html#topics-covered",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Topics covered",
    "text": "Topics covered\n\nMethods for single, bivariate, multivariate\n\nnumerical variables\ncategorical variables\n\nMethods to accommodate temporal and spatial (maybe also networks) context\nHow to make effective comparisons\nUtilising computational methods to assess what you see is “real”"
  },
  {
    "objectID": "week1/slides.html#resources-1",
    "href": "week1/slides.html#resources-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nCook and Swayne (2007) Interactive and Dynamic Graphics for Data Analysis, Introduction\nDonoho (2017) 50 Years of Data Science\nStaniak and Biecek (2019) The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week1/worksheetsol.html",
    "href": "week1/worksheetsol.html",
    "title": "ETC5521 Worksheet 1",
    "section": "",
    "text": "The goal of this worksheet is to tackle a data analysis together, by\n\nmapping out an analysis, with class input and help of AI\nwork on cleaning a data set\nmaking some plots\ndiscussing what is surprising, and what is expected"
  },
  {
    "objectID": "week1/worksheetsol.html#objectives",
    "href": "week1/worksheetsol.html#objectives",
    "title": "ETC5521 Worksheet 1",
    "section": "",
    "text": "The goal of this worksheet is to tackle a data analysis together, by\n\nmapping out an analysis, with class input and help of AI\nwork on cleaning a data set\nmaking some plots\ndiscussing what is surprising, and what is expected"
  },
  {
    "objectID": "week1/worksheetsol.html#about-the-data",
    "href": "week1/worksheetsol.html#about-the-data",
    "title": "ETC5521 Worksheet 1",
    "section": "📋 About the data",
    "text": "📋 About the data\nThe data to use is available from Tidy Tuesday 28 May 2024 page. Download the data from here, ideally using the tidytuesdayR package. You should only download the data from the Tidy Tuesday once, and save a copy locally on your computer.\nIn addition the gardenR package, available from remotes::install_github(\"llendway/gardenR\") has extra details about the garden.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThis is the code to download the data, and load relevant libraries.\n\n\nCode\n# remotes::install_github(\"llendway/gardenR\")\n# install.packages(\"tidytuesdayR\")\nlibrary(tidytuesdayR)\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-28')\nsave(tuesdata, file=\"tuesdata.rda\")\n\n\n\n\nCode\nlibrary(gardenR)\nlibrary(tidyverse)\nlibrary(ggbeeswarm)\n\nload(\"tuesdata.rda\")\nharvest_2020 &lt;- tuesdata$harvest_2020\nharvest_2021 &lt;- tuesdata$harvest_2021\nplanting_2020 &lt;- tuesdata$planting_2020\nplanting_2021 &lt;- tuesdata$planting_2021\nspending_2020 &lt;- tuesdata$spending_2020\nspending_2021 &lt;- tuesdata$spending_2021\n\ndata(\"garden_coords\")"
  },
  {
    "objectID": "week1/worksheetsol.html#tasks",
    "href": "week1/worksheetsol.html#tasks",
    "title": "ETC5521 Worksheet 1",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. What are the variable types, how and when was the data collected?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThis is collected in 2020 and 2021, and has a variety of numeric and categorical and time variables.\n\n\n\n\n\n\n2. What would some possible questions be to ask about this data?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe link to the ChatGPT conversation is here. It was prompted by\n\nDo you know the gardenR package by Lisa Lendway?\nWhat might be some questions that we could answer with this data?\n\nWe chose to tackle one of the Economics questions: “How much produce (by weight or value) was harvested per dollar spent?” But realised that it was not possible answer this particular question with this data. It was refined to be:\nCompare the ROI for varieties of beans.\nThe next step was to filter the data to focus on one vegetable, beans, and one year to get started.\n\n\nCode\nbeans_planting_2020 &lt;- planting_2020 |&gt;\n  filter(vegetable == \"beans\")\nbeans_harvest_2020 &lt;- harvest_2020 |&gt;\n  filter(vegetable == \"beans\")\nbeans_spending_2020 &lt;- spending_2020 |&gt;\n  filter(vegetable == \"beans\")\n\n\n\n\n\n\n\n\n3. What do you expect to find?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe might expect some difference in ROI between varieties.\n\n\n\n\n\n\n4. Pick one of the questions, and let’s try to answer it.\n\nWhat summaries should we make?\nAre we likely going to need to pre-process the data in any way?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe need to decide on a common scale. Steps are:\n\nexamine the price for each variety\nsummarise the planting data by variety and join to spending data.\nsummarise the harvest data by variety and join to spending.\n\n\n\nCode\nbeans_spending_2020 |&gt;\n  select(variety, brand, price_with_tax)\n\n\n# A tibble: 3 × 3\n  variety             brand          price_with_tax\n  &lt;chr&gt;               &lt;chr&gt;                   &lt;dbl&gt;\n1 Bush Bush Slender   Renee's Garden           3.01\n2 Chinese Red Noodle  Baker Creek              3.24\n3 Classic Slenderette Renee's Garden           3.23\n\n\nCode\nbeans_planting_2020_smry &lt;- beans_planting_2020 |&gt;\n  group_by(variety) |&gt;\n  summarise(number_seeds_planted = sum(number_seeds_planted))\n\nbeans_2020_smry &lt;- beans_spending_2020 |&gt;\n  select(variety, brand, price_with_tax) |&gt;\n  left_join(beans_planting_2020_smry)\n\nggplot(beans_2020_smry, \n       aes(price_with_tax, \n           number_seeds_planted)) + geom_point()\n\n\n\n\n\n\n\n\n\nCode\nbeans_2020_smry |&gt; \n  mutate(pr_per_seed = price_with_tax/number_seeds_planted) |&gt;\n  select(variety, pr_per_seed)\n\n\n# A tibble: 3 × 2\n  variety             pr_per_seed\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 Bush Bush Slender        0.0752\n2 Chinese Red Noodle       0.324 \n3 Classic Slenderette      0.111 \n\n\nCode\n# Do all the seed packs have the same number of seeds, \n# or did Lisa plant every seed in every pack?\n\n# Harvest\nbeans_harvest_2020_smry &lt;- beans_harvest_2020 |&gt;\n  group_by(variety) |&gt;\n  summarise(weight = sum(weight))\nbeans_harvest_2020_smry\n\n\n# A tibble: 3 × 2\n  variety             weight\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Bush Bush Slender    10038\n2 Chinese Red Noodle     356\n3 Classic Slenderette   1635\n\n\nCode\nbeans_2020_smry &lt;- beans_2020_smry |&gt;\n  left_join(beans_harvest_2020_smry)\n\nbeans_2020_smry &lt;- beans_2020_smry |&gt;\n  mutate(psy = weight/number_seeds_planted)\nbeans_2020_smry\n\n\n# A tibble: 3 × 6\n  variety             brand     price_with_tax number_seeds_planted weight   psy\n  &lt;chr&gt;               &lt;chr&gt;              &lt;dbl&gt;                &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Bush Bush Slender   Renee's …           3.01                   40  10038 251. \n2 Chinese Red Noodle  Baker Cr…           3.24                   10    356  35.6\n3 Classic Slenderette Renee's …           3.23                   29   1635  56.4\n\n\nProblems encountered.\n\nSpending didn’t specify what the cost involved, whether it was one packet of seeds,\nNot clear whether all seeds in each packet were planted. Numbers suggest, no, because some varieties had many sends and others very few.\n\nAdjustments:\n\nweight was adjusted by number of seeds planted\n\nConclusion:\nBush Bush Slender outperforms the other two, by a lot! This should also indicate that this variety is a better ROI.\nCaveats: Need to check that\n\nthe plots where each was planted was a reasonable chance that they had same access to sunlight and nutrients.\nAssuming that seeds were planted at same distance from each other.\n\n\nNext steps\nExamine the 2021 data. If same varieties grown do the same results happen.\nProblems discovered in doing this: Name of variety in 2021 might have changed to be just “Bush”, and the other two were not used. Could compare against the one new variety."
  },
  {
    "objectID": "week1/worksheet.html",
    "href": "week1/worksheet.html",
    "title": "ETC5521 Worksheet 1",
    "section": "",
    "text": "The goal of this worksheet is to tackle a data analysis together, by\n\nmapping out an analysis, with class input and help of AI\nwork on cleaning a data set\nmaking some plots\ndiscussing what is surprising, and what is expected"
  },
  {
    "objectID": "week1/worksheet.html#objectives",
    "href": "week1/worksheet.html#objectives",
    "title": "ETC5521 Worksheet 1",
    "section": "",
    "text": "The goal of this worksheet is to tackle a data analysis together, by\n\nmapping out an analysis, with class input and help of AI\nwork on cleaning a data set\nmaking some plots\ndiscussing what is surprising, and what is expected"
  },
  {
    "objectID": "week1/worksheet.html#about-the-data",
    "href": "week1/worksheet.html#about-the-data",
    "title": "ETC5521 Worksheet 1",
    "section": "📋 About the data",
    "text": "📋 About the data\nThe data to use is available from Tidy Tuesday 28 May 2024 page. Download the data from here, ideally using the tidytuesdayR package. You should only download the data from the Tidy Tuesday once, and save a copy locally on your computer.\nIn addition the gardenR package, available from remotes::install_github(\"llendway/gardenR\") has extra details about the garden.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThis is the code to download the data, and load relevant libraries.\n\n\nCode\n# remotes::install_github(\"llendway/gardenR\")\n# install.packages(\"tidytuesdayR\")\nlibrary(tidytuesdayR)\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-28')\nsave(tuesdata, file=\"tuesdata.rda\")\n\n\n\n\nCode\nlibrary(gardenR)\nlibrary(tidyverse)\nlibrary(ggbeeswarm)\n\nload(\"tuesdata.rda\")\nharvest_2020 &lt;- tuesdata$harvest_2020\nharvest_2021 &lt;- tuesdata$harvest_2021\nplanting_2020 &lt;- tuesdata$planting_2020\nplanting_2021 &lt;- tuesdata$planting_2021\nspending_2020 &lt;- tuesdata$spending_2020\nspending_2021 &lt;- tuesdata$spending_2021\n\ndata(\"garden_coords\")"
  },
  {
    "objectID": "week1/worksheet.html#tasks",
    "href": "week1/worksheet.html#tasks",
    "title": "ETC5521 Worksheet 1",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. What are the variable types, how and when was the data collected?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThis is collected in 2020 and 2021, and has a variety of numeric and categorical and time variables.\n\n\n\n\n\n\n2. What would some possible questions be to ask about this data?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe link to the ChatGPT conversation is here. It was prompted by\n\nDo you know the gardenR package by Lisa Lendway?\nWhat might be some questions that we could answer with this data?\n\nWe chose to tackle one of the Economics questions: “How much produce (by weight or value) was harvested per dollar spent?” But realised that it was not possible answer this particular question with this data. It was refined to be:\nCompare the ROI for varieties of beans.\nThe next step was to filter the data to focus on one vegetable, beans, and one year to get started.\n\n\nCode\nbeans_planting_2020 &lt;- planting_2020 |&gt;\n  filter(vegetable == \"beans\")\nbeans_harvest_2020 &lt;- harvest_2020 |&gt;\n  filter(vegetable == \"beans\")\nbeans_spending_2020 &lt;- spending_2020 |&gt;\n  filter(vegetable == \"beans\")\n\n\n\n\n\n\n\n\n3. What do you expect to find?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe might expect some difference in ROI between varieties.\n\n\n\n\n\n\n4. Pick one of the questions, and let’s try to answer it.\n\nWhat summaries should we make?\nAre we likely going to need to pre-process the data in any way?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe need to decide on a common scale. Steps are:\n\nexamine the price for each variety\nsummarise the planting data by variety and join to spending data.\nsummarise the harvest data by variety and join to spending.\n\n\n\nCode\nbeans_spending_2020 |&gt;\n  select(variety, brand, price_with_tax)\n\n\n# A tibble: 3 × 3\n  variety             brand          price_with_tax\n  &lt;chr&gt;               &lt;chr&gt;                   &lt;dbl&gt;\n1 Bush Bush Slender   Renee's Garden           3.01\n2 Chinese Red Noodle  Baker Creek              3.24\n3 Classic Slenderette Renee's Garden           3.23\n\n\nCode\nbeans_planting_2020_smry &lt;- beans_planting_2020 |&gt;\n  group_by(variety) |&gt;\n  summarise(number_seeds_planted = sum(number_seeds_planted))\n\nbeans_2020_smry &lt;- beans_spending_2020 |&gt;\n  select(variety, brand, price_with_tax) |&gt;\n  left_join(beans_planting_2020_smry)\n\nggplot(beans_2020_smry, \n       aes(price_with_tax, \n           number_seeds_planted)) + geom_point()\n\n\n\n\n\n\n\n\n\nCode\nbeans_2020_smry |&gt; \n  mutate(pr_per_seed = price_with_tax/number_seeds_planted) |&gt;\n  select(variety, pr_per_seed)\n\n\n# A tibble: 3 × 2\n  variety             pr_per_seed\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 Bush Bush Slender        0.0752\n2 Chinese Red Noodle       0.324 \n3 Classic Slenderette      0.111 \n\n\nCode\n# Do all the seed packs have the same number of seeds, \n# or did Lisa plant every seed in every pack?\n\n# Harvest\nbeans_harvest_2020_smry &lt;- beans_harvest_2020 |&gt;\n  group_by(variety) |&gt;\n  summarise(weight = sum(weight))\nbeans_harvest_2020_smry\n\n\n# A tibble: 3 × 2\n  variety             weight\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Bush Bush Slender    10038\n2 Chinese Red Noodle     356\n3 Classic Slenderette   1635\n\n\nCode\nbeans_2020_smry &lt;- beans_2020_smry |&gt;\n  left_join(beans_harvest_2020_smry)\n\nbeans_2020_smry &lt;- beans_2020_smry |&gt;\n  mutate(psy = weight/number_seeds_planted)\nbeans_2020_smry\n\n\n# A tibble: 3 × 6\n  variety             brand     price_with_tax number_seeds_planted weight   psy\n  &lt;chr&gt;               &lt;chr&gt;              &lt;dbl&gt;                &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Bush Bush Slender   Renee's …           3.01                   40  10038 251. \n2 Chinese Red Noodle  Baker Cr…           3.24                   10    356  35.6\n3 Classic Slenderette Renee's …           3.23                   29   1635  56.4\n\n\nProblems encountered.\n\nSpending didn’t specify what the cost involved, whether it was one packet of seeds,\nNot clear whether all seeds in each packet were planted. Numbers suggest, no, because some varieties had many sends and others very few.\n\nAdjustments:\n\nweight was adjusted by number of seeds planted\n\nConclusion:\nBush Bush Slender outperforms the other two, by a lot! This should also indicate that this variety is a better ROI.\nCaveats: Need to check that\n\nthe plots where each was planted was a reasonable chance that they had same access to sunlight and nutrients.\nAssuming that seeds were planted at same distance from each other.\n\n\nNext steps\nExamine the 2021 data. If same varieties grown do the same results happen.\nProblems discovered in doing this: Name of variety in 2021 might have changed to be just “Bush”, and the other two were not used. Could compare against the one new variety."
  },
  {
    "objectID": "week11/index.html",
    "href": "week11/index.html",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "",
    "text": "How to use a tour to check if your model suffers from multicollinearity How to use a tour to check if your model suffers from multicollinearity"
  },
  {
    "objectID": "week11/index.html#main-reference",
    "href": "week11/index.html#main-reference",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "",
    "text": "How to use a tour to check if your model suffers from multicollinearity How to use a tour to check if your model suffers from multicollinearity"
  },
  {
    "objectID": "week11/index.html#what-you-will-learn-this-week",
    "href": "week11/index.html#what-you-will-learn-this-week",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nDifferent types of model fitting\nDecomposing data from model\n\nfitted\nresidual\n\nDiagnostic calculations\n\nanomalies\nleverage\ninfluence"
  },
  {
    "objectID": "week11/index.html#lecture-slides",
    "href": "week11/index.html#lecture-slides",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week11/index.html#tutorial-instructions",
    "href": "week11/index.html#tutorial-instructions",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week11/index.html#assignments",
    "href": "week11/index.html#assignments",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week11/index.html#assignments-1",
    "href": "week11/index.html#assignments-1",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Assignments",
    "text": "Assignments\n\nProject Part 1 is due on Monday 13 October.\nQuiz 10 is due on Thursday 16 October.\nQuiz 11 is due on Thursday 23 October.\nProject Part 2 is due on Monday 03 November."
  },
  {
    "objectID": "week2/index.html",
    "href": "week2/index.html",
    "title": "Week 2: Learning from history",
    "section": "",
    "text": "EDA Case Study: Bay area blues"
  },
  {
    "objectID": "week2/index.html#main-reference",
    "href": "week2/index.html#main-reference",
    "title": "Week 2: Learning from history",
    "section": "",
    "text": "EDA Case Study: Bay area blues"
  },
  {
    "objectID": "week2/index.html#what-you-will-learn-this-week",
    "href": "week2/index.html#what-you-will-learn-this-week",
    "title": "Week 2: Learning from history",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nThe historical origins of EDA\nPencil and paper methods like stem-and-leaf plots\nHow to symmetrise and linearise your data\nWhere EDA is relevant today"
  },
  {
    "objectID": "week2/index.html#lecture-slides",
    "href": "week2/index.html#lecture-slides",
    "title": "Week 2: Learning from history",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week2/index.html#worksheet",
    "href": "week2/index.html#worksheet",
    "title": "Week 2: Learning from history",
    "section": "Worksheet",
    "text": "Worksheet\n\nqmd\nhtml"
  },
  {
    "objectID": "week2/index.html#tutorial-instructions",
    "href": "week2/index.html#tutorial-instructions",
    "title": "Week 2: Learning from history",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week2/index.html#assignments",
    "href": "week2/index.html#assignments",
    "title": "Week 2: Learning from history",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 1 is due on Thursday 07 August.\nQuiz 2 is due on Thursday 14 August.\nExercises 1 is due on Monday 18 August.\nQuiz 3 is due on Thursday 21 August."
  },
  {
    "objectID": "week2/worksheetsol.html",
    "href": "week2/worksheetsol.html",
    "title": "ETC5521 Worksheet Week 2",
    "section": "",
    "text": "The goal of this worksheet is to search for R packages that can be used for exploring data, and to understand their capacity.\nThe paper The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week2/worksheetsol.html#objectives",
    "href": "week2/worksheetsol.html#objectives",
    "title": "ETC5521 Worksheet Week 2",
    "section": "",
    "text": "The goal of this worksheet is to search for R packages that can be used for exploring data, and to understand their capacity.\nThe paper The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week2/worksheetsol.html#tasks",
    "href": "week2/worksheetsol.html#tasks",
    "title": "ETC5521 Worksheet Week 2",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. Use AI to obtain a list of packages that might be used for exploring data in R.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nHere is my list of prompts:\n\nDo you know the paper “The Landscape of R Packages for Automated Exploratory Data Analysis” ?\nWhat would you recommend for R packages for exploratory data analysis today? I’m not sure that the package list in that paper are reasonable 5 years into the future of today.\nWhat about databot? (Notice the botching of author: “Jennifer (Yihui) Cheng and others”)\nAre there any packages that are true to Tukey’s original book Exploratory Data Analysis from 1977?\nWhat about packages that can explore high-dimensional data?\n\n\n\n\n\n\n\n2. Pick one of the packages mentioned and give it a spin\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Use AI to learn about some of Tukey’s most famous quotes, and also what methods he developed are still in common use today\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMy prompts:\n\nWhat are some of Tukey’s most famous quotes?\nWhat are some methods that Tukey developed that are commonly used today?\nWhat are the R packages that have these methods?\nWhat about the letter value plot?\nI would love a “A one-page “Tukey contributions” teaching sheet for a lecture?” Can you provide it as a quarto document?\n\n\n\n\n\n\n\n4. For the gardenR example from week 1, ask for some Tukey-style summaries of this data\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nPrompt:\n\nFor the gardenR package data, could you make some code to do Tukey-like summaries of the data?"
  },
  {
    "objectID": "week2/worksheet.html",
    "href": "week2/worksheet.html",
    "title": "ETC5521 Worksheet Week 2",
    "section": "",
    "text": "The goal of this worksheet is to search for R packages that can be used for exploring data, and to understand their capacity.\nThe paper The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week2/worksheet.html#objectives",
    "href": "week2/worksheet.html#objectives",
    "title": "ETC5521 Worksheet Week 2",
    "section": "",
    "text": "The goal of this worksheet is to search for R packages that can be used for exploring data, and to understand their capacity.\nThe paper The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week2/worksheet.html#tasks",
    "href": "week2/worksheet.html#tasks",
    "title": "ETC5521 Worksheet Week 2",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. Use AI to obtain a list of packages that might be used for exploring data in R.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nHere is my list of prompts:\n\nDo you know the paper “The Landscape of R Packages for Automated Exploratory Data Analysis” ?\nWhat would you recommend for R packages for exploratory data analysis today? I’m not sure that the package list in that paper are reasonable 5 years into the future of today.\nWhat about databot? (Notice the botching of author: “Jennifer (Yihui) Cheng and others”)\nAre there any packages that are true to Tukey’s original book Exploratory Data Analysis from 1977?\nWhat about packages that can explore high-dimensional data?\n\n\n\n\n\n\n\n2. Pick one of the packages mentioned and give it a spin\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Use AI to learn about some of Tukey’s most famous quotes, and also what methods he developed are still in common use today\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMy prompts:\n\nWhat are some of Tukey’s most famous quotes?\nWhat are some methods that Tukey developed that are commonly used today?\nWhat are the R packages that have these methods?\nWhat about the letter value plot?\nI would love a “A one-page “Tukey contributions” teaching sheet for a lecture?” Can you provide it as a quarto document?\n\n\n\n\n\n\n\n4. For the gardenR example from week 1, ask for some Tukey-style summaries of this data\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nPrompt:\n\nFor the gardenR package data, could you make some code to do Tukey-like summaries of the data?"
  },
  {
    "objectID": "week4/index.html",
    "href": "week4/index.html",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "",
    "text": "Wickham et al. (2010) Graphical inference for Infovis Wickham et al. (2010) Graphical inference for Infovis"
  },
  {
    "objectID": "week4/index.html#main-reference",
    "href": "week4/index.html#main-reference",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "",
    "text": "Wickham et al. (2010) Graphical inference for Infovis Wickham et al. (2010) Graphical inference for Infovis"
  },
  {
    "objectID": "week4/index.html#what-you-will-learn-this-week",
    "href": "week4/index.html#what-you-will-learn-this-week",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nrevision of statistical inference\nusing re-sampling methods to calibrate reading patterns\ngenerating lineups of plots\nhow to specify the null hypothesis\ncalculating p-value and power"
  },
  {
    "objectID": "week4/index.html#lecture-slides",
    "href": "week4/index.html#lecture-slides",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week4/index.html#tutorial-instructions",
    "href": "week4/index.html#tutorial-instructions",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week4/index.html#assignments",
    "href": "week4/index.html#assignments",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Assignments",
    "text": "Assignments\n\nExercises 1 is due on Monday 18 August.\nQuiz 3 is due on Thursday 21 August.\nQuiz 4 is due on Thursday 28 August.\nExercises 2 is due on Monday 01 September.\nQuiz 5 is due on Thursday 04 September."
  },
  {
    "objectID": "week6/index.html",
    "href": "week6/index.html",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "",
    "text": "Wilke (2019) Ch 12 Visualising associations Wilke (2019) Ch 12 Visualising associations"
  },
  {
    "objectID": "week6/index.html#main-reference",
    "href": "week6/index.html#main-reference",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "",
    "text": "Wilke (2019) Ch 12 Visualising associations Wilke (2019) Ch 12 Visualising associations"
  },
  {
    "objectID": "week6/index.html#what-you-will-learn-this-week",
    "href": "week6/index.html#what-you-will-learn-this-week",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nThe humble but powerful scatterplot\nAdditions and variations\nTransformations to linearity\n(Robust) numerical measures of association\nSimpson’s paradox\nMaking null samples to test for association\nImputing missing values"
  },
  {
    "objectID": "week6/index.html#lecture-slides",
    "href": "week6/index.html#lecture-slides",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week6/index.html#tutorial-instructions",
    "href": "week6/index.html#tutorial-instructions",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week6/index.html#assignments",
    "href": "week6/index.html#assignments",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Assignments",
    "text": "Assignments\n\nExercises 2 is due on Monday 01 September.\nQuiz 5 is due on Thursday 04 September.\nQuiz 6 is due on Thursday 11 September.\nQuiz 7 is due on Thursday 18 September."
  },
  {
    "objectID": "week8/index.html",
    "href": "week8/index.html",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "",
    "text": "Cook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1"
  },
  {
    "objectID": "week8/index.html#main-reference",
    "href": "week8/index.html#main-reference",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "",
    "text": "Cook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1"
  },
  {
    "objectID": "week8/index.html#what-you-will-learn-this-week",
    "href": "week8/index.html#what-you-will-learn-this-week",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nWhat is high-dimensional data? (If all variables are quantitative)\nExploring relationships between more than two variables\n\nTours - scatterplots of combinations of variables\nMatrix of plots\nParallel coordinates\n\nWhat can be hidden\nAutomating the search for pairwise relationships using scagnostics\nLinking elements of multiple plots\nExploring multiple categorical variables"
  },
  {
    "objectID": "week8/index.html#lecture-slides",
    "href": "week8/index.html#lecture-slides",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week8/index.html#tutorial-instructions",
    "href": "week8/index.html#tutorial-instructions",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week8/index.html#assignments",
    "href": "week8/index.html#assignments",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 7 is due on Thursday 18 September.\nExercises 3 is due on Monday 22 September.\nQuiz 8 is due on Thursday 25 September.\nQuiz 9 is due on Thursday 09 October."
  },
  {
    "objectID": "week3/slides.html#the-role-of-initial-data-analysis",
    "href": "week3/slides.html#the-role-of-initial-data-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "The role of initial data analysis",
    "text": "The role of initial data analysis\n\n\nThe first thing to do with data is to look at them …. usually means tabulating and plotting the data in many different ways to see what’s going on. With the wide availability of computer packages and graphics nowadays there is no excuse for ducking the labour of this preliminary phase, and it may save some red faces later.\nCrowder, M. J. & Hand, D. J. (1990) “Analysis of Repeated Measures”"
  },
  {
    "objectID": "week3/slides.html#initial-data-analysis-and-confirmatory-analysis",
    "href": "week3/slides.html#initial-data-analysis-and-confirmatory-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Initial Data Analysis and Confirmatory Analysis",
    "text": "Initial Data Analysis and Confirmatory Analysis\n\n\n\nPrior to conducting a confirmatory data analysis, it is important to conduct an initial data analysis (IDA).\n\n\n\nConfirmatory data analysis (CDA) is focused on statistical inference and includes procedures for:\n\nhypothesis testing,\npredictive modelling,\nparameter estimation including uncertainty,\nmodel selection.\n\n\n\n\n\n\nIDA includes:\n\ndescribing the data and collection procedures\nscrutinise data for errors, outliers, missing observations\ncheck assumptions for confirmatory data analysis\n\n\n\nIDA is sometimes called preliminary data analysis.\n\n\n\n\nIDA is related to exploratory data analysis (EDA) in the sense that it is primarily conducted graphically, and there are few formal tests available."
  },
  {
    "objectID": "week3/slides.html#taxonomies-are-useful-but-rarely-perfect",
    "href": "week3/slides.html#taxonomies-are-useful-but-rarely-perfect",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Taxonomies are useful but rarely perfect",
    "text": "Taxonomies are useful but rarely perfect"
  },
  {
    "objectID": "week3/slides.html#objectives-of-ida",
    "href": "week3/slides.html#objectives-of-ida",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Objectives of IDA?",
    "text": "Objectives of IDA?\n\nThe main objective for IDA is to intercept any problems in the data that might adversely affect the confirmatory data analysis.\n\n\n\nThe role of CDA is to answer the intended question(s) that the data were collected for.\n\n\nIDA is often unreported in the data analysis reports or scientific papers, for various reasons. It might not have been done, or it may have been conducted but there was no space in the paper to report on it."
  },
  {
    "objectID": "week3/slides.html#ida-in-government-statistics",
    "href": "week3/slides.html#ida-in-government-statistics",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA in government statistics",
    "text": "IDA in government statistics\nThe purpose of data cleaning is to bring data up to a level of quality such that it can reliably be used for the production of statistical models or statements.\nA statistical value chain is constructed by defining a number of meaningful intermediate data products, for which a chosen set of quality attributes are well described.\n\n\n\n\nvan der Loo & de Jonge (2018) Statistical Data Cleaning with Applications in R"
  },
  {
    "objectID": "week3/slides.html#ida-in-health-and-medical-data",
    "href": "week3/slides.html#ida-in-health-and-medical-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA in health and medical data",
    "text": "IDA in health and medical data\n\nHuebner et al (2018)’s six steps of IDA: (1) Metadata setup, (2) Data cleaning, (3) Data screening, (4) Initial reporting, (5) Refining and updating the analysis plan, (6) Reporting IDA in documentation."
  },
  {
    "objectID": "week3/slides.html#heed-these-words",
    "href": "week3/slides.html#heed-these-words",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Heed these words",
    "text": "Heed these words\n\n\n\nIDA prepares an analyst for CDA. One needs to be careful about NOT compromising the inference.\n\nHow do you compromise inference?\n\n\nChange your inference or questions based on what you find in IDA.\nOutlier removal or not.\nMissing value imputation choices.\nTreatment of zeros.\nHandling of variable type, categorical temporal.\nLack of multivariate relationship checking, including subsets based on levels of categorical variables.\nChoosing variables and observations.\n\n\n\n\nHow do you avoid these errors?\n\nDocument ALL the IDA, using a reproducible analysis script.\nPre-register your CDA plan, so that your CDA questions do not change.\nDecisions made on outlier removal, variable selection, recoding, sampling, handling of zeros have known affects on results, and are justifiable.\n\nInsure yourself against accusations of data snooping, data dredging, data fishing."
  },
  {
    "objectID": "week3/slides.html#data-screening",
    "href": "week3/slides.html#data-screening",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data screening",
    "text": "Data screening"
  },
  {
    "objectID": "week3/slides.html#data-screening-1",
    "href": "week3/slides.html#data-screening-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data screening",
    "text": "Data screening\n\n\n\nIt’s important to check how the data are understood by the computer.\n\nthat is, checking for data type:\n\nWas the date read in as character?\nWas a factor read in as numeric?\n\n\n\nAlso important for making inference is to know whether the data supports making broader conclusions. How was the data collected? Is it clear what the population of interest is, and that the data is a representative sample of this population?"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-12",
    "href": "week3/slides.html#example-checking-the-data-type-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type (1/2)",
    "text": "Example: Checking the data type (1/2)\n\n\nlecture3-example.xlsx\n\n\n\n\n\nlibrary(readxl)\nlibrary(here)\ndf &lt;- read_excel(here(\"data/lecture3-example.xlsx\"))\ndf\n\n# A tibble: 5 × 4\n     id date                loc       temp\n  &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;    &lt;dbl&gt;\n1     1 2010-01-03 00:00:00 New York  42  \n2     2 2010-02-03 00:00:00 New York  41.4\n3     3 2010-03-03 00:00:00 New York  38.5\n4     4 2010-04-03 00:00:00 New York  41.1\n5     5 2010-05-03 00:00:00 New York  39.8\n\n\n\nWhat problems are there with the computer’s interpretation of data type?\nWhat context specific issues indicate incorrect computer interpretation?"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-22",
    "href": "week3/slides.html#example-checking-the-data-type-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type (2/2)",
    "text": "Example: Checking the data type (2/2)\n\n\n\nlibrary(lubridate)\ndf &lt;- read_excel(here(\"data/lecture3-example.xlsx\"), \n                 col_types = c(\"text\", \n                               \"date\", \n                               \"text\",\n                               \"numeric\"))\n\ndf |&gt; \n  mutate(id = as.factor(id),\n         date = ydm(date)) |&gt;\n  mutate(\n         day = day(date),\n         month = month(date),\n         year = year(date)) \n\n# A tibble: 5 × 7\n  id    date       loc       temp   day month  year\n  &lt;fct&gt; &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1     2010-03-01 New York  42       1     3  2010\n2 2     2010-03-02 New York  41.4     2     3  2010\n3 3     2010-03-03 New York  38.5     3     3  2010\n4 4     2010-03-04 New York  41.1     4     3  2010\n5 5     2010-03-05 New York  39.8     5     3  2010\n\n\n\n\nid is now a factor instead of integer\nday, month and year are now extracted from the date\nIs it okay now?\n\n\n\n\nIn the United States, it’s common to use the date format MM/DD/YYYY (gasps) while the rest of the world commonly uses DD/MM/YYYY or better still YYYY/MM/DD.\n\n\n\n\nIt’s highly probable that the dates are 1st-5th March and not 3rd of Jan-May.\n\n\n\n\nYou can validate interpretation of temperature using weather database."
  },
  {
    "objectID": "week3/slides.html#example-specifying-the-data-type-with-r",
    "href": "week3/slides.html#example-specifying-the-data-type-with-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Specifying the data type with R",
    "text": "Example: Specifying the data type with R\n\n\n\nYou can robustify your workflow by ensuring you have a check for the expected data type in your code.\n\n\nxlsx_df &lt;- read_excel(here(\"data/lecture3-example.xlsx\"),\n                 col_types = c(\"text\", \"date\", \"text\", \"numeric\"))  |&gt; \n  mutate(id = as.factor(id), \n         date = as.character(date),\n         date = as.Date(date, format = \"%Y-%d-%m\"))\n\n\n\nread_csv has a broader support for col_types\n\n\ncsv_df &lt;- read_csv(here::here(\"data/lecture3-example.csv\"),\n                 col_types = cols(\n                      id = col_factor(),\n                      date = col_date(format = \"%m/%d/%y\"),\n                      loc = col_character(),\n                      temp = col_double()))\n\n\nThe checks (or coercions) ensure that even if the data are updated, you can have some confidence that any data type error will be picked up before further analysis."
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-with-r",
    "href": "week3/slides.html#example-checking-the-data-type-with-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type with R",
    "text": "Example: Checking the data type with R\nYou can have a quick glimpse of the data type with:\n\ndplyr::glimpse(xlsx_df)\n\nRows: 5\nColumns: 4\n$ id   &lt;fct&gt; 1, 2, 3, 4, 5\n$ date &lt;date&gt; 2010-03-01, 2010-03-02, 2010-03-03, 2010-03-0…\n$ loc  &lt;chr&gt; \"New York\", \"New York\", \"New York\", \"New Yor…\n$ temp &lt;dbl&gt; 42, 41, 38, 41, 40\n\ndplyr::glimpse(csv_df)\n\nRows: 5\nColumns: 4\n$ id   &lt;fct&gt; 1, 2, 3, 4, 5\n$ date &lt;date&gt; 2010-03-01, 2010-03-02, 2010-03-03, 2010-03-0…\n$ loc  &lt;chr&gt; \"New York\", \"New York\", \"New York\", \"New Yor…\n$ temp &lt;dbl&gt; 42, 41, 38, 41, 40"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-visually",
    "href": "week3/slides.html#example-checking-the-data-type-visually",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type visually",
    "text": "Example: Checking the data type visually\n\n\nYou can also visualise the data type with:\n\nlibrary(visdat)\nvis_dat(xlsx_df)\n\n\n\n\n\n\n\n\n\n\nlibrary(inspectdf)\ninspect_types(xlsx_df)  |&gt; \n  show_plot()"
  },
  {
    "objectID": "week3/slides.html#data-cleaning",
    "href": "week3/slides.html#data-cleaning",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning",
    "text": "Data cleaning"
  },
  {
    "objectID": "week3/slides.html#data-cleaning-12",
    "href": "week3/slides.html#data-cleaning-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning (1/2)",
    "text": "Data cleaning (1/2)\nData quality checks should be one of the first steps in the data analysis to assess any problems with the data.\nThese include using common or domain knowledge to check if the recorded data have sensible values.\n\n\nAre positive values, e.g. height and weight, recorded as positive values with a plausible range?\nIf the data are counts, do the recorded values contain non-integer values?\nFor compositional data, do the values add up to 100% (or 1)? If not, is that a measurement error or due to rounding? Or is another variable missing?\nDoes the data contain only positives, ie disease occurrences, or warranty claims? If so, what would the no report group look like?"
  },
  {
    "objectID": "week3/slides.html#data-cleaning-22",
    "href": "week3/slides.html#data-cleaning-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning (2/2)",
    "text": "Data cleaning (2/2)\nIn addition, numerical or graphical summaries may reveal that there is unwanted structure in the data, for example,\n\n\n\nDoes the treatment group have different demographic characteristics to the control group?\nAre the distributions similar between the or training and test sets?\nAre there sufficient measurements for each level of categorical variable, or across the range of numerical variables?\n\n\n\n\nDoes the distribution of the data imply violations of assumptions for the CDA, such as\n\nnon-normality,\ndiscrete rather real-valued, or\ndifferent variance in different domains?\n\n\n\n\n\nData scrutinizing is a process that you get better at with practice and have familiarity with the domain area."
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-quality",
    "href": "week3/slides.html#example-checking-the-data-quality",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data quality",
    "text": "Example: Checking the data quality\n\n\n\n\n# A tibble: 9 × 4\n  id    date       loc        temp\n  &lt;fct&gt; &lt;date&gt;     &lt;chr&gt;     &lt;dbl&gt;\n1 1     2010-03-01 New York   42  \n2 2     2010-03-02 New York   41.4\n3 3     2010-03-03 New York   38.5\n4 4     2010-03-04 New York   41.1\n5 5     2010-03-05 New York   39.8\n6 6     2020-03-01 Melbourne  30.6\n7 7     2020-03-02 Melbourne  17.9\n8 8     2020-03-03 Melbourne  18.6\n9 9     2020-03-04 &lt;NA&gt;       21.3\n\n\n\n\nNumerical or graphical summaries or even just eye-balling the data helps to uncover some data quality issues.\nAny issues here?\n\n\n\n\nThere’s a missing value in loc.\nTemperature is in Farenheit for New York but Celsius in Melbourne (you can validate this again using external sources)."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-17",
    "href": "week3/slides.html#case-study-world-development-indicators-17",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (1/7)",
    "text": "Case study: World development indicators (1/7)\n\n\n\noptions(width=80)\nraw_dat &lt;- read_csv(here(\"data/world-development-indicators.csv\"), \n                    na = \"..\", n_max = 11935)\nglimpse(raw_dat)\n\nRows: 11,935\nColumns: 54\n$ `Country Name`  &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Argentina\", \"A…\n$ `Country Code`  &lt;chr&gt; \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\"…\n$ `Series Name`   &lt;chr&gt; \"Adolescent fertility rate (births per 1,000 women age…\n$ `Series Code`   &lt;chr&gt; \"SP.ADO.TFRT\", \"NV.AGR.TOTL.ZS\", \"ER.H2O.FWTL.ZS\", \"SH…\n$ `1969 [YR1969]` &lt;dbl&gt; 6.4e+01, 9.2e+00, NA, NA, 3.3e+00, NA, 2.2e+01, NA, NA…\n$ `1970 [YR1970]` &lt;dbl&gt; 6.5e+01, 9.6e+00, NA, NA, 3.5e+00, NA, 2.5e+01, NA, NA…\n$ `1971 [YR1971]` &lt;dbl&gt; 6.7e+01, 1.1e+01, NA, NA, 3.7e+00, NA, 2.4e+01, 8.7e+0…\n$ `1972 [YR1972]` &lt;dbl&gt; 6.8e+01, 1.1e+01, NA, NA, 3.6e+00, NA, 1.9e+01, 9.2e+0…\n$ `1973 [YR1973]` &lt;dbl&gt; 7.1e+01, 1.2e+01, NA, NA, 3.7e+00, NA, 2.7e+01, 9.6e+0…\n$ `1974 [YR1974]` &lt;dbl&gt; 7.5e+01, 1.0e+01, NA, NA, 3.7e+00, NA, 3.0e+01, 9.9e+0…\n$ `1975 [YR1975]` &lt;dbl&gt; 7.8e+01, 6.6e+00, NA, NA, 3.6e+00, NA, 2.9e+01, 1.0e+0…\n$ `1976 [YR1976]` &lt;dbl&gt; 8.1e+01, 8.2e+00, NA, NA, 3.8e+00, NA, 2.0e+01, 1.0e+0…\n$ `1977 [YR1977]` &lt;dbl&gt; 8.4e+01, 8.1e+00, 9.5e+00, NA, 3.7e+00, NA, 2.6e+01, 1…\n$ `1978 [YR1978]` &lt;dbl&gt; 8.2e+01, 7.5e+00, NA, NA, 3.8e+00, NA, 2.9e+01, 1.1e+0…\n$ `1979 [YR1979]` &lt;dbl&gt; 8.0e+01, 7.8e+00, NA, NA, 4.0e+00, NA, 3.1e+01, 1.2e+0…\n$ `1980 [YR1980]` &lt;dbl&gt; 7.8e+01, 6.4e+00, NA, NA, 3.9e+00, NA, 3.3e+01, 1.2e+0…\n$ `1981 [YR1981]` &lt;dbl&gt; 7.6e+01, 6.5e+00, NA, NA, 3.6e+00, NA, 4.8e+01, 1.2e+0…\n$ `1982 [YR1982]` &lt;dbl&gt; 7.4e+01, 9.6e+00, NA, NA, 3.6e+00, NA, 4.6e+01, 1.2e+0…\n$ `1983 [YR1983]` &lt;dbl&gt; 7.4e+01, 8.7e+00, NA, NA, 3.6e+00, NA, 4.6e+01, 1.3e+0…\n$ `1984 [YR1984]` &lt;dbl&gt; 7.4e+01, 8.3e+00, NA, NA, 3.6e+00, NA, 4.2e+01, 1.3e+0…\n$ `1985 [YR1985]` &lt;dbl&gt; 7.4e+01, 7.6e+00, NA, NA, 3.3e+00, NA, 3.3e+01, 1.3e+0…\n$ `1986 [YR1986]` &lt;dbl&gt; 7.4e+01, 7.8e+00, NA, NA, 3.4e+00, NA, 3.3e+01, 1.3e+0…\n$ `1987 [YR1987]` &lt;dbl&gt; 7.3e+01, 8.1e+00, NA, NA, 3.7e+00, NA, 4.8e+01, 1.4e+0…\n$ `1988 [YR1988]` &lt;dbl&gt; 7.3e+01, 9.0e+00, NA, NA, 3.8e+00, NA, 4.3e+01, 1.4e+0…\n$ `1989 [YR1989]` &lt;dbl&gt; 7.3e+01, 9.6e+00, NA, NA, 3.6e+00, NA, 8.0e+01, 1.3e+0…\n$ `1990 [YR1990]` &lt;dbl&gt; 7.3e+01, 8.1e+00, NA, 9.7e+01, 3.4e+00, NA, 3.2e+01, 1…\n$ `1991 [YR1991]` &lt;dbl&gt; 7.3e+01, 6.7e+00, NA, NA, 3.5e+00, NA, 2.3e+01, 1.3e+0…\n$ `1992 [YR1992]` &lt;dbl&gt; 7.3e+01, 6.0e+00, NA, 9.6e+01, 3.6e+00, NA, 2.2e+01, 1…\n$ `1993 [YR1993]` &lt;dbl&gt; 7.3e+01, 5.1e+00, NA, NA, 3.5e+00, NA, 2.6e+01, 1.5e+0…\n$ `1994 [YR1994]` &lt;dbl&gt; 7.2e+01, 5.1e+00, NA, NA, 3.5e+00, NA, 2.7e+01, 1.6e+0…\n$ `1995 [YR1995]` &lt;dbl&gt; 7.1e+01, 5.4e+00, NA, 9.8e+01, 3.7e+00, NA, 2.8e+01, 1…\n$ `1996 [YR1996]` &lt;dbl&gt; 7.0e+01, 5.6e+00, NA, NA, 3.8e+00, NA, 2.8e+01, 1.7e+0…\n$ `1997 [YR1997]` &lt;dbl&gt; 7.0e+01, 5.2e+00, 9.8e+00, 9.7e+01, 3.9e+00, NA, 3.0e+…\n$ `1998 [YR1998]` &lt;dbl&gt; 6.9e+01, 5.3e+00, NA, 9.8e+01, 3.9e+00, NA, 3.3e+01, 2…\n$ `1999 [YR1999]` &lt;dbl&gt; 6.8e+01, 4.5e+00, NA, 9.8e+01, 4.0e+00, NA, 3.6e+01, 2…\n$ `2000 [YR2000]` &lt;dbl&gt; 6.7e+01, 4.7e+00, NA, 9.9e+01, 3.8e+00, NA, 3.4e+01, 2…\n$ `2001 [YR2001]` &lt;dbl&gt; 6.6e+01, 4.6e+00, NA, 9.8e+01, 3.6e+00, 6.5e+01, 3.7e+…\n$ `2002 [YR2002]` &lt;dbl&gt; 6.5e+01, 1.0e+01, NA, 9.9e+01, 3.3e+00, NA, 6.2e+01, 2…\n$ `2003 [YR2003]` &lt;dbl&gt; 6.5e+01, 1.0e+01, NA, 9.9e+01, 3.5e+00, NA, 5.1e+01, 2…\n$ `2004 [YR2004]` &lt;dbl&gt; 6.4e+01, 8.4e+00, NA, 9.9e+01, 4.1e+00, NA, 4.2e+01, 2…\n$ `2005 [YR2005]` &lt;dbl&gt; 6.4e+01, 7.9e+00, NA, 9.9e+01, 4.1e+00, 7.9e+01, 3.5e+…\n$ `2006 [YR2006]` &lt;dbl&gt; 6.4e+01, 6.9e+00, NA, 9.9e+01, 4.4e+00, NA, 2.8e+01, 2…\n$ `2007 [YR2007]` &lt;dbl&gt; 6.4e+01, 7.5e+00, NA, 9.9e+01, 4.4e+00, NA, 2.6e+01, 2…\n$ `2008 [YR2008]` &lt;dbl&gt; 6.4e+01, 7.3e+00, NA, 9.5e+01, 4.7e+00, NA, 2.2e+01, 2…\n$ `2009 [YR2009]` &lt;dbl&gt; 6.4e+01, 5.3e+00, NA, 9.8e+01, 4.4e+00, NA, 2.6e+01, 2…\n$ `2010 [YR2010]` &lt;dbl&gt; 6.4e+01, 7.1e+00, NA, 9.5e+01, 4.6e+00, NA, 2.5e+01, 2…\n$ `2011 [YR2011]` &lt;dbl&gt; 6.4e+01, 7.0e+00, NA, 9.7e+01, 4.6e+00, NA, 2.6e+01, 2…\n$ `2012 [YR2012]` &lt;dbl&gt; 6.4e+01, 5.8e+00, 1.3e+01, 9.8e+01, 4.6e+00, 5.5e+01, …\n$ `2013 [YR2013]` &lt;dbl&gt; 6.4e+01, 6.1e+00, NA, 9.7e+01, 4.5e+00, 8.1e+01, 3.3e+…\n$ `2014 [YR2014]` &lt;dbl&gt; 6.4e+01, 6.7e+00, 1.3e+01, 1.0e+02, 4.7e+00, NA, 3.4e+…\n$ `2015 [YR2015]` &lt;dbl&gt; 6.3e+01, 5.2e+00, NA, 1.0e+02, NA, NA, 4.0e+01, NA, NA…\n$ `2016 [YR2016]` &lt;dbl&gt; 6.3e+01, 6.4e+00, NA, NA, NA, NA, 3.8e+01, NA, NA, 1.3…\n$ `2017 [YR2017]` &lt;dbl&gt; NA, 5.6e+00, NA, NA, NA, NA, 3.9e+01, NA, NA, 1.1e+01,…\n$ `2018 [YR2018]` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nWorld Development Indicators (WDI), sourced from the World Bank Group (2019)\n\n\n\nWhat are the data types?\nWhat are the variables?\nWhat are the observations?\nIs the data in tidy form?"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-27",
    "href": "week3/slides.html#case-study-world-development-indicators-27",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (2/7)",
    "text": "Case study: World development indicators (2/7)\n\n\n\ncountry_code_df &lt;- raw_dat  |&gt;\n  distinct(`Country Name`, `Country Code`)  |&gt;\n  rename_all(janitor::make_clean_names)  |&gt;\n  left_join(\n    countrycode::codelist |&gt; select(iso3c, region, continent),\n    by = c(\"country_code\" = \"iso3c\")\n  )  |&gt;\n  arrange(continent, region) \n\n\n\n\nRows: 217\nColumns: 4\n$ country_name &lt;chr&gt; \"Algeria\", \"Djibouti\", \"Egypt, Arab Rep.\", \"Libya\", \"Moro…\n$ country_code &lt;chr&gt; \"DZA\", \"DJI\", \"EGY\", \"LBY\", \"MAR\", \"TUN\", \"AGO\", \"BEN\", \"…\n$ region       &lt;chr&gt; \"Middle East & North Africa\", \"Middle East & North Africa…\n$ continent    &lt;chr&gt; \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa…\n\n\n# A tibble: 6 × 2\n  continent     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Africa       54\n2 Americas     46\n3 Asia         50\n4 Europe       46\n5 Oceania      19\n6 &lt;NA&gt;          2\n\n\n# A tibble: 8 × 2\n  region                         n\n  &lt;chr&gt;                      &lt;int&gt;\n1 East Asia & Pacific           37\n2 Europe & Central Asia         56\n3 Latin America & Caribbean     42\n4 Middle East & North Africa    21\n5 North America                  3\n6 South Asia                     8\n7 Sub-Saharan Africa            48\n8 &lt;NA&gt;                           2\n\n\n\n\n\n\nHow many countries are included\nHow many continents, regions?\nWhy are there NAs here?\n\n\n\ncountry_code_df |&gt; filter(is.na(continent))\n\n# A tibble: 2 × 4\n  country_name    country_code region continent\n  &lt;chr&gt;           &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;    \n1 Channel Islands CHI          &lt;NA&gt;   &lt;NA&gt;     \n2 Kosovo          XKX          &lt;NA&gt;   &lt;NA&gt;"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-37",
    "href": "week3/slides.html#case-study-world-development-indicators-37",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (3/7)",
    "text": "Case study: World development indicators (3/7)\n\n\n\nwdi_vars &lt;- raw_dat  |&gt;\n  select(`Series Name`, `Series Code`) |&gt;\n  distinct() |&gt;\n  rename_all(janitor::make_clean_names) \n\n\n\n\n\n\n\n\n\n\n\nseries_name\nseries_code\n\n\n\n\nAdolescent fertility rate (births per 1,000 women ages 15-19)\nSP.ADO.TFRT\n\n\nAgriculture, forestry, and fishing, value added (% of GDP)\nNV.AGR.TOTL.ZS\n\n\nAnnual freshwater withdrawals, total (% of internal resources)\nER.H2O.FWTL.ZS\n\n\nBirths attended by skilled health staff (% of total)\nSH.STA.BRTC.ZS\n\n\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n\n\nContraceptive prevalence, any methods (% of women ages 15-49)\nSP.DYN.CONU.ZS\n\n\nDomestic credit provided by financial sector (% of GDP)\nFS.AST.DOMS.GD.ZS\n\n\nElectric power consumption (kWh per capita)\nEG.USE.ELEC.KH.PC\n\n\nEnergy use (kg of oil equivalent per capita)\nEG.USE.PCAP.KG.OE\n\n\nExports of goods and services (% of GDP)\nNE.EXP.GNFS.ZS\n\n\nExternal debt stocks, total (DOD, current US$)\nDT.DOD.DECT.CD\n\n\nFertility rate, total (births per woman)\nSP.DYN.TFRT.IN\n\n\nForeign direct investment, net inflows (BoP, current US$)\nBX.KLT.DINV.CD.WD\n\n\nForest area (sq. km)\nAG.LND.FRST.K2\n\n\nGDP (current US$)\nNY.GDP.MKTP.CD\n\n\nGDP growth (annual %)\nNY.GDP.MKTP.KD.ZG\n\n\nGNI per capita, Atlas method (current US$)\nNY.GNP.PCAP.CD\n\n\nGNI per capita, PPP (current international $)\nNY.GNP.PCAP.PP.CD\n\n\nGNI, Atlas method (current US$)\nNY.GNP.ATLS.CD\n\n\nGNI, PPP (current international $)\nNY.GNP.MKTP.PP.CD\n\n\nGross capital formation (% of GDP)\nNE.GDI.TOTL.ZS\n\n\nHigh-technology exports (% of manufactured exports)\nTX.VAL.TECH.MF.ZS\n\n\nImmunization, measles (% of children ages 12-23 months)\nSH.IMM.MEAS\n\n\nImports of goods and services (% of GDP)\nNE.IMP.GNFS.ZS\n\n\nIncome share held by lowest 20%\nSI.DST.FRST.20\n\n\nIndustry (including construction), value added (% of GDP)\nNV.IND.TOTL.ZS\n\n\nInflation, GDP deflator (annual %)\nNY.GDP.DEFL.KD.ZG\n\n\nLife expectancy at birth, total (years)\nSP.DYN.LE00.IN\n\n\nMerchandise trade (% of GDP)\nTG.VAL.TOTL.GD.ZS\n\n\nMilitary expenditure (% of GDP)\nMS.MIL.XPND.GD.ZS\n\n\nMobile cellular subscriptions (per 100 people)\nIT.CEL.SETS.P2\n\n\nMortality rate, under-5 (per 1,000 live births)\nSH.DYN.MORT\n\n\nNet barter terms of trade index (2000 = 100)\nTT.PRI.MRCH.XD.WD\n\n\nNet migration\nSM.POP.NETM\n\n\nNet official development assistance and official aid received (current US$)\nDT.ODA.ALLD.CD\n\n\nPersonal remittances, received (current US$)\nBX.TRF.PWKR.CD.DT\n\n\nPopulation density (people per sq. km of land area)\nEN.POP.DNST\n\n\nPopulation growth (annual %)\nSP.POP.GROW\n\n\nPopulation, total\nSP.POP.TOTL\n\n\nPoverty headcount ratio at $1.90 a day (2011 PPP) (% of population)\nSI.POV.DDAY\n\n\nPoverty headcount ratio at national poverty lines (% of population)\nSI.POV.NAHC\n\n\nPrevalence of HIV, total (% of population ages 15-49)\nSH.DYN.AIDS.ZS\n\n\nPrevalence of underweight, weight for age (% of children under 5)\nSH.STA.MALN.ZS\n\n\nPrimary completion rate, total (% of relevant age group)\nSE.PRM.CMPT.ZS\n\n\nRevenue, excluding grants (% of GDP)\nGC.REV.XGRT.GD.ZS\n\n\nSchool enrollment, primary (% gross)\nSE.PRM.ENRR\n\n\nSchool enrollment, primary and secondary (gross), gender parity index (GPI)\nSE.ENR.PRSC.FM.ZS\n\n\nSchool enrollment, secondary (% gross)\nSE.SEC.ENRR\n\n\nStatistical Capacity score (Overall average)\nIQ.SCI.OVRL\n\n\nSurface area (sq. km)\nAG.SRF.TOTL.K2\n\n\nTax revenue (% of GDP)\nGC.TAX.TOTL.GD.ZS\n\n\nTerrestrial and marine protected areas (% of total territorial area)\nER.PTD.TOTL.ZS\n\n\nTime required to start a business (days)\nIC.REG.DURS\n\n\nTotal debt service (% of exports of goods, services and primary income)\nDT.TDS.DECT.EX.ZS\n\n\nUrban population growth (annual %)\nSP.URB.GROW\n\n\n\n\n\n\n\n\n\n\nAnalysis will use the short name (series_code) for variables.\nStore full variable name (series_name) and short name (series_code) in a separate table.\nThe series_code will be used as the key whenever the full name is needed."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-47",
    "href": "week3/slides.html#case-study-world-development-indicators-47",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (4/7)",
    "text": "Case study: World development indicators (4/7)\n\n\n\nwdi &lt;- raw_dat  |&gt;\n  select(`Country Code`, `Series Code`, `1969 [YR1969]`:`2018 [YR2018]`) |&gt;\n  rename_all(janitor::make_clean_names) |&gt;\n  pivot_longer(x1969_yr1969:x2018_yr2018,\n               names_to = \"year\", \n               values_to = \"value\") |&gt;\n  mutate(year = as.numeric(str_sub(year, 2, 5)) ) |&gt;\n  pivot_wider(names_from = series_code,\n              values_from = value)\n\nwdi2017 &lt;- wdi  |&gt; filter(year == 2017)\n\n\nOrganise data into tidy form\nCheck missing value distribution"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-57",
    "href": "week3/slides.html#case-study-world-development-indicators-57",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (5/7)",
    "text": "Case study: World development indicators (5/7)\n\n\nCheck missings by\n\nvariable\ncountry"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-67",
    "href": "week3/slides.html#case-study-world-development-indicators-67",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (6/7)",
    "text": "Case study: World development indicators (6/7)\n\n\nLook at Costa Rica (CRI), most complete country\n\n\n\n\n\n\n\n\n\n\nTo illustrate imputation, we’ll show one of the variables, that is relatively complete.\n\n\n\n\n\n\n\n\n\nImpute a few temporal missings using nearest neighbours."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-67-1",
    "href": "week3/slides.html#case-study-world-development-indicators-67-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (6/7)",
    "text": "Case study: World development indicators (6/7)\n\n\nMissings imputed using imputeTS using the moving average method.\n\n\n\n\n\n\n\n\n\n\n\nDon’t have to impute before scrutinizing data\nWhat are these numbers supposed to be?\n\nSE.PRM.CMPT.ZS is “Primary completion rate, total (% of relevant age group)”\nDo we have any problems?\n\nYes. The explanation of the variable suggests the numbers should range between 0-100."
  },
  {
    "objectID": "week3/slides.html#summary-of-the-process",
    "href": "week3/slides.html#summary-of-the-process",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "📋 Summary of the process",
    "text": "📋 Summary of the process\nThe steps we took roughly followed these:\n\n\n\n\n\n\n\n\nAt the end of this stage we would have:\n\n3 tables of data: country name/code, variables name/key, time series of multiple variables for many countries\nWhat would you like to learn from this data? What sort of models might be fitted? What types of hypotheses might be tested?\nHave we done anything that might have compromised the later analysis?"
  },
  {
    "objectID": "week3/slides.html#data-collection",
    "href": "week3/slides.html#data-collection",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data collection",
    "text": "Data collection"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-13",
    "href": "week3/slides.html#case-study-employment-data-in-australia-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (1/3)",
    "text": "Case study: Employment Data in Australia (1/3)\nBelow is the data from ABS that shows the total number of people employed in a given month from February 1976 to December 2019 using the original time series.\n\n\nload(here(\"data/employed.rda\"))\nglimpse(employed)\n\nRows: 557\nColumns: 4\n$ date  &lt;date&gt; 1978-02-01, 1978-03-01, 1978-04-01, 1978-05-01, 1978-06-01, 197…\n$ month &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1…\n$ year  &lt;dbl&gt; 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978…\n$ value &lt;dbl&gt; 5986, 6041, 6054, 6038, 6031, 6036, 6005, 6024, 6046, 6034, 6125…\n\n\nAustralian Bureau of Statistics, Labour force, Australia, Table 01. Labour force status by Sex, Australia - Trend, Seasonally adjusted and Original"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-23",
    "href": "week3/slides.html#case-study-employment-data-in-australia-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (2/3)",
    "text": "Case study: Employment Data in Australia (2/3)\n\n\nDo you notice anything?\n\n\n\n\n\n\n\n\n\n\n\nWhy do you think the number of people employed is going up each year?\n\n\n\nAustralian population is 25.39 million in 2019\n1.5% annual increase in population\nVic population is 6.681 million (Sep 2020) - 26%\nNSW population is 8.166 (Sep 2020) - 32%"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-33",
    "href": "week3/slides.html#case-study-employment-data-in-australia-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (3/3)",
    "text": "Case study: Employment Data in Australia (3/3)\n\n\n\nThere’s a suspicious change in August numbers from 2014.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA potential explanation for this is that there was a change in the survey from 2014.\n\nSee discussion on this at Hyndsight blog (10 October 2014)."
  },
  {
    "objectID": "week3/slides.html#case-study-2014-data-mining-cup-winners",
    "href": "week3/slides.html#case-study-2014-data-mining-cup-winners",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: 2014 Data Mining Cup winners",
    "text": "Case study: 2014 Data Mining Cup winners\n\n\n\nUgly plot of all observations provided in training sample, with response variable in colour, and test sample to predict.\nWhat does this tell you about the test sample?"
  },
  {
    "objectID": "week3/slides.html#case-study-french-frieshot-chips-12",
    "href": "week3/slides.html#case-study-french-frieshot-chips-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: french fries/hot chips (1/2)",
    "text": "Case study: french fries/hot chips (1/2)\n\n\n\n\nRows: 696\nColumns: 9\n$ time      &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ treatment &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ subject   &lt;fct&gt; 3, 3, 10, 10, 15, 15, 16, 16, …\n$ rep       &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, …\n$ potato    &lt;dbl&gt; 2.9, 14.0, 11.0, 9.9, 1.2, 8.8…\n$ buttery   &lt;dbl&gt; 0.0, 0.0, 6.4, 5.9, 0.1, 3.0, …\n$ grassy    &lt;dbl&gt; 0.0, 0.0, 0.0, 2.9, 0.0, 3.6, …\n$ rancid    &lt;dbl&gt; 0.0, 1.1, 0.0, 2.2, 1.1, 1.5, …\n$ painty    &lt;dbl&gt; 5.5, 0.0, 0.0, 0.0, 5.1, 2.3, …\n\n\n\n10 week sensory experiment, 12 individuals assessed taste of french fries on several scales (how potato-y, buttery, grassy, rancid, paint-y do they taste?), fried in one of 3 different oils, replicated twice.\n\nIs the design complete?\nAre replicates like each other?\nHow do the ratings on the different scales differ?\nAre raters giving different scores on average?\nDo ratings change over the weeks?"
  },
  {
    "objectID": "week3/slides.html#case-study-french-frieshot-chips-22",
    "href": "week3/slides.html#case-study-french-frieshot-chips-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: french fries/hot chips (2/2)",
    "text": "Case study: french fries/hot chips (2/2)\n\n\n\nIs the design complete?\n\n\nfrench_fries |&gt; count(subject)\n\n# A tibble: 12 × 2\n   subject     n\n   &lt;fct&gt;   &lt;int&gt;\n 1 3          54\n 2 10         60\n 3 15         60\n 4 16         60\n 5 19         60\n 6 31         54\n 7 51         60\n 8 52         60\n 9 63         60\n10 78         60\n11 79         54\n12 86         54\n\n\n\n\n\nfrench_fries |&gt; count(time)\n\n# A tibble: 10 × 2\n   time      n\n   &lt;fct&gt; &lt;int&gt;\n 1 1        72\n 2 2        72\n 3 3        72\n 4 4        72\n 5 5        72\n 6 6        72\n 7 7        72\n 8 8        72\n 9 9        60\n10 10       60\n\nfrench_fries |&gt; count(treatment)\n\n# A tibble: 3 × 2\n  treatment     n\n  &lt;fct&gt;     &lt;int&gt;\n1 1           232\n2 2           232\n3 3           232\n\nfrench_fries |&gt; count(rep)\n\n# A tibble: 2 × 2\n    rep     n\n  &lt;dbl&gt; &lt;int&gt;\n1     1   348\n2     2   348"
  },
  {
    "objectID": "week3/slides.html#case-study-warranty-claims",
    "href": "week3/slides.html#case-study-warranty-claims",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Warranty claims",
    "text": "Case study: Warranty claims\n\n\n\n\nRows: 4,561\nColumns: 14\n$ Region           &lt;chr&gt; \"East\", \"West\", \"North …\n$ State            &lt;chr&gt; \"Delhi\", \"Gujarat\", \"We…\n$ Area             &lt;chr&gt; \"Urban\", \"Rural\", \"Urba…\n$ City             &lt;chr&gt; \"New Delhi\", \"Ahmedabad…\n$ Consumer_profile &lt;chr&gt; \"Personal\", \"Personal\",…\n$ TV_2001_Issue    &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 1, 1,…\n$ TV_2002_Issue    &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1,…\n$ TV_2003_Issue    &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 0,…\n$ Claim_Value      &lt;dbl&gt; 25000, 4216, 4000, 5000…\n$ Service_Centre   &lt;dbl&gt; 13, 10, 10, 12, 10, 10,…\n$ Product_Age      &lt;dbl&gt; 60, 672, 275, 10, 4, 34…\n$ Purchased_from   &lt;chr&gt; \"Dealer\", \"Dealer\", \"De…\n$ Call_details     &lt;dbl&gt; 1.3, 25.0, 11.0, 1.6, 0…\n$ Purpose          &lt;chr&gt; \"Complaint\", \"Other\", \"…\n\n\n\nTV_2001_Issue: failure of power supply\nTV_2002_Issue: failure of inverter\nTV_2003_Issue: failure of motherboard\n\n\n\n\nWhat is the population that this data is measuring?\nWhat is not measured?\n\n\n\n\n# A tibble: 2 × 2\n  City          n\n  &lt;chr&gt;     &lt;int&gt;\n1 Delhi       106\n2 Bangalore   320\n\n\nCan we say that Delhi has fewer problems with TVs than Bangalore?\n\n\nSource: ExcelR Projects. (2019). Warranty Claims. Kaggle."
  },
  {
    "objectID": "week3/slides.html#summary-of-checks-for-data-collection",
    "href": "week3/slides.html#summary-of-checks-for-data-collection",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "📋 Summary of checks for data collection",
    "text": "📋 Summary of checks for data collection\n✅ Has the collection process been consistent?\n✅ Does the set to be predicted match the training set?\n✅ Is the experimental design correctly applied?\n✅ Have treatments been appropriately randomised or assigned comprehensively across subjects?\n✅ What is the population that the collected data describes?\n✅ If the data is observational, can you group them into comparison sets?"
  },
  {
    "objectID": "week3/slides.html#imputing-missing-values",
    "href": "week3/slides.html#imputing-missing-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Imputing missing values",
    "text": "Imputing missing values"
  },
  {
    "objectID": "week3/slides.html#example-1-olympic-medals",
    "href": "week3/slides.html#example-1-olympic-medals",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 1: Olympic medals",
    "text": "Example 1: Olympic medals\n\n\n\n\n             country totalmedal\n1       UnitedStates        104\n2              China         88\n3             Russia         82\n4       GreatBritain         65\n5            Germany         44\n6              Japan         38\n7          Australia         35\n8             France         34\n9         SouthKorea         28\n10             Italy         28\n11       Netherlands         20\n12           Ukraine         20\n13            Canada         18\n14           Hungary         17\n15             Spain         17\n16            Brazil         17\n17              Cuba         14\n18        Kazakhstan         13\n19        NewZealand         13\n20              Iran         12\n21           Jamaica         12\n22           Belarus         12\n23             Kenya         11\n24     CzechRepublic         10\n25            Poland         10\n26        Azerbaijan         10\n27           Romania          9\n28           Denmark          9\n29            Sweden          8\n30          Colombia          8\n31          Ethiopia          7\n32            Mexico          7\n33           Georgia          7\n34        NorthKorea          6\n35       SouthAfrica          6\n36           Croatia          6\n37             India          6\n38            Turkey          5\n39         Lithuania          5\n40           Ireland          5\n41          Mongolia          5\n42       Switzerland          4\n43            Norway          4\n44          Slovenia          4\n45            Serbia          4\n46         Argentina          4\n47        Uzbekistan          4\n48 TrinidadandTobago          4\n49          Slovakia          4\n50           Tunisia          3\n51          Thailand          3\n52           Finland          3\n53           Belgium          3\n54           Armenia          3\n55 DominicanRepublic          2\n56            Latvia          2\n57             Egypt          2\n58        PuertoRico          2\n59          Malaysia          2\n60         Indonesia          2\n61           Estonia          2\n62            Taiwan          2\n63          Bulgaria          2\n64         Singapore          2\n65             Qatar          2\n66           Moldova          2\n67            Greece          2\n68         Venezuela          1\n69            Uganda          1\n70           Grenada          1\n71           Bahamas          1\n72           Algeria          1\n73          Portugal          1\n74        Montenegro          1\n75         Guatemala          1\n76             Gabon          1\n77            Cyprus          1\n78          Botswana          1\n79        Tajikistan          1\n80       SaudiArabia          1\n81           Morocco          1\n82            Kuwait          1\n83          HongKong          1\n84           Bahrain          1\n85       Afghanistan          1\n\n\n Is the average number of medals equal to 962/85 = 11.32?\n\n\nWhat is missing?\n\n\nWhat is the correct average number of medals?\n\n\n962/204 = 4.72\n Working out what is missing can be hard!"
  },
  {
    "objectID": "week3/slides.html#example-2-el-nino",
    "href": "week3/slides.html#example-2-el-nino",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 2: El Nino",
    "text": "Example 2: El Nino\n\n\nExplore missings\n\nplotting on edge of plots, or\nusing simple imputation like mean\n\n\noceanbuoys |&gt;\n  ggplot(aes(x=air_temp_c, y=humidity)) +\n  geom_miss_point()\n\n\n\n\n\n\n\n\n\nImpute and check\n\nImpute using regression or simulation\nCheck distribution relative to complete cases\n\n\n\n\nCode\nlibrary(simputation)\nocean_imp_yr &lt;- oceanbuoys %&gt;%\n  bind_shadow() %&gt;%\n  impute_lm(air_temp_c ~ wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  impute_lm(humidity ~  wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  impute_lm(sea_temp_c ~  wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  add_label_shadow()\n\nggplot(ocean_imp_yr,\n       aes(x = air_temp_c,\n           y = humidity,\n           color = any_missing)) + \n  geom_point() +\n  theme(legend.title = element_blank())"
  },
  {
    "objectID": "week3/slides.html#validators",
    "href": "week3/slides.html#validators",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Validators",
    "text": "Validators\nAutomating some checks"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-13",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (1/3)",
    "text": "Case study: Dutch supermarket revenue and cost (1/3)\n\nData contains the revenue and cost (in Euros) for 60 supermarkets\nData has been anonymised and distorted\n\n\ndata(\"SBS2000\", package = \"validate\")\ndplyr::glimpse(SBS2000)\n\nRows: 60\nColumns: 11\n$ id          &lt;fct&gt; RET01, RET02, RET03, RET04, …\n$ size        &lt;fct&gt; sc0, sc3, sc3, sc3, sc3, sc0…\n$ incl.prob   &lt;dbl&gt; 0.02, 0.14, 0.14, 0.14, 0.14…\n$ staff       &lt;int&gt; 75, 9, NA, NA, NA, 1, 5, 3, …\n$ turnover    &lt;int&gt; NA, 1607, 6886, 3861, NA, 25…\n$ other.rev   &lt;int&gt; NA, NA, -33, 13, 37, NA, NA,…\n$ total.rev   &lt;int&gt; 1130, 1607, 6919, 3874, 5602…\n$ staff.costs &lt;int&gt; NA, 131, 324, 290, 314, NA, …\n$ total.costs &lt;int&gt; 18915, 1544, 6493, 3600, 553…\n$ profit      &lt;int&gt; 20045, 63, 426, 274, 72, 3, …\n$ vat         &lt;int&gt; NA, NA, NA, NA, NA, NA, 1346…"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-23",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (2/3)",
    "text": "Case study: Dutch supermarket revenue and cost (2/3)\n\nChecking for completeness of records\n\n\nlibrary(validate)\nrules &lt;- validator(\n          is_complete(id),\n          is_complete(id, turnover),\n          is_complete(id, turnover, profit))\nout &lt;- confront(SBS2000, rules)\nsummary(out)\n\n  name items passes fails nNA error warning\n1   V1    60     60     0   0 FALSE   FALSE\n2   V2    60     56     4   0 FALSE   FALSE\n3   V3    60     52     8   0 FALSE   FALSE\n                         expression\n1                   is_complete(id)\n2         is_complete(id, turnover)\n3 is_complete(id, turnover, profit)"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-33",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (3/3)",
    "text": "Case study: Dutch supermarket revenue and cost (3/3)\n\nSanity check derived variables\n\n\nlibrary(validate)\nrules &lt;- validator(\n    total.rev - profit == total.costs,\n    turnover + other.rev == total.rev,\n    profit &lt;= 0.6 * total.rev\n)\nout &lt;- confront(SBS2000, rules)\nsummary(out)\n\n  name items passes fails nNA error warning\n1   V1    60     39    14   7 FALSE   FALSE\n2   V2    60     19     4  37 FALSE   FALSE\n3   V3    60     49     6   5 FALSE   FALSE\n                                      expression\n1 abs(total.rev - profit - total.costs) &lt;= 1e-08\n2 abs(turnover + other.rev - total.rev) &lt;= 1e-08\n3              profit - 0.6 * total.rev &lt;= 1e-08"
  },
  {
    "objectID": "week3/slides.html#ida-for-hypothesis-testing",
    "href": "week3/slides.html#ida-for-hypothesis-testing",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA for hypothesis testing",
    "text": "IDA for hypothesis testing"
  },
  {
    "objectID": "week3/slides.html#hypothesis-testing-13",
    "href": "week3/slides.html#hypothesis-testing-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hypothesis testing (1/3)",
    "text": "Hypothesis testing (1/3)\n\nState the hypothesis (pair), e.g. \\(H_o: \\mu_1 = \\mu_2\\) vs \\(H_a: \\mu_1 &lt; \\mu_2\\).\nTest statistic depends on assumption about the distribution, e.g. \n\n\\(t\\)-test will assume that distributions are normal, or small departures from if we have a large sample.\ntwo-sample might assume both groups have the same variance\n\n\n\n\nSteps to complete:\n\nCompute the test statistic\nMeasure it against a standard distribution\nIf it is extreme, \\(p\\)-value is small, decision is to reject \\(H_o\\)\n\\(p\\)-value is the probability of observing a value as large as this, or large, assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week3/slides.html#example-1-checking-variance-and-distribution-23",
    "href": "week3/slides.html#example-1-checking-variance-and-distribution-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 1: Checking variance and distribution (2/3)",
    "text": "Example 1: Checking variance and distribution (2/3)\n\n\n\n\n\nCode\ndata(sleep)\nggplot(sleep, aes(x=group, y=extra)) + \n  geom_boxplot() +\n  geom_point(colour=\"#D55E00\")\n\n\n\n\n\n\n\n\n\n\nCushny, A. R. and Peebles, A. R. (1905) The action of optical isomers: II hyoscines. The Journal of Physiology 32, 501–510.\n\n\nFew observations. Nothing strongly suggests violation of normality and spread of points is similar for each group.\n\ntt &lt;- with(sleep,\n     t.test(extra[group == 1],\n            extra[group == 2], \n            paired = TRUE))\ntt\n\n\n    Paired t-test\n\ndata:  extra[group == 1] and extra[group == 2]\nt = -4, df = 9, p-value = 0.003\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.5 -0.7\nsample estimates:\nmean difference \n           -1.6"
  },
  {
    "objectID": "week3/slides.html#example-2-checking-distribution-and-variance-33",
    "href": "week3/slides.html#example-2-checking-distribution-and-variance-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 2: Checking distribution and variance (3/3)",
    "text": "Example 2: Checking distribution and variance (3/3)\n\n\n\n\n\nCode\nInsectSprays  |&gt; \n  ggplot(aes(x=fct_reorder(spray, count), \n             y=count)) + \n  geom_jitter(width=0.1, height=0, colour=\"#D55E00\", size=3, alpha=0.8) +\n  xlab(\"\") \n\n\n\n\n\n\n\n\n\n\nIs it plausible that the samples are from a normal population? Do they have equal variance?\n\n\n\nfm1 &lt;- aov(count ~ spray, data = InsectSprays)\nsummary(fm1)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nspray        5   2669     534    34.7 &lt;2e-16 ***\nResiduals   66   1015      15                   \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhat hypothesis being tested? What would the decision be?\n\n\nWhy does equal variance matter in this test?"
  },
  {
    "objectID": "week3/slides.html#ida-for-inferential-modeling",
    "href": "week3/slides.html#ida-for-inferential-modeling",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA for inferential modeling",
    "text": "IDA for inferential modeling"
  },
  {
    "objectID": "week3/slides.html#linear-models-13",
    "href": "week3/slides.html#linear-models-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (1/3)",
    "text": "Linear models (1/3)\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nggplot(cars, aes(speed, dist)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + e_i\\]\nAssumptions:\n\nForm is linear\nError is normally distributed around 0\n\n\n\nCheck using residual plots\n\n\n\nCode\ncars_model &lt;- lm(dist ~ speed, data = cars)\ncars_fit &lt;- augment(cars_model)\n\ncars_p1 &lt;- ggplot(cars_fit, aes(x=.fitted, \n                                y=.resid)) + \n  geom_hline(yintercept = 0, colour=\"grey70\") +\n  geom_point() \ncars_p2 &lt;- ggplot(cars_fit, aes(x=.resid)) +\n  geom_density()\ncars_p1 + cars_p2 + plot_layout(ncol=2)"
  },
  {
    "objectID": "week3/slides.html#linear-models-23",
    "href": "week3/slides.html#linear-models-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (2/3)",
    "text": "Linear models (2/3)\n\n\n\nData and loess smoother\n\n\nCode\nggplot(diamonds, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(se=F)\n\n\n\n\n\n\n\n\n\n\nForm is not linear!\nAlso, insufficient data on large diamonds.\n\n\n\nFix 1: fit polynomial form\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + e_i.\\]\n\n\nCode\ndiamonds_sub &lt;- diamonds |&gt;\n  filter(carat &lt; 3)\n\ndiamonds_model &lt;- lm(price ~ poly(carat, 2),\n                     data=diamonds_sub)\ndiamonds_fit &lt;- diamonds_sub |&gt;\n  mutate(.fitted = diamonds_model$fitted.values, \n         .resid = diamonds_model$residuals)\n\ndiamonds_p1 &lt;- ggplot(diamonds_fit) +\n  geom_point(aes(x=carat, y=price)) +\n  geom_point(aes(x=carat, y=.fitted),\n             colour=\"#D55E00\")\ndiamonds_p2 &lt;- ggplot(diamonds_fit, \n                      aes(x=.fitted, \n                          y=.resid)) + \n  geom_hline(yintercept = 0, colour=\"grey70\") +\n  geom_point() \ndiamonds_p1 + diamonds_p2 + plot_layout(ncol=2)\n\n\n\n\n\n\n\n\n\n\nForm is not quadratic, continue to explore additional polynomial terms."
  },
  {
    "objectID": "week3/slides.html#linear-models-33",
    "href": "week3/slides.html#linear-models-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (3/3)",
    "text": "Linear models (3/3)\n\n\n\nData and loess smoother\n\n\nCode\nggplot(diamonds, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(se=F)\n\n\n\n\n\n\n\n\n\n\nForm is not linear!\nAlso, insufficient data on large diamonds.\n\n\n\nFix 2: linearise\n\nThe log transformation of both variables linearises the relationship, so that a simple linear model can be used, and can correct heteroskedasticity.\n\n\nCode\nggplot(diamonds_sub, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(method = lm) +\n  scale_x_log10() +\n  scale_y_log10()"
  },
  {
    "objectID": "week3/slides.html#cautions",
    "href": "week3/slides.html#cautions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Cautions",
    "text": "Cautions\n\nNotice that there was no formal statistical inference when trying to determine an appropriate model form.\n\n\nDiscarded models are hardly ever reported. Consequently, majority of reported statistics give a distorted view and it’s important to remind yourself what might not be reported."
  },
  {
    "objectID": "week3/slides.html#summary",
    "href": "week3/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\n\n\nIDA is a model-focused exploration to support a CDA with:\n\ndata description and collection\ndata quality checking, and\nchecking assumptions\nmodel fit without any formal statistical inference.\n\nIDA is part of EDA, even when no CDA is planned.\n\nIDA may never see the limelight BUT it forms the foundation that the main analysis is built upon. Document it! Do it well!\n\n\n\n\nThe Census Bureau tabulates same-sex couples in both the American Community Survey (ACS) and the Decennial Census. Two questions are used to identify same-sex couples: relationship and sex. The agency follows edit rules that are used to change data values for seemingly contradictory answers. The edit rules for combining information from relationship and sex have evolved since the category of unmarried partner was added in 1990. In that census, if a household consisted of a married couple and both spouses reported the same sex, the relationship category remained husband or wife, but the sex of the partner who reported being a spouse to the householder was changed. Humans all the way down\n\n\n\n\nHuman actions are ubiquitous in every part of data analysis! The most objective methods often have had subjective actions before and after."
  },
  {
    "objectID": "week3/slides.html#further-reading",
    "href": "week3/slides.html#further-reading",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Further reading",
    "text": "Further reading\n\nHuebner et al (2018) A Contemporary Conceptual Framework for Initial Data Analysis\nHuebner et al (2020) Hidden analyses\nChatfield (1985) The Initial Examination of Data. Journal of the Royal Statistical Society. Series A (General) 148 \nCox & Snell (1981) Applied Statistics. London: Chapman and Hall.\nvan der Loo and de Jonge (2018). Statistical Data Cleaning with Applications in R. John Wiley and Sons Ltd.\nHyndman (2014) Explaining the ABS unemployment fluctuations"
  },
  {
    "objectID": "week3/tutorial.html",
    "href": "week3/tutorial.html",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week3/tutorial.html#objectives",
    "href": "week3/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week3/tutorial.html#preparation",
    "href": "week3/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 3",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is The initial examination of data. It is authored by Chris Chatfield, and is a classic paper explaining the role of initial data analysis.\n\nComplete the weekly quiz, before the deadline!\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week3/tutorial.html#exercises",
    "href": "week3/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 3",
    "section": "📥 Exercises",
    "text": "📥 Exercises\nToday, you have sprint competition to discover as many problems as possible in this data, cafe.rda.\nA small cafe in the city of Melbourne is interested in determining whether the daily earnings depend on the weather. They compiled data for a period over 2000-2001 to study this question. The data has the following variables:\n\n\n\n \n  \n    var \n    description \n  \n \n\n  \n    dt \n    Date \n  \n  \n    wday \n    Day of the week \n  \n  \n    revenue \n    Daily revenue in hundreds, 11=1100 \n  \n  \n    expend \n    Daily expenses in hundreds \n  \n  \n    precip \n    Precipitation in mm \n  \n  \n    mint \n    Minimum temperature, Celsius \n  \n  \n    maxt \n    Maximum temperature, Celsius \n  \n  \n    source \n    Source of the weather data \n  \n\n\n\n\n\nGuidelines\n\nUse whatever R package or software or tool you’d like, and report how you found the error.\nFeel free to buddy up, and work with another student in your tutorial session.\nNo cheating! This time do the assignment without AI help, or searching for answers on the web.\nPlease don’t ruin the later tutorials by sharing your work!\n\nThe most complete list with best process for findings, as decided by your tutor, wins the prize! Your tutor’s decision is final!"
  },
  {
    "objectID": "week3/tutorial.html#finishing-up",
    "href": "week3/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 3",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/worksheet.html",
    "href": "week3/worksheet.html",
    "title": "ETC5521 Worksheet Week 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\ninstall.packages(c(\"tidyverse\", \"ggbeeswarm\", \"broom\", \"visdat\"))"
  },
  {
    "objectID": "week3/worksheet.html#objectives",
    "href": "week3/worksheet.html#objectives",
    "title": "ETC5521 Worksheet Week 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\ninstall.packages(c(\"tidyverse\", \"ggbeeswarm\", \"broom\", \"visdat\"))"
  },
  {
    "objectID": "week3/worksheet.html#tasks",
    "href": "week3/worksheet.html#tasks",
    "title": "ETC5521 Worksheet Week 3",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. Take a glimpse of the penguins data. What types are variables are present in the data?\n\n\n2. How was this data collected? You will need to read the documentation for the palmerpenguins package, or see if AI knows.\n\n\n3. Using the visdat package make an overview plot to examine types of variables and for missing values.\n\n\n4. Check the distributions of each species on each of the size variables, using a jittered dotplot, using the geom_quasirandom() function in the ggbeeswarm package. There seems to be some bimodality in some species on some variables eg bill_len. Why do you think this might be? Check your thinking by making a suitable plot.\n\n\n5. Is there any indication of outliers from the jittered dotplots of different variables?\n\n\n6. Make a scatterplot of body_mass_g vs flipper_length_mm for all the penguins. What do the vertical stripes indicate? Are there any other unusual patterns to note, such as outliers or clustering or nonlinearity?\n\n\n7. How well can penguin body mass be predicted based on the flipper length? Fit a linear model to check. Report the equation, the \\(R^2\\), \\(\\sigma\\), and make a residual plot of residuals vs flipper_length_mm. From the residual plot, are there any concerns about the model fit?"
  },
  {
    "objectID": "week3/index.html#worksheet",
    "href": "week3/index.html#worksheet",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Worksheet",
    "text": "Worksheet\n\nqmd\nhtml"
  },
  {
    "objectID": "week3/tutorialsol.html",
    "href": "week3/tutorialsol.html",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week3/tutorialsol.html#objectives",
    "href": "week3/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week3/tutorialsol.html#preparation",
    "href": "week3/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 3",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is The initial examination of data. It is authored by Chris Chatfield, and is a classic paper explaining the role of initial data analysis.\n\nComplete the weekly quiz, before the deadline!\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week3/tutorialsol.html#exercises",
    "href": "week3/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 3",
    "section": "📥 Exercises",
    "text": "📥 Exercises\nToday, you have sprint competition to discover as many problems as possible in this data, cafe.rda.\nA small cafe in the city of Melbourne is interested in determining whether the daily earnings depend on the weather. They compiled data for a period over 2000-2001 to study this question. The data has the following variables:\n\n\n\n \n  \n    var \n    description \n  \n \n\n  \n    dt \n    Date \n  \n  \n    wday \n    Day of the week \n  \n  \n    revenue \n    Daily revenue in hundreds, 11=1100 \n  \n  \n    expend \n    Daily expenses in hundreds \n  \n  \n    precip \n    Precipitation in mm \n  \n  \n    mint \n    Minimum temperature, Celsius \n  \n  \n    maxt \n    Maximum temperature, Celsius \n  \n  \n    source \n    Source of the weather data \n  \n\n\n\n\n\nGuidelines\n\nUse whatever R package or software or tool you’d like, and report how you found the error.\nFeel free to buddy up, and work with another student in your tutorial session.\nNo cheating! This time do the assignment without AI help, or searching for answers on the web.\nPlease don’t ruin the later tutorials by sharing your work!\n\nThe most complete list with best process for findings, as decided by your tutor, wins the prize! Your tutor’s decision is final!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nYour tutor has the solution!"
  },
  {
    "objectID": "week3/tutorialsol.html#finishing-up",
    "href": "week3/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 3",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/worksheetsol.html",
    "href": "week3/worksheetsol.html",
    "title": "ETC5521 Worksheet Week 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\ninstall.packages(c(\"tidyverse\", \"ggbeeswarm\", \"broom\", \"visdat\"))"
  },
  {
    "objectID": "week3/worksheetsol.html#objectives",
    "href": "week3/worksheetsol.html#objectives",
    "title": "ETC5521 Worksheet Week 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\ninstall.packages(c(\"tidyverse\", \"ggbeeswarm\", \"broom\", \"visdat\"))"
  },
  {
    "objectID": "week3/worksheetsol.html#tasks",
    "href": "week3/worksheetsol.html#tasks",
    "title": "ETC5521 Worksheet Week 3",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. Take a glimpse of the penguins data. What types are variables are present in the data?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nglimpse(penguins)\n\n\nRows: 344\nColumns: 8\n$ species     &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ad…\n$ island      &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Tor…\n$ bill_len    &lt;dbl&gt; 39, 40, 40, NA, 37, 39, 39, 39, 34, 42, 38, 38, 41, 39, 35…\n$ bill_dep    &lt;dbl&gt; 19, 17, 18, NA, 19, 21, 18, 20, 18, 20, 17, 17, 18, 21, 21…\n$ flipper_len &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180,…\n$ body_mass   &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, …\n$ sex         &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, …\n$ year        &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\n\n\n\n2. How was this data collected? You will need to read the documentation for the palmerpenguins package, or see if AI knows.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nDetails are at https://allisonhorst.github.io/palmerpenguins/articles/intro.html, and you learn “These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. The data were imported directly from the Environmental Data Initiative (EDI) Data Portal.” It is necessary to also read the original data collection article https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081 to obtain details of the sampling. Breeding pairs of penguins were included based on sampling of nests where pairs of adults were present, were chosen and marked, before onset of egg-laying.\n\n\n\n\n\n\n3. Using the visdat package make an overview plot to examine types of variables and for missing values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThere are three factor variables - species, island, sex - and three integer variables - flipper_len, body_mass and year - and two numeric variables - bill_len and bill_dep.\nIt is interesting that flipper_len and body_mass are reported without decimal places, and thus are integers, whereas bill_len and bill_dep are reported with one decimal place, and thus are doubles. Both are numeric variables, though.\nFour variables have missing values: sex, bill_len, bill_dep, flipper_len, body_mass. There are more missings on sex. The other missings occur together, the penguins are missing on all five variables.\n\n\nCode\nlibrary(visdat)\nvis_dat(penguins)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Check the distributions of each species on each of the size variables, using a jittered dotplot, using the geom_quasirandom() function in the ggbeeswarm package. There seems to be some bimodality in some species on some variables eg bill_len. Why do you think this might be? Check your thinking by making a suitable plot.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nParticularly on bill_len multimodality can be seen in the Chinstrap and Gentoo penguins. This corresponds to differences in the two species. Differences can be seen in the sexes for all of the variables and species, but it is only big enough in bill_len to be noticeable as bimodality.\n\n\nCode\nlibrary(ggbeeswarm)\nggplot(penguins, aes(x=species, \n                     y=bill_len)) +\n  geom_quasirandom() \n\n\n\n\n\n\n\n\n\nCode\nggplot(penguins, aes(x=species, \n                     y=bill_len, \n                     colour=sex)) +\n  geom_quasirandom() +\n  scale_color_brewer(\"\", palette=\"Dark2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Is there any indication of outliers from the jittered dotplots of different variables?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nOutliers can be seen on bill_len for Chinstrap and Gentoo. Interestingly, there is one female penguins with a really big bill, bigger than all the males even.\nAlso on flipper_len there is one female penguins with a much smaller value than others.\n\n\n\n\n\n\n6. Make a scatterplot of body_mass_g vs flipper_length_mm for all the penguins. What do the vertical stripes indicate? Are there any other unusual patterns to note, such as outliers or clustering or nonlinearity?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe striping corresponds to rounded values of flipper length, that have been reported to the nearest mm. It appears that it is done across all measuring as it is present for both sexes, for all species, for each island and each year. Otherwise there is nothing much to report as unusual.\n\n\nCode\npenguins |&gt;\n  na.omit() |&gt;\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nCode\npenguins |&gt;\n  na.omit() |&gt;\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~island, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nCode\npenguins |&gt;\n  na.omit() |&gt;\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~sex, ncol=2, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nCode\npenguins |&gt;\n  na.omit() |&gt;\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~species, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nCode\npenguins |&gt;\n  na.omit() |&gt;\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~year, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\n⚠️ Note: That using na.omit() is VERY DANGEROUS. You should very rarely use it and only in very clear situations like this data.\n\n\n\n\n\n\n7. How well can penguin body mass be predicted based on the flipper length? Fit a linear model to check. Report the equation, the \\(R^2\\), \\(\\sigma\\), and make a residual plot of residuals vs flipper_length_mm. From the residual plot, are there any concerns about the model fit?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe model fit statistics suggest it is a reasonably good model, with flipper length explaining about 76% of body mass. From the estimated standard deviation of the error, \\(\\sigma=393\\), we could say that the estimated body mass is likely accurate to within 800g. (Assuming normal distribution and 95% of observations within two \\(\\sigma\\).)\nThe residual suggests no major problems. There is a little heteroskedasticity. Perhaps if you look carefully, though, it might indicate that different models should have been fitted for the smaller penguins and the bigger penguins, perhaps models separately for sex and species would be advised.\n\n\nCode\nlibrary(broom)\npenguins_nona &lt;- penguins |&gt;\n  na.omit()\npenguins_fit &lt;- lm(body_mass~flipper_len, data=penguins_nona)\ntidy(penguins_fit)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -5872.     310.       -18.9 1.18e- 54\n2 flipper_len     50.2      1.54      32.6 3.13e-105\n\n\nCode\nglance(penguins_fit)\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.762         0.761  393.     1060. 3.13e-105     1 -2461. 4928. 4940.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nCode\npenguins_m &lt;- augment(penguins_fit)\nggplot(penguins_m, aes(x=flipper_len, y=.resid)) +\n  geom_hline(yintercept=1, colour=\"grey70\") +\n  geom_point() +\n  theme(aspect.ratio=1)"
  },
  {
    "objectID": "week4/tutorialsol.html",
    "href": "week4/tutorialsol.html",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorialsol.html#objectives",
    "href": "week4/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorialsol.html#preparation",
    "href": "week4/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 3",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is The initial examination of data. It is authored by Chris Chatfield, and is a classic paper explaining the role of initial data analysis. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"ggbeeswarm\", \"broom\", \"nullabor\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week4/tutorialsol.html#exercises",
    "href": "week4/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 3",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: IDA on penguins data\n\nTake a glimpse of the penguins data. What types are variables are present in the data?\nHow was this data collected? You will need to read the documentation for the palmerpenguins package.\nUsing the visdat package make an overview plot to examine types of variables and for missing values.\nCheck the distributions of each species on each of the size variables, using a jittered dotplot, using the geom_quasirandom() function in the ggbeeswarm package. There seems to be some bimodality in some species on some variables eg bill_len. Why do you think this might be? Check your thinking by making a suitable plot.\nIs there any indication of outliers from the jittered dotplots of different variables?\nMake a scatterplot of body_mass_g vs flipper_length_mm for all the penguins. What do the vertical stripes indicate? Are there any other unusual patterns to note, such as outliers or clustering or nonlinearity?\nHow well can penguin body mass be predicted based on the flipper length? Fit a linear model to check. Report the equation, the \\(R^2\\), \\(\\sigma\\), and make a residual plot of residuals vs flipper_length_mm. From the residual plot, are there any concerns about the model fit?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species     &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ad…\n$ island      &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Tor…\n$ bill_len    &lt;dbl&gt; 39, 40, 40, NA, 37, 39, 39, 39, 34, 42, 38, 38, 41, 39, 35…\n$ bill_dep    &lt;dbl&gt; 19, 17, 18, NA, 19, 21, 18, 20, 18, 20, 17, 17, 18, 21, 21…\n$ flipper_len &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180,…\n$ body_mass   &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, …\n$ sex         &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, …\n$ year        &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\nDetails are at https://allisonhorst.github.io/palmerpenguins/articles/intro.html, and you learn “These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. The data were imported directly from the Environmental Data Initiative (EDI) Data Portal.” It is necessary to also read the original data collection article https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081 to obtain details of the sampling. Breeding pairs of penguins were included based on sampling of nests where pairs of adults were present, were chosen and marked, before onset of egg-laying.\nThere are three factor variables - species, island, sex - and three integer variables - flipper_len, body_mass and year - and two numeric variables - bill_len and bill_dep.\n\nIt is interesting that flipper_len and body_mass are reported without decimal places, and thus are integers, whereas bill_len and bill_dep are reported with one decimal place, and thus are doubles. Both are numeric variables, though.\nFour variables have missing values: sex, bill_len, bill_dep, flipper_len, body_mass. There are more missings on sex. The other missings occur together, the penguins are missing on all five variables.\n\nlibrary(visdat)\nvis_dat(penguins)\n\n\n\n\n\n\n\n\n\nParticularly on bill_len multimodality can be seen in the Chinstrap and Gentoo penguins. This corresponds to differences in the two species. Differences can be seen in the sexes for all of the variables and species, but it is only big enough in bill_len to be noticeable as bimodality.\n\n\nlibrary(ggbeeswarm)\nggplot(penguins, aes(x=species, \n                     y=bill_len)) +\n  geom_quasirandom() \n\n\n\n\n\n\n\nggplot(penguins, aes(x=species, \n                     y=bill_len, \n                     colour=sex)) +\n  geom_quasirandom() +\n  scale_color_brewer(\"\", palette=\"Dark2\")\n\n\n\n\n\n\n\n\n\nOutliers can be seen on bill_len for Chinstrap and Gentoo. Interestingly, there is one female penguins with a really big bill, bigger than all the males even.\n\nAlso on flipper_len there is one female penguins with a much smaller value than others.\n\nThe striping corresponds to rounded values of flipper length, that have been reported to the nearest mm. It appears that it is done across all measuring as it is present for both sexes, for all species, for each island and each year. Otherwise there is nothing much to report as unusual.\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~island, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~sex, ncol=2, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~species, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~year, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nThe model fit statistics suggest it is a reasonably good model, with flipper length explaining about 76% of body mass. From the estimated standard deviation of the error, \\(\\sigma=393\\), we could say that the estimated body mass is likely accurate to within 800g. (Assuming normal distribution and 95% of observations within two \\(\\sigma\\).)\n\nThe residual suggests no major problems. There is a little heteroskedasticity. Perhaps if you look carefully, though, it might indicate that different models should have been fitted for the smaller penguins and the bigger penguins, perhaps models separately for sex and species would be advised.\n\nlibrary(broom)\npenguins_nona &lt;- penguins %&gt;%\n  na.omit()\npenguins_fit &lt;- lm(body_mass~flipper_len, data=penguins_nona)\ntidy(penguins_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -5872.     310.       -18.9 1.18e- 54\n2 flipper_len     50.2      1.54      32.6 3.13e-105\n\nglance(penguins_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.762         0.761  393.     1060. 3.13e-105     1 -2461. 4928. 4940.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\npenguins_m &lt;- augment(penguins_fit)\nggplot(penguins_m, aes(x=flipper_len, y=.resid)) +\n  geom_hline(yintercept=1, colour=\"grey70\") +\n  geom_point() +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Can we believe what we see?\nThis question uses material from this week’s lecture, from a few hours ago.\n\nIn the previous question we made subjective statements about the residual plot to determine if the model was a good fit or not. We’ll use randomisation to check any observations we made from the residual plot. The code below makes a lineup of the true plot against plots made with rotation residuals (nulls/good). When you run the code you will get a line decrypt(\"....\"), which you can copy and paste back in to the console window to get the location of the true plot (in case you forgot which it is). Does the true plot look like the null plots? If not, describe how it differs.\n\n\nlibrary(nullabor)\nggplot(lineup(null_lm(body_mass~flipper_len, method=\"rotate\"),\n              penguins_m),\n       aes(x=flipper_len, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n\nPick one group, males or females, and one of Adelie, Chinstrap or Gentoo, and choose two of the four measurements. Fit a linear model, and do a lineup of the residuals. Can you tell which is the true plot? Show your lineup to your tutorial partner or someone else nearby and ask them\n\n\nto pick the plot that is most different.\nexplain why they picked that plot.\n\nUsing your decrypt() code locate the true plot. Is the true plot different from the nulls?\nDid you or your friend choose the data plot? Was it identifiable from the lineup or indistinguishable from the null plots?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe true plot looks a little different from the nulls. It has more of a V shape, which might suggest that the model fits poorly, that the smaller penguins have a different relationship than the larger penguins. It is evidence to support fitting separate models to the sexes and species.\nSomething like the following\n\n\nlibrary(nullabor)\npenguins_f_adelie &lt;- penguins_nona %&gt;%\n  filter(species == \"Adelie\",\n         sex == \"female\")\npenguins_f_adelie_bl_bd_fit &lt;- \n  lm(bill_dep ~ bill_len,\n     data=penguins_f_adelie)\npenguins_f_adelie_bl_bd_m &lt;-\n  augment(penguins_f_adelie_bl_bd_fit)\nggplot(lineup(null_lm(bill_dep ~ \n                        bill_len,\n                      method=\"rotate\"),\n              penguins_f_adelie_bl_bd_m),\n       aes(x=bill_len, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n\n\n\n\n\n\n\nI would not be able to distinguish which is the true plot in this lineup."
  },
  {
    "objectID": "week4/tutorialsol.html#finishing-up",
    "href": "week4/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 3",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week4/tutorial.html",
    "href": "week4/tutorial.html",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorial.html#objectives",
    "href": "week4/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorial.html#preparation",
    "href": "week4/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 3",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is The initial examination of data. It is authored by Chris Chatfield, and is a classic paper explaining the role of initial data analysis. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"ggbeeswarm\", \"broom\", \"nullabor\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week4/tutorial.html#exercises",
    "href": "week4/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 3",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: IDA on penguins data\n\nTake a glimpse of the penguins data. What types are variables are present in the data?\nHow was this data collected? You will need to read the documentation for the palmerpenguins package.\nUsing the visdat package make an overview plot to examine types of variables and for missing values.\nCheck the distributions of each species on each of the size variables, using a jittered dotplot, using the geom_quasirandom() function in the ggbeeswarm package. There seems to be some bimodality in some species on some variables eg bill_len. Why do you think this might be? Check your thinking by making a suitable plot.\nIs there any indication of outliers from the jittered dotplots of different variables?\nMake a scatterplot of body_mass_g vs flipper_length_mm for all the penguins. What do the vertical stripes indicate? Are there any other unusual patterns to note, such as outliers or clustering or nonlinearity?\nHow well can penguin body mass be predicted based on the flipper length? Fit a linear model to check. Report the equation, the \\(R^2\\), \\(\\sigma\\), and make a residual plot of residuals vs flipper_length_mm. From the residual plot, are there any concerns about the model fit?\n\n\n\nExercise 2: Can we believe what we see?\nThis question uses material from this week’s lecture, from a few hours ago.\n\nIn the previous question we made subjective statements about the residual plot to determine if the model was a good fit or not. We’ll use randomisation to check any observations we made from the residual plot. The code below makes a lineup of the true plot against plots made with rotation residuals (nulls/good). When you run the code you will get a line decrypt(\"....\"), which you can copy and paste back in to the console window to get the location of the true plot (in case you forgot which it is). Does the true plot look like the null plots? If not, describe how it differs.\n\n\nlibrary(nullabor)\nggplot(lineup(null_lm(body_mass~flipper_len, method=\"rotate\"),\n              penguins_m),\n       aes(x=flipper_len, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n\nPick one group, males or females, and one of Adelie, Chinstrap or Gentoo, and choose two of the four measurements. Fit a linear model, and do a lineup of the residuals. Can you tell which is the true plot? Show your lineup to your tutorial partner or someone else nearby and ask them\n\n\nto pick the plot that is most different.\nexplain why they picked that plot.\n\nUsing your decrypt() code locate the true plot. Is the true plot different from the nulls?\nDid you or your friend choose the data plot? Was it identifiable from the lineup or indistinguishable from the null plots?"
  },
  {
    "objectID": "week4/tutorial.html#finishing-up",
    "href": "week4/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 3",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  }
]