[
  {
    "objectID": "week9/index.html",
    "href": "week9/index.html",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "",
    "text": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R"
  },
  {
    "objectID": "week9/index.html#main-reference",
    "href": "week9/index.html#main-reference",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "",
    "text": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R"
  },
  {
    "objectID": "week9/index.html#what-you-will-learn-this-week",
    "href": "week9/index.html#what-you-will-learn-this-week",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week9/index.html#lecture-slides",
    "href": "week9/index.html#lecture-slides",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week9/index.html#tutorial-instructions",
    "href": "week9/index.html#tutorial-instructions",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week9/index.html#assignments",
    "href": "week9/index.html#assignments",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week9/index.html#assignments-1",
    "href": "week9/index.html#assignments-1",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 3 is due on Monday 16 September.\nAssignment 4 Part 1 is due on Monday 07 October.\nAssignment 4 Part 2 is due on Monday 28 October."
  },
  {
    "objectID": "week7/index.html",
    "href": "week7/index.html",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "",
    "text": "Wilke (2019) Ch 9, 10.2-4, 11.2"
  },
  {
    "objectID": "week7/index.html#main-reference",
    "href": "week7/index.html#main-reference",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "",
    "text": "Wilke (2019) Ch 9, 10.2-4, 11.2"
  },
  {
    "objectID": "week7/index.html#what-you-will-learn-this-week",
    "href": "week7/index.html#what-you-will-learn-this-week",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week7/index.html#lecture-slides",
    "href": "week7/index.html#lecture-slides",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week7/index.html#tutorial-instructions",
    "href": "week7/index.html#tutorial-instructions",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week7/index.html#assignments",
    "href": "week7/index.html#assignments",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week7/index.html#assignments-1",
    "href": "week7/index.html#assignments-1",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 3 is due on Monday 16 September."
  },
  {
    "objectID": "week6/slides.html#the-world-is-full-of-obvious-things-which-nobody-by-any-chance-observes",
    "href": "week6/slides.html#the-world-is-full-of-obvious-things-which-nobody-by-any-chance-observes",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "The world is full of obvious things which nobody by any chance observes",
    "text": "The world is full of obvious things which nobody by any chance observes\n Quote from Arthur Conan Doyle, The Hound of the Baskervilles"
  },
  {
    "objectID": "week6/slides.html#the-story-of-the-galloping-horse",
    "href": "week6/slides.html#the-story-of-the-galloping-horse",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "The story of the galloping horse",
    "text": "The story of the galloping horse\nGalloping horses throughout history were drawn with all four legs out.  \n\n\n\n\n\n\n\n\n\n\n\nBaronet, 1794\n\n\nDerby D‚ÄôEpsom 1821\n\n\n\n \nRead more in Lankester: The Problem of the Galloping Horse"
  },
  {
    "objectID": "week6/slides.html#the-story-of-the-galloping-horse-1",
    "href": "week6/slides.html#the-story-of-the-galloping-horse-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "The story of the galloping horse",
    "text": "The story of the galloping horse\nWith the birth of photography, and particular motion photography, Muybridge was able to illustrate that all four legs are never extended simultaneously.\n\n\n\n\n\n\nSource: wikimedia\n\n\n\n\n\n\nSource: wikimedia"
  },
  {
    "objectID": "week6/slides.html#an-evolution-in-seeing-the-world-13",
    "href": "week6/slides.html#an-evolution-in-seeing-the-world-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "An evolution in seeing the world (1/3)",
    "text": "An evolution in seeing the world (1/3)\n\nhills - beginnerhills - terrible fixhills - what do you see\n\n\n\n\n\n\n Mrs Robinson says Hills are more interesting than that. Usually you can see valleys and shadows.\n\n\nso I put some shadows on.\n \n\n\n\n\n\nMrs Robinson looks horrified!\nGo outside and take a look at the hills in the distance"
  },
  {
    "objectID": "week6/slides.html#an-evolution-in-seeing-the-world-23",
    "href": "week6/slides.html#an-evolution-in-seeing-the-world-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "An evolution in seeing the world (2/3)",
    "text": "An evolution in seeing the world (2/3)\n\n\nDrawing lemons\n\n\n\n\n\nIs something is missing?\n\n\n\n\n\n\nMy sketch\n\n\n\n\n\n\n\nNotice the yellow reflection(s) and shine on the skin?"
  },
  {
    "objectID": "week6/slides.html#an-evolution-in-seeing-the-world-33",
    "href": "week6/slides.html#an-evolution-in-seeing-the-world-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "An evolution in seeing the world (3/3)",
    "text": "An evolution in seeing the world (3/3)\n\n\nDrawing trees\n\n\n Does it look like a tree? What is not realistic?\nSource: https://toppng.com\n\n\n\n\nTree foliage has lots of different colours"
  },
  {
    "objectID": "week6/slides.html#philosophical-reflection",
    "href": "week6/slides.html#philosophical-reflection",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Philosophical reflection",
    "text": "Philosophical reflection\n\nYou, me, we humans have a tendency to\n\nonly see what other people have done or say,\nnot what we can see,\nor impose beliefs, like trees are green.\n\n When you look at data, you might discover that there is a different story, or many different stories."
  },
  {
    "objectID": "week6/slides.html#try-to-see-with-fresh-eyes",
    "href": "week6/slides.html#try-to-see-with-fresh-eyes",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Try to see with fresh eyes",
    "text": "Try to see with fresh eyes"
  },
  {
    "objectID": "week6/slides.html#outline",
    "href": "week6/slides.html#outline",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Outline",
    "text": "Outline\n\nThe humble but powerful scatterplot\nAdditions and variations\nTransformations to linearity\n(Robust) numerical measures of association\nSimpson‚Äôs paradox\nMaking null samples to test for association"
  },
  {
    "objectID": "week6/slides.html#the-scatterplot",
    "href": "week6/slides.html#the-scatterplot",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "The scatterplot",
    "text": "The scatterplot\n\nScatterplots are the natural plot to make to explore association between two continuous (quantitative or numeric) variables.\n\nThey are not just for linear relationships but are useful for examining nonlinear patterns, clustering and outliers.\nWe also can think about scatterplots in terms of statistical distributions: if a histogram shows a marginal distribution, a scatterplot allows us to examine the bivariate distribution of a sample."
  },
  {
    "objectID": "week6/slides.html#history",
    "href": "week6/slides.html#history",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "History",
    "text": "History\n\nDescartes provided the Cartesian coordinate system in the 17th century, with perpendicular lines indicating two axes.\n\n\n\nIt wasn‚Äôt until 1832 that the scatterplot appeared, when John Frederick Herschel plotted position and time of double stars.\n\n\n\n\nThis is 200 years after the Cartesian coordinate system, and 50 years after bar charts and line charts appeared, used in the work of William Playfair to examine economic data.\n\n\n\n\nKopf argues that The scatter plot, by contrast, proved more useful for scientists, but it clearly is useful for economics today.\n\n\nhttp://www.datavis.ca/milestones/"
  },
  {
    "objectID": "week6/slides.html#language-and-terminology",
    "href": "week6/slides.html#language-and-terminology",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Language and terminology",
    "text": "Language and terminology\nAre the words ‚Äúcorrelation‚Äù and ‚Äúassociation‚Äù interchangeable?\n\nIn the broadest sense correlation is any statistical association, though it commonly refers to the degree to which a pair of variables are linearly related. Wikipedia\n\n \n\nIf the relationship is not linear, call it association, and avoid correlated."
  },
  {
    "objectID": "week6/slides.html#features-of-a-pair-of-continuous-variables-13",
    "href": "week6/slides.html#features-of-a-pair-of-continuous-variables-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Features of a pair of continuous variables (1/3)",
    "text": "Features of a pair of continuous variables (1/3)\n\n\n\n\n\nFeature\nExample\nDescription\n\n\n\n\npositive trend\n\nLow value corresponds to low value, and high to high.\n\n\nnegative trend\n\nLow value corresponds to high value, and high to low.\n\n\nno trend\n\nNo relationship\n\n\nstrong\n\nVery little variation around the trend\n\n\nmoderate\n\nVariation around the trend is almost as much as the trend\n\n\nweak\n\nA lot of variation making it hard to see any trend"
  },
  {
    "objectID": "week6/slides.html#features-of-a-pair-of-continuous-variables-23",
    "href": "week6/slides.html#features-of-a-pair-of-continuous-variables-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Features of a pair of continuous variables (2/3)",
    "text": "Features of a pair of continuous variables (2/3)\n\n\n\n\n\nFeature\nExample\nDescription\n\n\n\n\nlinear form\n\nThe shape is linear\n\n\nnonlinear form\n\nThe shape is more of a curve\n\n\nnonlinear form\n\nThe shape is more of a curve\n\n\noutliers\n\nThere are one or more points that do not fit the pattern on the others\n\n\nclusters\n\nThe observations group into multiple clumps\n\n\ngaps\n\nThere is a gap, or gaps, but its not clumped"
  },
  {
    "objectID": "week6/slides.html#features-of-a-pair-of-continuous-variables-33",
    "href": "week6/slides.html#features-of-a-pair-of-continuous-variables-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Features of a pair of continuous variables (3/3)",
    "text": "Features of a pair of continuous variables (3/3)\n\n\n\n\n\nFeature\nExample\nDescription\n\n\n\n\nbarrier\n\nThere is combination of the variables which appears impossible\n\n\nl-shape\n\nWhen one variable changes the other is approximately constant\n\n\ndiscreteness\n\nRelationship between two variables is different from the overall, and observations are in a striped pattern\n\n\nheteroskedastic\n\nVariation is different in different areas, maybe depends on value of x variable\n\n\nweighted\n\nIf observations have an associated weight, reflect in scatterplot, e.g. bubble chart"
  },
  {
    "objectID": "week6/slides.html#additional-considerations-unwin-2015",
    "href": "week6/slides.html#additional-considerations-unwin-2015",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Additional considerations (Unwin, 2015):",
    "text": "Additional considerations (Unwin, 2015):\n\ncausation: one variable has a direct influence on the other variable, in some way. For example, people who are taller tend to weigh more. The dependent variable is conventionally on the y axis. It‚Äôs not generally possible to tell from the plot that the relationship is causal, which typically needs to be argued from other sources of information.\nassociation: variables may be related to one another, but through a different variable, eg ice cream sales are positively correlated with beach drownings, is most likely a temperature relationship.\nconditional relationships: the relationship between variables is conditionally dependent on another, such as income against age likely has a different relationship depending on retired or not."
  },
  {
    "objectID": "week6/slides.html#famous-data-examples",
    "href": "week6/slides.html#famous-data-examples",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Famous data examples",
    "text": "Famous data examples"
  },
  {
    "objectID": "week6/slides.html#famous-scatterplot-examples",
    "href": "week6/slides.html#famous-scatterplot-examples",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Famous scatterplot examples",
    "text": "Famous scatterplot examples\n\n\nAnscombe‚Äôs quartet\n\n\n\n\n\n\n\n\n\nAll four sets of Anscombe has same means, standard deviations and correlations, \\(\\bar{x}\\) = 9, \\(\\bar{y}\\) = 7.5, \\(s_x\\) = 3.3, \\(s_y\\) = 2, \\(r\\) = 0.82.\n\n\nNumerical statistics are the same, for very different association.\n\n\nDatasaurus dozen\nAnd similarly all 13 sets of the datasaurus dozen have same means, standard deviations and correlations, \\(\\bar{x}\\) = 54, \\(\\bar{y}\\) = 48, \\(s_x\\) = 17, \\(s_y\\) = 27, \\(r\\) = -0.06."
  },
  {
    "objectID": "week6/slides.html#scatterplot-case-studies",
    "href": "week6/slides.html#scatterplot-case-studies",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scatterplot case studies",
    "text": "Scatterplot case studies"
  },
  {
    "objectID": "week6/slides.html#case-study-olympics",
    "href": "week6/slides.html#case-study-olympics",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Olympics",
    "text": "Case study: Olympics\n\nüñºdataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Warning message: Removed 1346 rows containing missing values (geom_point)\nFeatures:\n\nlinear relationship (expected, more than?)\noutliers\ndiscretization\n\nSubstantial overplotting, &gt;10000 athletes.\nWhat is interesting? Are there some sport(s) where you would expect specific relationships?\n\n\n\n\n\n\n\nskimr::skim(oly12)\n\n\nData summary\n\n\nName\noly12\n\n\nNumber of rows\n10384\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nDate\n1\n\n\nfactor\n6\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nDOB\n6192\n0.4\n1947-06-01\n1997-07-09\n1986-09-11\n2149\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nName\n0\n1\nFALSE\n10366\nLei: 3, Lin: 3, Ale: 2, Hao: 2\n\n\nCountry\n0\n1\nFALSE\n205\nGre: 523, Uni: 518, Rus: 414, Aus: 399\n\n\nSex\n0\n1\nFALSE\n2\nM: 5756, F: 4628\n\n\nPlaceOB\n0\n1\nFALSE\n4108\nemp: 2690, Seo: 57, Bud: 54, Mos: 50\n\n\nSport\n0\n1\nFALSE\n42\nAth: 2119, Swi: 907, Foo: 596, Row: 524\n\n\nEvent\n0\n1\nFALSE\n763\nMen: 336, Wom: 260, Wom: 210, Men: 206\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAge\n0\n1.00\n26.07\n5.44\n13.0\n22.0\n25.0\n29.0\n71.0\n‚ñÜ‚ñá‚ñÅ‚ñÅ‚ñÅ\n\n\nHeight\n561\n0.95\n1.77\n0.11\n1.3\n1.7\n1.8\n1.9\n2.2\n‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÅ\n\n\nWeight\n1280\n0.88\n72.85\n16.07\n36.0\n61.0\n70.0\n81.0\n218.0\n‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÅ\n\n\nGold\n0\n1.00\n0.02\n0.14\n0.0\n0.0\n0.0\n0.0\n2.0\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nSilver\n0\n1.00\n0.02\n0.13\n0.0\n0.0\n0.0\n0.0\n2.0\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nBronze\n0\n1.00\n0.02\n0.14\n0.0\n0.0\n0.0\n0.0\n2.0\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nTotal\n0\n1.00\n0.05\n0.25\n0.0\n0.0\n0.0\n0.0\n5.0\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\n\n\nggplot(oly12, aes(x = Height, y = Weight, label = Sport)) +\n  geom_point()"
  },
  {
    "objectID": "week6/slides.html#try-this",
    "href": "week6/slides.html#try-this",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üß© Try this",
    "text": "üß© Try this\n\nInteractivity can be a useful tool for exploring relationships.\n\nCut and paste the code into your R console, and the resulting plot to examine the sport of the athlete.\n\nlibrary(tidyverse) \nlibrary(plotly) \ndata(oly12, package = \"VGAMdata\") \np &lt;- ggplot(oly12, aes(x = Height, y = Weight, label = Sport)) + \n  geom_point() \nggplotly(p)"
  },
  {
    "objectID": "week6/slides.html#how-many-athletes-in-the-different-sports",
    "href": "week6/slides.html#how-many-athletes-in-the-different-sports",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How many athletes in the different sports?",
    "text": "How many athletes in the different sports?\n\n\n\n\n\n\n\n\n\n\n\nCategories need re-working:\n\nso many different events grouped into athletics\ncycling split among many categories"
  },
  {
    "objectID": "week6/slides.html#consolidate-factor-levels",
    "href": "week6/slides.html#consolidate-factor-levels",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Consolidate factor levels",
    "text": "Consolidate factor levels\nThere are several cycling events that are reasonable to combine into one category. Similarly for gymnastics and athletics.\n\n\noly12 &lt;- oly12 |&gt;\n  mutate(Sport = as.character(Sport)) |&gt;\n  mutate(Sport = ifelse(grepl(\"Cycling\", Sport), \n    \"Cycling\", Sport\n  )) |&gt; \n  mutate(Sport = ifelse(grepl(\"Gymnastics\", Sport),\n    \"Gymnastics\", Sport\n  )) |&gt;\n  mutate(Sport = ifelse(grepl(\"Athletics\", Sport),\n    \"Athletics\", Sport\n  )) |&gt;\n  mutate(Sport = as.factor(Sport))"
  },
  {
    "objectID": "week6/slides.html#drill-down-the-by-sport",
    "href": "week6/slides.html#drill-down-the-by-sport",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Drill down the by sport",
    "text": "Drill down the by sport\n\nüñºlearnR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is interesting?\n\n\nSome sports have no data for height, weight\nThe positive association between height and weight is visible across sports\nNonlinear in wrestling?\nAn outlier in judo, and football, and archery\nMaybe flatter among swimmers\nTaller in basketball, volleyball and handball\nShorter in athletics, weightlifting and wrestling\nLittle variance in tennis players\nThere‚Äôs a lot to digest\n\n\n\n\nRefine plots to make comparisons easier:\n\nRemove sports with missings\nOverlay model to assess linear relationships\nDrill down into other strata, eg male/female athletes\nCompare in small chunks, one group against the rest\n\n\n\n\n\n\n\nggplot(oly12, aes(x = Height, y = Weight)) +\n  geom_point(alpha = 0.5) + \n  facet_wrap(~Sport, ncol = 8) +\n  theme(aspect.ratio = 1) \n\n  Note: alpha transparency, and aspect ratio"
  },
  {
    "objectID": "week6/slides.html#remove-missings-explore-difference-by-sex",
    "href": "week6/slides.html#remove-missings-explore-difference-by-sex",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Remove missings, explore difference by sex",
    "text": "Remove missings, explore difference by sex\n\nüñºÔ∏èlearnR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Because the focus is now on males vs females association shape within sport, make plots scale separately.\n\nAthletics category should have been broken into several more categories like track, field: a shot-putter has a very different physique to a sprinter.\nGenerally, clustering of male/female athletes\n\n\n\nOutliers: a tall skinny male archer, a medium height very light female athletics athlete, tall light female weightlifter, tall light male volleyballer\nCanoe slalom athletes, divers, cyclists are tiny.\n\n\n\n\n\n\noly12 |&gt;\n  filter(!(Sport %in% c(\"Boxing\", \"Gymnastics\", \"Synchronised Swimming\", \"Taekwondo\", \"Trampoline\"))) |&gt;\n  mutate(Sport = fct_drop(Sport)) |&gt;\n  ggplot(aes(x = Height, y = Weight, colour = Sex)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~Sport, ncol = 7, scales = \"free\") +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  theme(aspect.ratio = 1, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))"
  },
  {
    "objectID": "week6/slides.html#common-ways-to-augment-scatterplots",
    "href": "week6/slides.html#common-ways-to-augment-scatterplots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Common ways to augment scatterplots",
    "text": "Common ways to augment scatterplots\n\n\n\n\n\nModification\nExample\nPurpose\n\n\n\n\nalpha-blend\n\nalleviate overplotting to examine density at centre\n\n\nmodel overlay\n\nfocus on the trend\n\n\nmodel + data\n\ntrend plus variation\n\n\ndensity\n\noverall distribution, variation and clustering\n\n\nfilled density\n\nhigh density locations in distribution (modes), variation and clustering\n\n\ncolour\n\nrelationship with conditioning and lurking variables"
  },
  {
    "objectID": "week6/slides.html#comparing-association",
    "href": "week6/slides.html#comparing-association",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Comparing association",
    "text": "Comparing association\n\nüñºÔ∏èR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeightlifters are much heavier relative to height\nSwimmers are leaner relative to height\nTennis players are a bit mixed, shorter tend to be heavier, taller tend to be lighter\n\n\n\n\n\n\noly12 |&gt;\n  filter(Sport %in% c(\n    \"Swimming\", \"Archery\", \"Basketball\",\n    \"Handball\", \"Hockey\", \"Tennis\",\n    \"Weightlifting\", \"Wrestling\"\n  )) |&gt;\n  filter(Sex == \"F\") |&gt;\n  mutate(Sport = fct_drop(Sport), Sex = fct_drop(Sex)) |&gt;\n  ggplot(aes(x = Height, y = Weight, colour = Sport)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  )"
  },
  {
    "objectID": "week6/slides.html#comparing-spread",
    "href": "week6/slides.html#comparing-spread",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Comparing spread",
    "text": "Comparing spread\n\nüñºÔ∏èR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModern pentathlon athletes are uniformly height and weight related\nShooters are quite varied in body type\n\n\n\n\n\n\noly12 |&gt;\n  filter(Sport %in% c(\"Shooting\", \"Modern Pentathlon\", \"Basketball\")) |&gt; \n  filter(Sex == \"F\") |&gt;\n  mutate(Sport = fct_drop(Sport), Sex = fct_drop(Sex)) |&gt;\n  ggplot(aes(x = Height, y = Weight, colour = Sport)) +\n  geom_density2d() + \n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  )"
  },
  {
    "objectID": "week6/slides.html#case-study-olympics-1",
    "href": "week6/slides.html#case-study-olympics-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Olympics",
    "text": "Case study: Olympics\nWe learned that association between height and weight is different strata, defined by categorical variables: sport, gender, and possibly country and age, too.\nSome of the association may be due to unmeasured variables, for example, ‚ÄúAthletics‚Äù is masking different body types in throwing vs running. This is a lurking variable.\n If you were just given the Height and Weight in this data could you have detected the presence of conditional relationships?\n\nIt may appear as multimodality."
  },
  {
    "objectID": "week6/slides.html#can-you-see-conditional-dependencies",
    "href": "week6/slides.html#can-you-see-conditional-dependencies",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Can you see conditional dependencies?",
    "text": "Can you see conditional dependencies?\n\nüñºR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nThere is a barely hint of multimodality.\nIt‚Äôs not easy to detect the presence of the additional variable, and thus accurately describe the relationship between height and weight among Olympic athletes.\n\n\n\n\n\np1 &lt;- oly12 |&gt;\n  filter(Sport == \"Athletics\") |&gt;\n  ggplot(aes(x = Height, y = Weight)) +\n  geom_point(alpha = 0.2, size = 4) \np2 &lt;- oly12 |&gt;\n  filter(Sport == \"Athletics\") |&gt;\n  ggplot(aes(x = Height, y = Weight)) +\n  geom_density2d_filled() +\n  theme(legend.position = \"none\")\np3 &lt;- oly12 |&gt;\n  filter(Sport == \"Athletics\") |&gt;\n  ggplot(aes(x = Height, y = Weight)) +\n  geom_density2d(binwidth = 0.01) \np4 &lt;- oly12 |&gt;\n  filter(Sport == \"Athletics\") |&gt;\n  ggplot(aes(x = Height, y = Weight)) +\n  geom_density2d(binwidth = 0.001, color = \"white\", size = 0.2) +\n  geom_density2d_filled(binwidth = 0.001) +\n  theme(legend.position = \"none\")\ngrid.arrange(p1, p3, p2, p4, ncol = 2)"
  },
  {
    "objectID": "week6/slides.html#numerical-measures-of-association",
    "href": "week6/slides.html#numerical-measures-of-association",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Numerical measures of association",
    "text": "Numerical measures of association"
  },
  {
    "objectID": "week6/slides.html#correlation",
    "href": "week6/slides.html#correlation",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Correlation",
    "text": "Correlation\n\n\nCorrelation between variables \\(x_1\\) and \\(x_2\\), with \\(n\\) observations in each.\n\n\\[r = \\frac{\\sum_{i=1}^n (x_{i1}-\\bar{x}_1)(x_{i2}-\\bar{x}_2)}{\\sqrt{\\sum_{i=1}^n(x_{i1}-\\bar{x}_1)^2\\sum_{i=1}^n(x_{i2}-\\bar{x}_2)^2}} = \\frac{\\mbox{covariance}(x_1, x_2)}{(n-1)s_{x_1}s_{x_2}}\\]\n\nTest for statistical significance, whether population correlation could be 0 based on observed \\(r\\), using a \\(t_{n-2}\\) distribution:\n\n\\[t=\\frac{r}{\\sqrt{1-r^2}}\\sqrt{n-2}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor(d1$x, d1$y)\n\n[1] 0.52\n\n\n\ncor.test(d1$x, d1$y)\n\n\n    Pearson's product-moment correlation\n\ndata:  d1$x and d1$y\nt = 9, df = 198, p-value = 2e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.41 0.62\nsample estimates:\n cor \n0.52"
  },
  {
    "objectID": "week6/slides.html#problems-with-correlation-12",
    "href": "week6/slides.html#problems-with-correlation-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Problems with correlation (1/2)",
    "text": "Problems with correlation (1/2)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor(d2$x, d2$y)\n\n[1] -0.05\n\n\n\ncor.test(d2$x, d2$y)\n\n\n    Pearson's product-moment correlation\n\ndata:  d2$x and d2$y\nt = -0.7, df = 198, p-value = 0.5\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.187  0.089\nsample estimates:\n  cor \n-0.05 \n\n\n It does not summarise non-linear associations."
  },
  {
    "objectID": "week6/slides.html#problems-with-correlation-22",
    "href": "week6/slides.html#problems-with-correlation-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Problems with correlation (2/2)",
    "text": "Problems with correlation (2/2)\n\n\n\n\n\n\n\n\n\n\n\n\nAll observations\n\n\n$estimate\ncor \n0.3 \n\n$statistic\n  t \n4.4 \n\n$p.value\n[1] 1.6e-05\n\n\n\nWithout outlier\n\n\n$estimate\n   cor \n-0.012 \n\n$statistic\n    t \n-0.17 \n\n$p.value\n[1] 0.87\n\n\n It is affected by extreme values."
  },
  {
    "objectID": "week6/slides.html#perceiving-correlation",
    "href": "week6/slides.html#perceiving-correlation",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Perceiving correlation",
    "text": "Perceiving correlation\n\nüñºÔ∏èanswersR\n\n\n\nLet‚Äôs play a game: Guess the correlation!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerally, people don‚Äôt do very well at this task. Typically people under-estimate \\(r\\) from scatterplots, particularly when it is around 0.4-0.7. The variation in a scatterplot perceptually doesn‚Äôt vary is not linearly with \\(r\\).\nWhen someone says correlation is 0.5 it sounds impressive. BUT when someone shows you a scatterplot of data that has correlation 0.5, you will say that‚Äôs a weak relationship.\n\n\n\n\nset.seed(7777)\nvc &lt;- matrix(c(1, 0, 0, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np1 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, 0.4, 0.4, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np2 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, 0.6, 0.6, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np3 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, 0.8, 0.8, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np4 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, -0.2, -0.2, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np5 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, -0.5, -0.5, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np6 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, -0.7, -0.7, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np7 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, -0.9, -0.9, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np8 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\ngrid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, ncol = 4)"
  },
  {
    "objectID": "week6/slides.html#robust-correlation-measures-12",
    "href": "week6/slides.html#robust-correlation-measures-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Robust correlation measures (1/2)",
    "text": "Robust correlation measures (1/2)\n\n\nSpearman (based on ranks)\n\nSort each variable, and return rank (of actual value)\nCompute correlation between ranks of each variable\n\n\n\n\n\n\nset.seed(60)\ndf &lt;- tibble(\n  x = c(round(rnorm(5), 1), 10),\n  y = c(round(rnorm(5), 1), 10)\n) |&gt;\n  mutate(xr = rank(x), yr = rank(y))\ndf\n\n# A tibble: 6 √ó 4\n      x     y    xr    yr\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.7  -1.7     5     1\n2   0.5   1.1     4     5\n3  -0.6   0.3     2     3\n4  -0.2  -0.9     3     2\n5  -1.7   0.4     1     4\n6  10    10       6     6\n\n\n\n\ncor(df$x, df$y)\n\n[1] 0.94\n\ncor(df$xr, df$yr)\n\n[1] 0.2\n\ncor(df$x, df$y, method = \"spearman\")\n\n[1] 0.2"
  },
  {
    "objectID": "week6/slides.html#robust-correlation-measures-22",
    "href": "week6/slides.html#robust-correlation-measures-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Robust correlation measures (2/2)",
    "text": "Robust correlation measures (2/2)\n\n\nKendall \\(\\tau\\) (based on comparing pairs of observations)\n\nSort each variable, and return rank (of actual value)\nFor all pairs of observations \\((x_i, y_i), (x_j, y_j)\\), determine if concordant, \\(x_i &lt; x_j, y_i &lt; y_j\\) or \\(x_i &gt; x_j, y_i &gt; y_j\\), or discordant, \\(x_i &lt; x_j, y_i &gt; y_j\\) or \\(x_i &gt; x_j, y_i &lt; y_j\\).\n\n\n\\[\\tau = \\frac{n_c-n_d}{\\frac12 n(n-1)}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor(df$x, df$y)\n\n[1] 0.94\n\ncor(df$x, df$y, method = \"kendall\")\n\n[1] 0.067"
  },
  {
    "objectID": "week6/slides.html#comparison-of-correlation-measures",
    "href": "week6/slides.html#comparison-of-correlation-measures",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Comparison of correlation measures",
    "text": "Comparison of correlation measures\n\n\n\n\n\nsample\ncorr\nspearman\nkendall\n\n\n\n\n\n0.52\n0.512\n0.355\n\n\n\n-0.05\n-0.087\n-0.073\n\n\n\n0.30\n-0.023\n-0.014\n\n\n\n\n\n\n\n\nRobust calculation corrects outlier problems, but nothing measures the non-linear association."
  },
  {
    "objectID": "week6/slides.html#transformations",
    "href": "week6/slides.html#transformations",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Transformations",
    "text": "Transformations\nfor skewness, heteroskedasticity and linearising relationships, and to emphasize association"
  },
  {
    "objectID": "week6/slides.html#circle-of-transformations-for-linearising",
    "href": "week6/slides.html#circle-of-transformations-for-linearising",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Circle of transformations for linearising",
    "text": "Circle of transformations for linearising\n\n\n\n\n\n\n\n\n\n\n\n\nRemember the power ladder:\n-1, 0, 1/3, 1/2, 1, 2, 3, 4\n\n\nLook at the shape of the relationship.\nImagine this to be a number plane, and depending on which quadrant the shape falls in, you either transform \\(x\\) or \\(y\\), up or down the ladder: +,+ both up; +,- x up, y down; -,- both down; -,+ x down, y up\n\n\nIf there is heteroskedasticity, try transforming \\(y\\), may or may not help"
  },
  {
    "objectID": "week6/slides.html#scatterplot-case-studies-1",
    "href": "week6/slides.html#scatterplot-case-studies-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scatterplot case studies",
    "text": "Scatterplot case studies"
  },
  {
    "objectID": "week6/slides.html#case-study-soils-14",
    "href": "week6/slides.html#case-study-soils-14",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Soils (1/4)",
    "text": "Case study: Soils (1/4)\n\n\n\n\n\n\n\n\n\n\n\n\nInterplay between skewness and association\nData is from a soil chemical analysis of a farm field in Iowa. Is there a relationship between Yield and Boron?\n You can get a marginal plot of each variable added to the scatterplot using ggMarginal. This is useful for assessing the skewness in each variable.\n Boron is right-skewed Yield is left-skewed. With skewed distributions in marginal variables it is hard to assess the relationship between the two. Make a transformation to fix, first."
  },
  {
    "objectID": "week6/slides.html#case-study-soils-24",
    "href": "week6/slides.html#case-study-soils-24",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Soils (2/4)",
    "text": "Case study: Soils (2/4)\n\n\n\n\n\n\n\n\n\n\n\n\n \n\np &lt;- ggplot(\n  baker,\n  aes(x = B, y = Corn97BU^2)\n) + \n  geom_point() +\n  xlab(\"log Boron (ppm)\") +\n  ylab(\"Corn Yield^2 (bushells)\") +\n  scale_x_log10() \nggMarginal(p, type = \"density\")"
  },
  {
    "objectID": "week6/slides.html#case-study-soils-34",
    "href": "week6/slides.html#case-study-soils-34",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Soils (3/4)",
    "text": "Case study: Soils (3/4)\n\n\n\n\n\n\n\n\n\n\n\n\n Lurking variable?\n \n\np &lt;- ggplot(\n  baker,\n  aes(x = Fe, y = Corn97BU^2)\n) +\n  geom_density2d(colour = \"orange\") +\n  geom_point() +\n  xlab(\"Iron (ppm)\") + \n  ylab(\"Corn Yield^2 (bushells)\")\nggMarginal(p, type = \"density\")"
  },
  {
    "objectID": "week6/slides.html#case-study-soils-44",
    "href": "week6/slides.html#case-study-soils-44",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Soils (4/4)",
    "text": "Case study: Soils (4/4)\n\n\n\n\n\n\n\n\n\n\n\n\nColour high calcium (&gt;5200ppm) calcium values\n\nggplot(baker, aes(\n  x = Fe, y = Corn97BU^2,\n  colour = ifelse(Ca &gt; 5200, \n    \"high\", \"low\"\n  )\n)) + \n  geom_point() +\n  xlab(\"Iron (ppm)\") +\n  ylab(\"Corn Yield^2 (bushells)\") +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  theme(\n    aspect.ratio = 1,\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  )\n\nIf calcium levels in the soil are high, yield is consistently high. If calcium levels are low, then there is a positive relationship between yield and iron, with higher iron leading to higher yields."
  },
  {
    "objectID": "week6/slides.html#case-study-covid-19",
    "href": "week6/slides.html#case-study-covid-19",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: COVID-19",
    "text": "Case study: COVID-19\n\nüñºÔ∏èinfoR\n\n\n\n\n\n\n\n\n\n\n\n\n\n Bubble plots, size of point is mapped to another variable.\nThis bubble plot here shows total count of COVID-19 incidence (as of Aug 30, 2020) for every county in the USA, inspired by the New York Times coverage.\n\n\n\nload(here(\"data/nyt_covid.rda\"))\nusa &lt;- map_data(\"state\")\nggplot() +\n  geom_polygon(\n    data = usa,\n    aes(x = long, y = lat, group = group),\n    fill = \"grey90\", colour = \"white\"\n  ) +\n  geom_point(\n    data = nyt_county_total,\n    aes(x = lon, y = lat, size = cases),\n    colour = \"red\", shape = 1\n  ) +\n  geom_point(\n    data = nyt_county_total,\n    aes(x = lon, y = lat, size = cases),\n    colour = \"red\", fill = \"red\", alpha = 0.1, shape = 16\n  ) +\n  scale_size(\"\", range = c(1, 30)) +\n  theme_map() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "week6/slides.html#scales-matter",
    "href": "week6/slides.html#scales-matter",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scales matter",
    "text": "Scales matter\n\n\n \n\n\n\n\n\n\n\n\n\n\n  Where has COVID-19 hit the hardest? \nWhere are there more people?   \n\nThis plot tells you NOTHING except where the population centres are in the USA.\n\nTo understand relative incidence/risk, report COVID numbers relative the population. For example, number of cases per 100,000 people."
  },
  {
    "objectID": "week6/slides.html#beyond-quantitative-variables",
    "href": "week6/slides.html#beyond-quantitative-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Beyond quantitative variables",
    "text": "Beyond quantitative variables"
  },
  {
    "objectID": "week6/slides.html#when-variables-are-not-quantitative",
    "href": "week6/slides.html#when-variables-are-not-quantitative",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "When variables are not quantitative",
    "text": "When variables are not quantitative\nWhat do you do if the variables are not continuous/quantitative?\n\nType of variable determines the appropriate mapping.\n\nContinuous and categorical: side-by-side boxplots, side-by-side density plots\nBoth categorical: faceted bar charts, stacked bar charts, mosaic plots, double decker plots\n\n  Stay tuned!"
  },
  {
    "objectID": "week6/slides.html#paradoxes",
    "href": "week6/slides.html#paradoxes",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Paradoxes",
    "text": "Paradoxes"
  },
  {
    "objectID": "week6/slides.html#simpsons-paradox",
    "href": "week6/slides.html#simpsons-paradox",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Simpsons paradox",
    "text": "Simpsons paradox\nThere is an additional variable, which if used for conditioning, changes the association between the variables, you have a paradox."
  },
  {
    "objectID": "week6/slides.html#simpsons-paradox-famous-example",
    "href": "week6/slides.html#simpsons-paradox-famous-example",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Simpsons paradox: famous example",
    "text": "Simpsons paradox: famous example\n\n\nDid Berkeley discriminate against female applicants?\nExample from Unwin (2015)"
  },
  {
    "objectID": "week6/slides.html#simpsons-paradox-famous-example-1",
    "href": "week6/slides.html#simpsons-paradox-famous-example-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Simpsons paradox: famous example",
    "text": "Simpsons paradox: famous example\n\nBased on separately examining each department, there is no evidence of discrimination against female applicants.\nExample from Unwin (2015)"
  },
  {
    "objectID": "week6/slides.html#always-examine-the-associations-in-each-strata",
    "href": "week6/slides.html#always-examine-the-associations-in-each-strata",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Always examine the associations in each strata",
    "text": "Always examine the associations in each strata"
  },
  {
    "objectID": "week6/slides.html#is-what-you-see-really-association",
    "href": "week6/slides.html#is-what-you-see-really-association",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Is what you see really association?",
    "text": "Is what you see really association?"
  },
  {
    "objectID": "week6/slides.html#checking-association-with-visual-inference",
    "href": "week6/slides.html#checking-association-with-visual-inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Checking association with visual inference",
    "text": "Checking association with visual inference\n\nSoilsROlympicsR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(\n  lineup(null_permute(\"Corn97BU\"), baker, n = 12),\n  aes(x = B, y = Corn97BU)\n) +\n  geom_point() +\n  facet_wrap(~.sample, ncol = 4)\n\n11 of the panels have had the association broken by permuting one variable. There is no association in these data sets, and hence plots. Does the data plot stand out as being different from the null (no association) plots?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata(oly12, package = \"VGAMdata\")\noly12_sub &lt;- oly12 |&gt;\n  filter(Sport %in% c(\n    \"Swimming\", \"Archery\",\n    \"Hockey\", \"Tennis\"\n  )) |&gt;\n  filter(Sex == \"F\") |&gt;\n  mutate(Sport = fct_drop(Sport), Sex = fct_drop(Sex))\n\nggplot(\n  lineup(null_permute(\"Sport\"), oly12_sub, n = 12),\n  aes(x = Height, y = Weight, colour = Sport)\n) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  facet_wrap(~.sample, ncol = 4) +\n  theme(legend.position = \"none\")\n\n\n11 of the panels have had the association broken by permuting the Sport label. There is no difference in the association between weight and height across sports in these data sets, and hence plots. Does the data plot stand out as being different from the null (no association difference between sports) plots?"
  },
  {
    "objectID": "week6/slides.html#resources",
    "href": "week6/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nUnwin (2015) Graphical Data Analysis with R\nGraphics using ggplot2\nWilke (2019) Fundamentals of Data Visualization https://clauswilke.com/dataviz/\nFriendly and Denis ‚ÄúMilestones in History of Thematic Cartography, Statistical Graphics and Data Visualisation‚Äù available at http://www.datavis.ca/milestones/\n\n\n\n\nETC5521 Lecture 6 | ddde.numbat.space"
  },
  {
    "objectID": "week5/tutorial.html",
    "href": "week5/tutorial.html",
    "title": "ETC5521 Tutorial 5",
    "section": "",
    "text": "Refresh thinking about statistical inference.\nLearn to apply inference for data plots."
  },
  {
    "objectID": "week5/tutorial.html#objectives",
    "href": "week5/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 5",
    "section": "",
    "text": "Refresh thinking about statistical inference.\nLearn to apply inference for data plots."
  },
  {
    "objectID": "week5/tutorial.html#preparation",
    "href": "week5/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 5",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is Wickham et al.¬†(2010) Graphical inference for Infovis. It is a basic introduction to inference for exploratory data analysis, especially for data visualisation. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\ninstall.packages(c(\"tidyverse\", \"datarium\", \"broom\", \"nullabor\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week5/tutorial.html#exercises",
    "href": "week5/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 5",
    "section": "üì• Exercises",
    "text": "üì• Exercises"
  },
  {
    "objectID": "week5/tutorial.html#exercise-1-skittles-experiment",
    "href": "week5/tutorial.html#exercise-1-skittles-experiment",
    "title": "ETC5521 Tutorial 5",
    "section": "Exercise 1: Skittles experiment",
    "text": "Exercise 1: Skittles experiment\n\nSkittles come in five colors (orange, yellow, red, purple, green) each with their own flavours (orange, lemon, strawberry, grape, green apple). Data was collected by Dr Nick Tierney to explore whether a sample of 3 people could identify the flavour of skittles while blindfolded. You can find the cleaned tidy data here.\n\nHow many skittles did each person taste?\n\n\nA person with loss of taste is called ageusia and a person who has a loss of smell is called anosmia. The loss of taste and loss of smell will not allow you to distinguish flavours in food. What is the probability that a person with ageusia and anosmia will guess the skittle flavour correctly (out of the five flavours) for one skittle?\n\n\nWhat is the probability that a person with ageusia and anosmia will guess the skittle flavour correctly for 2 out of 10 skittles, assuming the order of taste does not matter?\n\n\nTest the null hypothesis that people cannot distinguish the flavours correctly, against the alternative that they can. Assume that the order of tasting does not matter and each person has the same ability to correctly identify the flavours. In conducting your test, define your null and alternate hypothesis, in statistical notation, your assumptions, the test statistics and calculate the \\(p\\)-value.\n\n\nIn part (d) we disregarded the order of the tasting and the possible variability in people‚Äôs ability to correctly identify the flavour. If in fact these do matter, then how would you construct the test statistic? Is it easy?\n\n\nConsider the plot below that shows in each tile whether a person guessed correctly by order of their tasting. Suppose that under the null hypothesis, the order of tasting does not matter and people have no ability to distinguish the flavours. Generate a null plot under this null hypothesis.\n\n\n\n\n\nBased on (f), construct a lineup (using nullabor or otherwise) of 20 plots. Ask your classmate, which plot looks different.\n\n\nSuppose that you have a response from 100 people based on your line-up from (g) and 76 correctly identified the data plot. What is the \\(p\\)-value from this visual inference?\n\n\nNow consider the plot below. Use the same null data in (g) to construct a lineup based on below visual statistic. Suppose we had 28 people out of 100 who correctly identified the data plot in this lineup. What is the difference in power of visual statistic in (f) and this one?"
  },
  {
    "objectID": "week5/tutorial.html#exercise-2-social-media-marketing",
    "href": "week5/tutorial.html#exercise-2-social-media-marketing",
    "title": "ETC5521 Tutorial 5",
    "section": "Exercise 2: Social media marketing",
    "text": "Exercise 2: Social media marketing\nThe data marketing in the datarium R-package contains information on sales with advertising budget for three advertising media (youtube, facebook and newspaper). This advertising experiment was repeated 200 times to study the impact of the advertisting media on sales.\n\ndata(marketing, package = \"datarium\")\n\n\nStudy the pairs plot. Which of the advertising medium do you think affects the sales?\n\n\nConstruct a coplot for sales vs advertising budget for facebook conditioned on advertising budget for youtube and newspaper. (You may like to make the intervals non-overlapping to make it easier to plot in ggplot). What do you see in the plot?\n\n\nNow construct a coplot for sales vs advertising budget for facebook conditioned on advertising budget for youtube alone. Superimpose a linear model on each facet. Is there an interval where the linear model is not a good fit?\n\n\nConsider the following interaction model (which has the same symbolic model formulae as sales ~ facebook*youtube) for data where the advertising budget for youtube is at least $90,000. Construct a QQ-plot of the residuals. Do you think the errors are normally distributed? Construct a lineup for the QQ-plot assuming that the null distribution is Normally distributed with mean zero and variance as estimated from the model fit."
  },
  {
    "objectID": "week5/tutorial.html#exercise-3-ida-skill-sprint",
    "href": "week5/tutorial.html#exercise-3-ida-skill-sprint",
    "title": "ETC5521 Tutorial 5",
    "section": "Exercise 3: IDA skill sprint",
    "text": "Exercise 3: IDA skill sprint\nSet the timer. You have 15 minutes to discover as many problems as possible in this data, cafe.rda.\nA small cafe in the city of Melbourne is interested in determining whether the daily earnings depend on the weather. They compiled data for a period over 2000-2001 to study this question. The data has the following variables:\n\n\n\n \n  \n    var \n    description \n  \n \n\n  \n    dt \n    Date \n  \n  \n    wday \n    Day of the week \n  \n  \n    revenue \n    Daily revenue in hundreds, 11=1100 \n  \n  \n    expend \n    Daily expenses in hundreds \n  \n  \n    precip \n    Precipitation in mm \n  \n  \n    mint \n    Minimum temperature, Celsius \n  \n  \n    maxt \n    Maximum temperature, Celsius \n  \n  \n    source \n    Source of the weather data \n  \n\n\n\n\n\nYour tutor has the list of problems."
  },
  {
    "objectID": "week5/tutorial.html#finishing-up",
    "href": "week5/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 5",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week5/tutorialsol.html",
    "href": "week5/tutorialsol.html",
    "title": "ETC5521 Tutorial 5",
    "section": "",
    "text": "Refresh thinking about statistical inference.\nLearn to apply inference for data plots."
  },
  {
    "objectID": "week5/tutorialsol.html#objectives",
    "href": "week5/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 5",
    "section": "",
    "text": "Refresh thinking about statistical inference.\nLearn to apply inference for data plots."
  },
  {
    "objectID": "week5/tutorialsol.html#preparation",
    "href": "week5/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 5",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is Wickham et al.¬†(2010) Graphical inference for Infovis. It is a basic introduction to inference for exploratory data analysis, especially for data visualisation. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\ninstall.packages(c(\"tidyverse\", \"datarium\", \"broom\", \"nullabor\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week5/tutorialsol.html#exercises",
    "href": "week5/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 5",
    "section": "üì• Exercises",
    "text": "üì• Exercises"
  },
  {
    "objectID": "week5/tutorialsol.html#exercise-1-skittles-experiment",
    "href": "week5/tutorialsol.html#exercise-1-skittles-experiment",
    "title": "ETC5521 Tutorial 5",
    "section": "Exercise 1: Skittles experiment",
    "text": "Exercise 1: Skittles experiment\n\nSkittles come in five colors (orange, yellow, red, purple, green) each with their own flavours (orange, lemon, strawberry, grape, green apple). Data was collected by Dr Nick Tierney to explore whether a sample of 3 people could identify the flavour of skittles while blindfolded. You can find the cleaned tidy data here.\n\nHow many skittles did each person taste?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nskittle &lt;- read_csv(\"https://raw.githubusercontent.com/njtierney/skittles/master/data/skittles.csv\")\n\nEach person tasted 10 skittles.\n\nskittle %&gt;% \n  with(table(person))\n\nperson\n a  b  c \n10 10 10 \n\n\n\n\n\n\n\nA person with loss of taste is called ageusia and a person who has a loss of smell is called anosmia. The loss of taste and loss of smell will not allow you to distinguish flavours in food. What is the probability that a person with ageusia and anosmia will guess the skittle flavour correctly (out of the five flavours) for one skittle?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nIf a person cannot distinguish flavours then they will randomly choose one of the five flavours. So the probability that they select the correct flavour is 1/5.\n\n\n\n\n\nWhat is the probability that a person with ageusia and anosmia will guess the skittle flavour correctly for 2 out of 10 skittles, assuming the order of taste does not matter?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nSuppose \\(X\\) is the number of skittles that they correctly identified the flavour. Then assuming that the person cannot distinguish flavours and order of tasting the skittles does not matter, \\(X \\sim B(10, 0.2)\\). Then \\(P(X = 2) = {10 \\choose 2} 0.2^2 0.8^8\\approx 0.3\\). So there‚Äôs only about 30% chance such an event happens!\n\ndbinom(2, 10, 0.2)\n\n[1] 0.3\n\n\n\n\n\n\n\nTest the null hypothesis that people cannot distinguish the flavours correctly, against the alternative that they can. Assume that the order of tasting does not matter and each person has the same ability to correctly identify the flavours. In conducting your test, define your null and alternate hypothesis, in statistical notation, your assumptions, the test statistics and calculate the \\(p\\)-value.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nSuppose \\(X\\) is the number of skittles that a person identified the flavour correctly out of 30 skittles. Suppose each tasting is independent and has a equal probability of identifying the flavour correctly; we denote this probability as \\(p\\). We test the hypotheses: \\(H_0: p = 0.2\\) vs.¬†\\(H_1: p &gt; 0.2\\). Under \\(H_0\\), \\(X\\sim B(30, 0.2)\\) and therefore the \\(p\\)-value is \\(P(X \\geq 15) \\approx 0.0002\\). The \\(p\\)-value is small so the data supports that people can correctly identify the flavour of a skittle!\n\nsum(skittle$correct)\n\n[1] 15\n\n1 - pbinom(sum(skittle$correct), 30, 0.2)\n\n[1] 5.2e-05\n\n\n\n\n\n\n\nIn part (d) we disregarded the order of the tasting and the possible variability in people‚Äôs ability to correctly identify the flavour. If in fact these do matter, then how would you construct the test statistic? Is it easy?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nTo construct a test statistic, we need to construct a summary statistic with some known distribution under the null hypothesis (if using a parametric approach) with large (or extreme) values indicating rejection of the null hypothesis. Suppose that \\(X_1\\), \\(X_2\\) and \\(X_3\\) are the number of skittles out of 10 that person a, b and c, respectively, correctly identified. If each tasting is independent, then \\(X_1 \\sim B(10, p_1)\\), \\(X_2 \\sim B(10, p_2)\\) and \\(X_3 \\sim B(10, p_3)\\) where \\(p_i\\) is the probability that the \\(i\\)-th person correctly identifies the flavour of a skittle. Now under \\(H_0\\) you may assume that \\(p_1 = p_2 = p_3 = 0.2\\) and assuming each person is independent, \\(X_1 + X_2 + X_3 \\sim B(30, 0.2)\\). Same as (d)! However, if we know remove the assumption that each tasting is independent (so the order of tasting does matter), then the distribution of the test statistic does not hold true any longer.\n\n\n\n\n\nConsider the plot below that shows in each tile whether a person guessed correctly by order of their tasting. Suppose that under the null hypothesis, the order of tasting does not matter and people have no ability to distinguish the flavours. Generate a null plot under this null hypothesis.\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe null plot is constructed as follows.\n\ngtile &lt;- skittle %&gt;% \n  ggplot(aes(factor(order), person, fill = factor(correct))) + \n  geom_tile(color = \"black\", size = 2) +\n  coord_equal() + \n  scale_fill_viridis_d() +\n  labs(x = \"Order\", y = \"Person\", fill = \"Correct\")\ngtile\n\n\n\n\n\n\n\n\n\nset.seed(1)\nmethod &lt;- null_dist(\"correct\", \"binom\", list(size = 1, prob = 0.2))\ngtile %+% method(skittle)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on (f), construct a lineup (using nullabor or otherwise) of 20 plots. Ask your classmate, which plot looks different.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nlineup_df &lt;- lineup(method, true = skittle)\n\ndecrypt(\"9inr Mqmq SV 0pwSmSpV lN\")\n\ngtile %+% lineup_df +\n  facet_wrap(~.sample) +\n  guides(fill = FALSE) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\ndecrypt(\"h8RX 5IvI ne TAynvnAe YL\")\n\n[1] \"vKh1 lgNg uj GqJuNuqj  NA\"\n\n\n\n\n\n\n\nSuppose that you have a response from 100 people based on your line-up from (g) and 76 correctly identified the data plot. What is the \\(p\\)-value from this visual inference?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe suppose that each person has the same ability to identify the data plot. If we let \\(X\\) be the number of people who correctly identified the data plot in the lineup, then \\(X \\sim B(100, p)\\). The visual inference \\(p\\)-value is calculated from testing the hypotheses \\(H_0: p = 0.05\\) vs \\(H_1: p \\neq 0.05\\), and so is \\(P(X\\geq 76) \\approx 0\\). The visual inference \\(p\\)-value is very small so there is strong evidence to believe that the structure in the data deviates away from the null distribution!\n\n1 - pbinom(75, 100, 0.05)\n\n[1] 0\n\n\n\n\n\n\n\nNow consider the plot below. Use the same null data in (g) to construct a lineup based on below visual statistic. Suppose we had 28 people out of 100 who correctly identified the data plot in this lineup. What is the difference in power of visual statistic in (f) and this one?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\ngbar &lt;- skittle %&gt;% \n  mutate(person = fct_reorder(person, correct, sum)) %&gt;% \n  group_by(person) %&gt;% \n  summarise(correct = sum(correct)) %&gt;% \n  ggplot(aes(person, correct)) + \n  geom_col() +\n  labs(x = \"Person\", y = \"Correct\") +\n  geom_hline(yintercept = 2, linetype = \"dashed\")\ngbar\n\n\n\n\n\n\n\n\n\ngbar %+% lineup_df +\n  facet_wrap(~.sample) +\n  guides(fill = FALSE) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\ndecrypt(\"h8RX 5IvI ne TAynvnAe YL\")\n\n[1] \"vKh1 lgNg uj GqJuNuqj  NA\"\n\n\nThe estimated power of visual statistic in (f) is 76% and for the barplot is 26%. So the difference in power is 50%."
  },
  {
    "objectID": "week5/tutorialsol.html#exercise-2-social-media-marketing",
    "href": "week5/tutorialsol.html#exercise-2-social-media-marketing",
    "title": "ETC5521 Tutorial 5",
    "section": "Exercise 2: Social media marketing",
    "text": "Exercise 2: Social media marketing\nThe data marketing in the datarium R-package contains information on sales with advertising budget for three advertising media (youtube, facebook and newspaper). This advertising experiment was repeated 200 times to study the impact of the advertisting media on sales.\n\ndata(marketing, package = \"datarium\")\n\n\nStudy the pairs plot. Which of the advertising medium do you think affects the sales?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nGGally::ggpairs(marketing)\n\n\n\n\n\n\n\n\nThe pairs plot suggest that advertising on youtube is highly correlated with the sales and advertising on facebook is moderately correlated with the sales. Newspaper advertisement does not appear to be correlated highly with the sales.\n\n\n\n\n\nConstruct a coplot for sales vs advertising budget for facebook conditioned on advertising budget for youtube and newspaper. (You may like to make the intervals non-overlapping to make it easier to plot in ggplot). What do you see in the plot?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nmarketing %&gt;% \n  ggplot(aes(facebook, sales)) +\n  geom_point() + \n  facet_grid(cut_number(youtube, 4) ~ cut_number(newspaper, 4)) + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nThe newspaper does not seem to have much affect on the sales however it is noticeable that sales is linearly related to advertisement budget for facebook conditioned on youtube.\n\n\n\n\n\nNow construct a coplot for sales vs advertising budget for facebook conditioned on advertising budget for youtube alone. Superimpose a linear model on each facet. Is there an interval where the linear model is not a good fit?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nmarketing %&gt;% \n  ggplot(aes(facebook, sales)) +\n  geom_point() + \n  facet_wrap(~cut_number(youtube, 4)) + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nThere is a noticeably higher variability along the line in the above plot where advertisement budget for youtube is less than $90,000. There appears to be a linear relationship between facebook and sales (conditioned on advertisement budget on youtube), however the fitted lines all appear to have different slopes.\n\n\n\n\n\nConsider the following interaction model (which has the same symbolic model formulae as sales ~ facebook*youtube) for data where the advertising budget for youtube is at least $90,000. Construct a QQ-plot of the residuals. Do you think the errors are normally distributed? Construct a lineup for the QQ-plot assuming that the null distribution is Normally distributed with mean zero and variance as estimated from the model fit.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nset.seed(1149)\nfit &lt;- lm(sales ~ facebook + youtube + facebook:youtube, data = filter(marketing, youtube &gt; 90))\n\ngqq &lt;- augment(fit) %&gt;% \n  ggplot(aes(sample = .resid)) +\n  stat_qq() +\n  stat_qq_line()\ngqq\n\n\n\n\n\n\n\ngqq %+% lineup(null_dist(\".resid\", \"norm\", list(mean = 0, sd = sigma(fit))),\n               true = augment(fit)) +\n  facet_wrap(~.sample) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(), \n        aspect.ratio = 1)\n\n\n\n\n\n\n\n# &gt; decrypt(\"clZx bKhK oL 3OHohoOL 0d\")\n# [1] \"True data in position  13\""
  },
  {
    "objectID": "week5/tutorialsol.html#exercise-3-ida-skill-sprint",
    "href": "week5/tutorialsol.html#exercise-3-ida-skill-sprint",
    "title": "ETC5521 Tutorial 5",
    "section": "Exercise 3: IDA skill sprint",
    "text": "Exercise 3: IDA skill sprint\nSet the timer. You have 15 minutes to discover as many problems as possible in this data, cafe.rda.\nA small cafe in the city of Melbourne is interested in determining whether the daily earnings depend on the weather. They compiled data for a period over 2000-2001 to study this question. The data has the following variables:\n\n\n\n \n  \n    var \n    description \n  \n \n\n  \n    dt \n    Date \n  \n  \n    wday \n    Day of the week \n  \n  \n    revenue \n    Daily revenue in hundreds, 11=1100 \n  \n  \n    expend \n    Daily expenses in hundreds \n  \n  \n    precip \n    Precipitation in mm \n  \n  \n    mint \n    Minimum temperature, Celsius \n  \n  \n    maxt \n    Maximum temperature, Celsius \n  \n  \n    source \n    Source of the weather data \n  \n\n\n\n\n\nYour tutor has the list of problems."
  },
  {
    "objectID": "week5/tutorialsol.html#finishing-up",
    "href": "week5/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 5",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week5/index.html",
    "href": "week5/index.html",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "",
    "text": "Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions"
  },
  {
    "objectID": "week5/index.html#main-reference",
    "href": "week5/index.html#main-reference",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "",
    "text": "Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions"
  },
  {
    "objectID": "week5/index.html#what-you-will-learn-this-week",
    "href": "week5/index.html#what-you-will-learn-this-week",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nNumeric and visual summaries for a single variable\nCommon features to discover\nTools for inference for a single variable\nImputing missings on a single variable"
  },
  {
    "objectID": "week5/index.html#lecture-slides",
    "href": "week5/index.html#lecture-slides",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week5/index.html#tutorial-instructions",
    "href": "week5/index.html#tutorial-instructions",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week5/index.html#assignments",
    "href": "week5/index.html#assignments",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 2 is due on Tuesday 27 August."
  },
  {
    "objectID": "week4/slides.html#what-this-class-is-about",
    "href": "week4/slides.html#what-this-class-is-about",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What this class is about",
    "text": "What this class is about"
  },
  {
    "objectID": "week4/slides.html#revisiting-hypothesis-testing",
    "href": "week4/slides.html#revisiting-hypothesis-testing",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Revisiting hypothesis testing",
    "text": "Revisiting hypothesis testing"
  },
  {
    "objectID": "week4/slides.html#frequentist-hypothesis-testing-framework",
    "href": "week4/slides.html#frequentist-hypothesis-testing-framework",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "(Frequentist) hypothesis testing framework",
    "text": "(Frequentist) hypothesis testing framework\n\n\nSuppose \\(X\\) is the number of heads out of \\(n\\) independent tosses.\nLet \\(p\\) be the probability of getting a  for this coin.\n\nHypotheses\n\\(H_0: p = 0.5\\) vs.¬†\\(H_a: p &gt; 0.5\\). Note \\(p_0=0.5\\).  Alternative \\(H_a\\) is saying we believe that the coin is biased to heads. \nNOTE: Alternative needs to be decided before seeing data.\n\nAssumptions Each toss is independent with equal chance of getting a head.\n\n\nTest statistic\n\\(X \\sim B(n, p)\\). Recall \\(E(X\\mid H_0) = np_0\\). We observe \\(n, x, \\widehat{p}\\). Test statistic is \\(\\widehat{p} - p_0\\).\n\n\nP-value  (or critical value or confidence interval) \\(P(X ~ \\geq ~ x\\mid H_0)\\)\n\n\nConclusion Reject null hypothesis when the \\(p\\)-value is less than some significance level \\(\\alpha\\). Usually \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "week4/slides.html#testing-coin-bias-14",
    "href": "week4/slides.html#testing-coin-bias-14",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Testing coin bias (1/4)",
    "text": "Testing coin bias (1/4)\n\nSuppose I have a coin that I‚Äôm going to flip  \nIf the coin is unbiased, what is the probability it will show heads?\n\n\n\nYup, the probability should be 0.5.\nSo how would I test if a coin is biased or unbiased?\nWe‚Äôll collect some data."
  },
  {
    "objectID": "week4/slides.html#testing-coin-bias-24",
    "href": "week4/slides.html#testing-coin-bias-24",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Testing coin bias (2/4)",
    "text": "Testing coin bias (2/4)\n\nExperiment 1: I flipped the coin 10 times and this is the result:\n\n\n\nset.seed(924)\nsamp10 &lt;- sample(rep(c(head, tail), c(7, 3)))\ncat(paste0(samp10, collapse = \"\"))\n\n\n\n\nThe result is 7 head and 3 tails. So 70% are heads.\nDo you believe the coin is biased based on this data?"
  },
  {
    "objectID": "week4/slides.html#testing-coin-bias-34",
    "href": "week4/slides.html#testing-coin-bias-34",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Testing coin bias (3/4)",
    "text": "Testing coin bias (3/4)\n\nExperiment 2: Suppose now I flip the coin 100 times and this is the outcome:\n\n\nsamp100 &lt;- sample(rep(c(head, tail), c(70, 30)))\ncat(paste0(samp100, collapse = \"\"))\n\n\n\nWe observe 70 heads and 30 tails. So again 70% are heads.\nBased on this data, do you think the coin is biased?"
  },
  {
    "objectID": "week4/slides.html#testing-coin-bias-44",
    "href": "week4/slides.html#testing-coin-bias-44",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Testing coin bias (4/4)",
    "text": "Testing coin bias (4/4)\nCalculate it\n\n\nExperiment 1 (n=10)\n\nWe observed \\(x=7\\), or \\(\\widehat{p} = 0.7\\).\nAssuming \\(H_0\\) is true, we expect \\(np=10\\times 0.5=5\\).\nCalculate the \\(P(X \\geq 7)\\)\n\n \n\n\nsum(dbinom(7:10, 10, 0.5))\n\n[1] 0.17\n\n\n\n\nExperiment 2 (n=100)\n\nWe observed \\(x=70\\), or \\(\\widehat{p} = 0.7\\).\nAssuming \\(H_0\\) is true, we expect \\(np=100\\times 0.5=50\\).\nCalculate the \\(P(X \\geq 70)\\)\n\n \n\n\nsum(dbinom(70:100, 100, 0.5))\n\n[1] 3.9e-05"
  },
  {
    "objectID": "week4/slides.html#why-is-the-null-hypothesis-always-specific",
    "href": "week4/slides.html#why-is-the-null-hypothesis-always-specific",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Why is the null hypothesis always specific?",
    "text": "Why is the null hypothesis always specific?\nYou need to be able to calculate the probability of something happening, if the null was true."
  },
  {
    "objectID": "week4/slides.html#judicial-system",
    "href": "week4/slides.html#judicial-system",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Judicial system",
    "text": "Judicial system\n\n\n\n\n\n\n\n\n\nEvidence by test statistic Judgement by \\(p\\)-value, critical value or confidence interval\n\n\nDoes the test statistic have to be numerical?"
  },
  {
    "objectID": "week4/slides.html#visual-inference",
    "href": "week4/slides.html#visual-inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference",
    "text": "Visual inference"
  },
  {
    "objectID": "week4/slides.html#visual-inference-1",
    "href": "week4/slides.html#visual-inference-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference",
    "text": "Visual inference\n\n\n\nHypothesis testing in a visual inference framework is where:\n\nthe test statistic is a plot and\njudgement is by human visual perception.\n\n\n\nWhy is the plot a test statistic? We‚Äôll see why soon.\n\n\n\n\nYou, we, me actually do visual inference many times but generally in an informal fashion.\nThe problem with doing this is we are making an inference on whether the plot has any patterns based on a single data plot.\nThe single data plot needs to be examined in the context of what might this look like if different samples were shown."
  },
  {
    "objectID": "week4/slides.html#reasons-to-use-visual-inference",
    "href": "week4/slides.html#reasons-to-use-visual-inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Reasons to use visual inference",
    "text": "Reasons to use visual inference\n\nData plots tend to be over-interpreted.\nReading data plots requires calibration."
  },
  {
    "objectID": "week4/slides.html#visual-inference-more-formally",
    "href": "week4/slides.html#visual-inference-more-formally",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference more formally",
    "text": "Visual inference more formally\n\n\n\nState your null and alternate hypotheses.\nDefine a visual test statistic, \\(V(.)\\), i.e.¬†a function of a sample to a plot.\nDefine a method to generate null data, \\(\\boldsymbol{y}_0\\).\n\\(V(\\boldsymbol{y})\\) maps the actual data, \\(\\boldsymbol{y}\\), to the plot. We call this the data plot.\n\\(V(\\boldsymbol{y}_0)\\) maps a null data to a plot of the same form. We call this the null plot. We repeat this \\(m - 1\\) times to generate \\(m-1\\) null plots.\nA lineup displays these \\(m\\) plots in a random order.\nAsk \\(n\\) human viewers to select a plot in the lineup that looks different to others without any context given."
  },
  {
    "objectID": "week4/slides.html#visual-inference-more-formally-1",
    "href": "week4/slides.html#visual-inference-more-formally-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference more formally",
    "text": "Visual inference more formally\n\n\n\nState your null and alternate hypotheses.\nDefine a visual test statistic, \\(V(.)\\), i.e.¬†a function of a sample to a plot.\nDefine a method to generate null data, \\(\\boldsymbol{y}_0\\).\n\\(V(\\boldsymbol{y})\\) maps the actual data, \\(\\boldsymbol{y}\\), to the plot. We call this the data plot.\n\\(V(\\boldsymbol{y}_0)\\) maps a null data to a plot of the same form. We call this the null plot. We repeat this \\(m - 1\\) times to generate \\(m-1\\) null plots.\nA lineup displays these \\(m\\) plots in a random order.\nAsk \\(n\\) human viewers to select a plot in the lineup that looks different to others without any context given.\n\n\n\nSuppose \\(x\\) out of \\(n\\) people detected the data plot from a lineup, then\n\nthe visual inference p-value is given as \\[P(X \\geq x)\\] where \\(X \\sim B(n, 1/m)\\), and\nthe power of a lineup is estimated as \\(x/n\\)."
  },
  {
    "objectID": "week4/slides.html#two-residual-plots-examples-seen-last-week",
    "href": "week4/slides.html#two-residual-plots-examples-seen-last-week",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Two residual plots examples seen last week",
    "text": "Two residual plots examples seen last week"
  },
  {
    "objectID": "week4/slides.html#lineup-which-plot-has-a-pattern-that-is-different-from-other-plots",
    "href": "week4/slides.html#lineup-which-plot-has-a-pattern-that-is-different-from-other-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Lineup: Which plot has a pattern that is different from other plots?",
    "text": "Lineup: Which plot has a pattern that is different from other plots?\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals from dist~speed using datasets::cars (week 3).\n\nlm(dist ~ speed, data = cars)\n\n\n\nThis is a lineup of the residual plot\nWhich plot (if any) looks different from the others?\nWhy do you think it looks different?\n\n\n\n\n&gt; decrypt(\"clZx bKhK oL 3OHohoOL 0B\")\n[1] \"True data in position  11\"\n  How do we calculate statistical significance from this?"
  },
  {
    "objectID": "week4/slides.html#visual-inference-p-value-or-see-value",
    "href": "week4/slides.html#visual-inference-p-value-or-see-value",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference \\(p\\)-value (or ‚Äúsee‚Äù-value)",
    "text": "Visual inference \\(p\\)-value (or ‚Äúsee‚Äù-value)\n\n\n\nSo \\(x\\) out of \\(n\\) people chose the data plot.\nSo the visual inference \\(p\\)-value is \\(P(X \\geq x)\\) where \\(X \\sim B(n, 1/10)\\).\nIn R, this is\n\n1 - pbinom(x - 1, n, 1/20) \n# OR \nnullabor::pvisual(x, n, 20)\n\nThe calculation is made with the assumption that the chance of a single observer randomly chooses the true plot is 1/20.\n\n\nSuppose \\(x=2\\) out of \\(n=16\\) people chose plot 11 (previous slide).\nThe probability that this happens by random guessing (p-value) is\n\n1 - pbinom(2 - 1, 16, 1/20)\n\n[1] 0.19\n\nnullabor::pvisual(2, 16, 20)\n\n     x simulated binom\n[1,] 2       0.2  0.19"
  },
  {
    "objectID": "week4/slides.html#lineup-which-plot-has-a-pattern-that-is-different-from-other-plots-1",
    "href": "week4/slides.html#lineup-which-plot-has-a-pattern-that-is-different-from-other-plots-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Lineup: Which plot has a pattern that is different from other plots?",
    "text": "Lineup: Which plot has a pattern that is different from other plots?\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals from log-transformed price~carat ggplot2::diamonds (week 3).\n\nd_fit &lt;- lm(lprice ~ lcarat, data=diamonds)\n\n\n\nThis is a lineup of the residual plot for the model where both carat and price are log-transformed\nWhich plot (if any) looks different from the others?\nWhy do you think it looks different?\n\n\n\n\n&gt; decrypt(\"clZx bKhK oL 3OHohoOL 0Q\")\n[1] \"True data in position  15\""
  },
  {
    "objectID": "week4/slides.html#visual-inference-p-value-or-see-value-1",
    "href": "week4/slides.html#visual-inference-p-value-or-see-value-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference \\(p\\)-value (or ‚Äúsee‚Äù-value)",
    "text": "Visual inference \\(p\\)-value (or ‚Äúsee‚Äù-value)\n\n\nSuppose \\(x=8\\) out of \\(n=12\\) people chose plot 15 (previous slide).\nThe probability that this happens by random guessing (p-value) is\n\n1 - pbinom(8 - 1, 12, 1/20)\n\n[1] 1.6e-08\n\nnullabor::pvisual(8, 12, 20)\n\n     x simulated   binom\n[1,] 8         0 1.6e-08\n\n\n\nThis is basically impossible to happen by chance.\n Next, how the residuals are different from ‚Äúgood‚Äù residuals has to be determined by the follow-up question: how did you decide your chosen plot was different?\n Plot 15 has a different variance pattern, it‚Äôs not the regular up-down pattern seen in the other plots. This suggests that there is some heteroskedasticity in the data that is not captured by the error distribution in the model."
  },
  {
    "objectID": "week4/slides.html#new-residual-plot-examples",
    "href": "week4/slides.html#new-residual-plot-examples",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "New residual plot examples",
    "text": "New residual plot examples"
  },
  {
    "objectID": "week4/slides.html#residual-plot-13",
    "href": "week4/slides.html#residual-plot-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plot (1/3)",
    "text": "Residual plot (1/3)\n\n\n Is there a problem with the model?"
  },
  {
    "objectID": "week4/slides.html#residual-plot-23",
    "href": "week4/slides.html#residual-plot-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plot (2/3)",
    "text": "Residual plot (2/3)\n\n\n Is there a problem with the model?"
  },
  {
    "objectID": "week4/slides.html#residual-plot-33",
    "href": "week4/slides.html#residual-plot-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plot (3/3)",
    "text": "Residual plot (3/3)\n\n\n Is there a problem with the model?"
  },
  {
    "objectID": "week4/slides.html#residual-plots-need-context",
    "href": "week4/slides.html#residual-plots-need-context",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plots need context",
    "text": "Residual plots need context\n You are asked to decide IF THERE IS NO PATTERN. This is hard!  \nResidual plots are better when viewed in the context of good residual plots, where we know the assumptions of the model are satisfied."
  },
  {
    "objectID": "week4/slides.html#which-is-the-worst-residual-plot",
    "href": "week4/slides.html#which-is-the-worst-residual-plot",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Which is the worst residual plot?",
    "text": "Which is the worst residual plot?\n\n\n\n\n\n\n\n\n\n\n\n\n19 of these plots are good residual (null) plots."
  },
  {
    "objectID": "week4/slides.html#section",
    "href": "week4/slides.html#section",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "All of the residual plots shown slides 22-24 were NULL plots."
  },
  {
    "objectID": "week4/slides.html#section-1",
    "href": "week4/slides.html#section-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The actual residual plot is"
  },
  {
    "objectID": "week4/slides.html#its-not-only-for-residual-plots",
    "href": "week4/slides.html#its-not-only-for-residual-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "It‚Äôs not only for residual plots",
    "text": "It‚Äôs not only for residual plots"
  },
  {
    "objectID": "week4/slides.html#sports-analytics-basketball",
    "href": "week4/slides.html#sports-analytics-basketball",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Sports analytics: basketball",
    "text": "Sports analytics: basketball\n\n\n Which plot is most different?"
  },
  {
    "objectID": "week4/slides.html#time-series-cross-currency-rates",
    "href": "week4/slides.html#time-series-cross-currency-rates",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Time series: cross-currency rates",
    "text": "Time series: cross-currency rates\n\n\n Which plot is most different?"
  },
  {
    "objectID": "week4/slides.html#association-cars",
    "href": "week4/slides.html#association-cars",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Association: cars",
    "text": "Association: cars\n\n\n Which plot is most different?"
  },
  {
    "objectID": "week4/slides.html#spatial-analysis-cancer-incidence",
    "href": "week4/slides.html#spatial-analysis-cancer-incidence",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Spatial analysis: cancer incidence",
    "text": "Spatial analysis: cancer incidence\n\n\n Which plot is most different?\n From Steff Kobakian‚Äôs Master‚Äôs thesis"
  },
  {
    "objectID": "week4/slides.html#reading-any-plot-is-easier-in-the-context-of-null-plots",
    "href": "week4/slides.html#reading-any-plot-is-easier-in-the-context-of-null-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Reading any plot is easier in the context of null plots",
    "text": "Reading any plot is easier in the context of null plots"
  },
  {
    "objectID": "week4/slides.html#why-is-a-data-plot-a-statistic",
    "href": "week4/slides.html#why-is-a-data-plot-a-statistic",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Why is a data plot a statistic?",
    "text": "Why is a data plot a statistic?"
  },
  {
    "objectID": "week4/slides.html#why-is-a-data-plot-a-statistic-12",
    "href": "week4/slides.html#why-is-a-data-plot-a-statistic-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Why is a data plot a statistic? (1/2)",
    "text": "Why is a data plot a statistic? (1/2)\n\nThe concept of tidy data matches elementary statistics\nTabular form puts variables in columns and observations in rows\n\n\\[X = \\left[ \\begin{array}{rrrr}\n           X_1 & X_2 & ... & X_p\n           \\end{array} \\right] \\\\\n  = \\left[ \\begin{array}{rrrr}\n           X_{11} & X_{12} & ... & X_{1p} \\\\\n           X_{21} & X_{22} & ... & X_{2p} \\\\\n           \\vdots & \\vdots & \\ddots& \\vdots \\\\\n           X_{n1} & X_{n2} & ... & X_{np}\n           \\end{array} \\right]\\]\n\nVariables can have distributions, e.g.¬†\\(X_1 \\sim N(0,1), ~~X_2 \\sim \\text{Exp}(1) ...\\)"
  },
  {
    "objectID": "week4/slides.html#why-is-a-data-plot-a-statistic-22",
    "href": "week4/slides.html#why-is-a-data-plot-a-statistic-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Why is a data plot a statistic? (2/2)",
    "text": "Why is a data plot a statistic? (2/2)\n\n\nA statistic is a function on the values of items in a sample, e.g.¬†for \\(n\\) iid random variates \\(\\bar{X}_1=\\sum_{i=1}^n X_{i1}\\), \\(s_1^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_{i1}-\\bar{X}_1)^2\\)\nWe study the behaviour of the statistic over all possible samples of size \\(n\\).\nThe grammar of graphics is the mapping of (random) variables to graphical elements, making plots of data into statistics\n\n\n\n\n\nExample 1:\n\nggplot(threept_sub, \n       aes(x=angle, y=r)) + \n  geom_point(alpha=0.3)\n\n\n\nangle is mapped to the x axis\nr is mapped to the y axis\n\n\n\n\nExample 2:\n\nggplot(penguins, \n      aes(x=bl, \n          y=fl, \n          colour=species)) +\n  geom_point()\n\n\n\nbl is mapped to the x axis\nfl is mapped to the y axis\nspecies is mapped to colour\n\n\n\n\nExample 3:\n\nggplot(aud, aes(x=date, y=rate)) + \n  geom_line() \n\n\n\ndate is mapped to the x axis\nrate is mapped to the y axis\ndisplayed as a line geom"
  },
  {
    "objectID": "week4/slides.html#determining-the-null-hypothesis",
    "href": "week4/slides.html#determining-the-null-hypothesis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Determining the null hypothesis",
    "text": "Determining the null hypothesis"
  },
  {
    "objectID": "week4/slides.html#what-is-the-null-hypothesis-12",
    "href": "week4/slides.html#what-is-the-null-hypothesis-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is the null hypothesis? (1/2)",
    "text": "What is the null hypothesis? (1/2)\nTo determine the null hypothesis, you need to think about what pattern would NOT be interesting.\n\n\nA\nggplot(data) + \n  geom_point(aes(x=x1, y=x2))\n\nB\nggplot(data) + \n  geom_point(aes(x=x1, \n    y=x2, colour=cl))\n\nC\nggplot(data) + \n  geom_histogram(aes(x=x1))\n\nD\nggplot(data) + \n  geom_boxplot(aes(x=cl, y=x1))\n\n\n\n\nü§î Which of these plot definitions would most match to a null hypothesis stating there is no difference in the distribution between the groups?"
  },
  {
    "objectID": "week4/slides.html#what-is-the-null-hypothesis-22",
    "href": "week4/slides.html#what-is-the-null-hypothesis-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is the null hypothesis? (2/2)",
    "text": "What is the null hypothesis? (2/2)\n\n\n\nA\n\\(H_o:\\) no association between x1 and x2\n \nB\n\\(H_o:\\) no difference in association of between x1 and x2 between levels of cl\n\nC\n\\(H_o:\\) the distribution of x1 is XXX\n \nD\n\\(H_o:\\) no difference in the distribution of x1 between levels of cl"
  },
  {
    "objectID": "week4/slides.html#how-do-you-generate-null-samples",
    "href": "week4/slides.html#how-do-you-generate-null-samples",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How do you generate null samples",
    "text": "How do you generate null samples"
  },
  {
    "objectID": "week4/slides.html#primary-null-generating-mechanisms",
    "href": "week4/slides.html#primary-null-generating-mechanisms",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Primary null-generating mechanisms",
    "text": "Primary null-generating mechanisms\n\nNull samples can be generated using two basic approaches:\n\nPermutation: randomizing the order of one of the variables breaks association, but keeps marginal distributions the same.\nSimulation: from a given distribution, or model. Assumption is that the data comes from that model.\n\napplied to subsets, or conditioning on other variables. Simulation may require computing summary statistics from the data to use as parameter estimates."
  },
  {
    "objectID": "week4/slides.html#association-cars-1",
    "href": "week4/slides.html#association-cars-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Association: cars",
    "text": "Association: cars\n\n\n\n\n\n\n\n\n\n\n\n\n\nNull plots generated by permuting x variable."
  },
  {
    "objectID": "week4/slides.html#time-series-cross-currency-rates-1",
    "href": "week4/slides.html#time-series-cross-currency-rates-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Time series: cross-currency rates",
    "text": "Time series: cross-currency rates\n\n\n\n\n\n\n\n\n\n\n\n\n Nulls generated by simulating from an ARIMA model."
  },
  {
    "objectID": "week4/slides.html#beyond-p-value-to-power",
    "href": "week4/slides.html#beyond-p-value-to-power",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Beyond \\(p\\)-value to power",
    "text": "Beyond \\(p\\)-value to power"
  },
  {
    "objectID": "week4/slides.html#what-is-power",
    "href": "week4/slides.html#what-is-power",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is power?",
    "text": "What is power?\n\n\n\nA statistic is said to be more powerful than another statistic if it has a higher probability of correctly rejecting the null hypothesis when the alternative hypothesis is true.\nThe effectiveness of two plots designs for the same data can be compared by computing power from a lineup.\nThe power of a lineup is calculated as \\(x/n\\) where \\(x\\) is the number of people who detected the data plot out of \\(n\\) people."
  },
  {
    "objectID": "week4/slides.html#which-of-these-plots-is-more-effective-for-assessing-difference-between-groups",
    "href": "week4/slides.html#which-of-these-plots-is-more-effective-for-assessing-difference-between-groups",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Which of these plots is more effective for assessing difference between groups?",
    "text": "Which of these plots is more effective for assessing difference between groups?"
  },
  {
    "objectID": "week4/slides.html#computing-the-power",
    "href": "week4/slides.html#computing-the-power",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Computing the power",
    "text": "Computing the power\nNote: Different people evaluated each lineup.\n\n\n\n\n\n\n\n\n\nPlot type\n\\(x\\)\n\\(n\\)\nPower\n\n\n\n\ngeom_point\n\\(x_1=4\\)\n\\(n_1=23\\)\n\\(x_1 / n_1=0.174\\)\n\n\ngeom_boxplot\n\\(x_2=5\\)\n\\(n_2=25\\)\n\\(x_2 / n_2=0.185\\)\n\n\ngeom_violin\n\\(x_3=6\\)\n\\(n_3=29\\)\n\\(x_3 / n_3=0.206\\)\n\n\nggbeeswarm::geom_quasirandom\n\\(x_4=8\\)\n\\(n_4=24\\)\n\\(x_4 / n_4=0.333\\)\n\n\n\n\n\n\nThe plot type with a higher power is preferable\nYou can use this framework to find the optimal plot design"
  },
  {
    "objectID": "week4/slides.html#using-the-nullabor",
    "href": "week4/slides.html#using-the-nullabor",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Using the nullabor üì¶",
    "text": "Using the nullabor üì¶"
  },
  {
    "objectID": "week4/slides.html#section-2",
    "href": "week4/slides.html#section-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "When you run the example yourself, you get a decrypt code line, that you run after deciding on a plot to print the location of the data plot amongst the nulls.\n\nplot is a scatterplot, null hypothesis is there is no association between the two variables mapped to the x, y axes\nnull generating mechanism: permutation\n\n\n\nset.seed(20190709)\nggplot(lineup(null_permute('mpg'), mtcars), \n  aes(x=mpg, y=wt)) +\n  geom_point() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())"
  },
  {
    "objectID": "week4/slides.html#some-considerations-in-visual-inference",
    "href": "week4/slides.html#some-considerations-in-visual-inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Some considerations in visual inference",
    "text": "Some considerations in visual inference\n\nIn practice you don‚Äôt want to bias the judgement of the human viewers so for a proper visual inference:\n\nyou should not show the data plot before the lineup\nyou should not give the context of the data\nyou should remove labels and other identifying information from plots\n\nThese methods can be used whenever formal inference is not possible/available, for EDA or IDA or diagnosing models.\nThe data collection is vital for good inference: bad data leads to bad inference.\nDetermining how to generate null samples can be complicated. We‚Äôll see more examples throughout the next few weeks."
  },
  {
    "objectID": "week4/slides.html#resources",
    "href": "week4/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nBuja, Andreas, Dianne Cook, Heike Hofmann, Michael Lawrence, Eun-Kyung Lee, Deborah F. Swayne, and Hadley Wickham. 2009. ‚ÄúStatistical Inference for Exploratory Data Analysis and Model Diagnostics.‚Äù Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences 367 (1906): 4361‚Äì83.\nWickham, Hadley, Dianne Cook, Heike Hofmann, and Andreas Buja. 2010. ‚ÄúGraphical Inference for Infovis.‚Äù IEEE Transactions on Visualization and Computer Graphics 16 (6): 973‚Äì79.\nHofmann, H., L. Follett, M. Majumder, and D. Cook. 2012. ‚ÄúGraphical Tests for Power Comparison of Competing Designs.‚Äù IEEE Transactions on Visualization and Computer Graphics 18 (12): 2441‚Äì48.\nMajumder, M., Heiki Hofmann, and Dianne Cook. 2013. ‚ÄúValidation of Visual Statistical Inference, Applied to Linear Models.‚Äù Journal of the American Statistical Association 108 (503): 942‚Äì56.\n\n\n\n\nETC5521 Lecture 4 | ddde.numbat.space"
  },
  {
    "objectID": "week3/tutorial.html",
    "href": "week3/tutorial.html",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week3/tutorial.html#objectives",
    "href": "week3/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week3/tutorial.html#preparation",
    "href": "week3/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 3",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is EDA Case Study: Bay area blues. It is authored by Hadley Wickham, Deborah F. Swayne, and David Poole. It appeared in the book ‚ÄúBeautiful Data‚Äù edited by Jeff Hammerbacher and Toby Segaran. Not all the chapters in the book are good examples of data analysis, though.\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"forcats\", \"patchwork\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week3/tutorial.html#exercises",
    "href": "week3/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 3",
    "section": "üì• Exercises",
    "text": "üì• Exercises\nPoint your web browser to the github site for the analysis, https://github.com/hadley/sfhousing. The main data file is house-sales.csv. Read this data into your R session. (üõë ARE YOU USING A PROJECT FOR THIS UNIT? IF NOT, STOP and OPEN IT NOW.)\nYou can read the data in directly from the web site using this code:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(forcats)\nsales &lt;- read_csv(\"https://raw.githubusercontent.com/hadley/sfhousing/master/house-sales.csv\")\n\n\n1. What‚Äôs in the data?\n\nIs the data in tidy form?\nOf the variables in the data, which are\n\nnumeric?\ncategorical?\ntemporal?\n\nWhat would be an appropriate plot to make to examine the\n\nnumeric variables?\ncategorical variables?\na categorical and numeric variable?\na temporal variable and a numeric variable?\n\n\n\n\n2. Time series plots\nReproduce the time series plots of weekly average price and volume of sales.\n\n\n\n\n\n\n\n\n\n\n\n3. Correlation between series\nIt looks like volume goes down as price goes up. There is a better plot to make to examine this. What is it? Make the plot. After making the plot, report what you learn about the apparent correlation.\n\n\n4. Geographic differences\nThink about potential plots you might make for examining differences by geographic region (as measured by zip, county or city). Make a plot, and report what you learn.\n\n\n5. The Rich Get Richer and the Poor Get Poorer\nIn the section ‚ÄúThe Rich Get Richer and the Poor Get Poorer‚Äù there are some interesting transformations of the data, and unusual types of plots. Explain why looking at proportional change in value refines the view of price movement in different higher vs lower priced properties.\n\n\n6. Anything surprising?\nWere there any findings that surprised the authors? Or would surprise you?\n\n\n7. Additional resources\nSome of the findings were compared against information gathered from external sources. Can you point to an example of this, and how the other information was used to support or question the finding?"
  },
  {
    "objectID": "week3/tutorial.html#finishing-up",
    "href": "week3/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 3",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/tutorialsol.html",
    "href": "week3/tutorialsol.html",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week3/tutorialsol.html#objectives",
    "href": "week3/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week3/tutorialsol.html#preparation",
    "href": "week3/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 3",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is EDA Case Study: Bay area blues. It is authored by Hadley Wickham, Deborah F. Swayne, and David Poole. It appeared in the book ‚ÄúBeautiful Data‚Äù edited by Jeff Hammerbacher and Toby Segaran. Not all the chapters in the book are good examples of data analysis, though.\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"forcats\", \"patchwork\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week3/tutorialsol.html#exercises",
    "href": "week3/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 3",
    "section": "üì• Exercises",
    "text": "üì• Exercises\nPoint your web browser to the github site for the analysis, https://github.com/hadley/sfhousing. The main data file is house-sales.csv. Read this data into your R session. (üõë ARE YOU USING A PROJECT FOR THIS UNIT? IF NOT, STOP and OPEN IT NOW.)\nYou can read the data in directly from the web site using this code:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(forcats)\nsales &lt;- read_csv(\"https://raw.githubusercontent.com/hadley/sfhousing/master/house-sales.csv\")\n\n\n1. What‚Äôs in the data?\n\nIs the data in tidy form?\nOf the variables in the data, which are\n\nnumeric?\ncategorical?\ntemporal?\n\nWhat would be an appropriate plot to make to examine the\n\nnumeric variables?\ncategorical variables?\na categorical and numeric variable?\na temporal variable and a numeric variable?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nYes\n\nprice, br, lsqft, bsqft\ncounty, city, zip, street\nyear, date, datesold\n\n\nscatterplots\nbar charts, pie charts, mosaic\nfacet by the categorical variable. could be boxplots, or density plots, or facetted scatterplots to look at multiple numeric variables\n\ntime series plot, connect lines to indicate time, maybe need to aggregate over time to get one value per time point\n\n\n\n\n\n\n\n\n2. Time series plots\nReproduce the time series plots of weekly average price and volume of sales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNote, stacking time series plots helps compare the series relative to the time point.\n\nsales_weekly &lt;- sales |&gt;\n  group_by(date) |&gt;\n  summarise(av_price = mean(price, na.rm=TRUE),\n            volume = n())\np1 &lt;- ggplot(sales_weekly, aes(x=date,\n                               y=av_price)) +\n  geom_line() +\n  scale_y_continuous(\"Average price (millions)\", \n              breaks = seq(500000, 800000, 50000), \n              labels = c(\"0.50\", \"0.55\", \"0.60\", \"0.65\",\n                         \"0.70\", \"0.75\", \"0.80\")) +\n  scale_x_date(\"\", date_breaks = \"1 years\", \n               minor_breaks = NULL, \n               date_labels = \"%Y\") +\n  theme(aspect.ratio = 0.5)\np2 &lt;- ggplot(sales_weekly, aes(x=date, y=volume)) + geom_line() +\n  scale_y_continuous(\"Number of sales\", \n              breaks = seq(500,3000,500), \n              labels = c(\"500\", \"1,000\", \"1,500\", \"2,000\",\n                         \"2,500\", \"3,000\")) +\n  scale_x_date(\"\", date_breaks = \"1 years\", \n               minor_breaks = NULL, \n               date_labels = \"%Y\") +\n  theme(aspect.ratio = 0.5)\np1 + p2 + plot_layout(ncol=1)\n\n\n\n\n\n\n\n3. Correlation between series\nIt looks like volume goes down as price goes up. There is a better plot to make to examine this. What is it? Make the plot. After making the plot, report what you learn about the apparent correlation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nggplot(sales_weekly, aes(x=av_price, y=volume)) +\n  geom_point() +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\nAny correlation is very weak, and negative.\n\n\n\n\n\n\n4. Geographic differences\nThink about potential plots you might make for examining differences by geographic region (as measured by zip, county or city). Make a plot, and report what you learn.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nggplot(sales, \n       aes(x = fct_reorder(county, \n                  price, na.rm=TRUE), \n           y = price)) +\n         geom_boxplot() + \n  scale_y_log10() +\n  xlab(\"\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nMarin County has the highest prices on average, and San Joaquin the lowest. The lowest priced house was sold in Sonoma County. The highest priced properties and lowest priced are pretty similar from one county to another - that is, the variability within county is large.\n\n\n\n\n\n\n5. The Rich Get Richer and the Poor Get Poorer\nIn the section ‚ÄúThe Rich Get Richer and the Poor Get Poorer‚Äù there are some interesting transformations of the data, and unusual types of plots. Explain why looking at proportional change in value refines the view of price movement in different higher vs lower priced properties.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe transformation makes changes relative to the initial average price at the start of the time period. All curves produced will start from the same point. This means that we only need to compare the end points of each line, saving us from calculating differences between lines relative to the difference at the beginning.\n\n\n\n\n\n\n6. Anything surprising?\nWere there any findings that surprised the authors? Or would surprise you?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nI found it interesting that Mountain View had no decline in housing prices. This city has the headquarters of many of the world‚Äôs largest technology companies are in the city, including Google, Mozilla Foundation, Symantec, and Intuit.\n\n\n\n\n\n\n7. Additional resources\nSome of the findings were compared against information gathered from external sources. Can you point to an example of this, and how the other information was used to support or question the finding?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAll of this is consistent with what we have learned about subprime mortgages since the housing bust hit the headlines.\nSubprime mortgages were offered on little collateral which meant they were quite risky, and they tended to be on the lower end of the housing market. This information was in all the news headlines at the time, and the analysis that these authors have done was checked against the common reporting at the time. The data was consistent with these reports."
  },
  {
    "objectID": "week3/tutorialsol.html#finishing-up",
    "href": "week3/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 3",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/index.html",
    "href": "week3/index.html",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "",
    "text": "The initial examination of data"
  },
  {
    "objectID": "week3/index.html#main-reference",
    "href": "week3/index.html#main-reference",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "",
    "text": "The initial examination of data"
  },
  {
    "objectID": "week3/index.html#what-you-will-learn-this-week",
    "href": "week3/index.html#what-you-will-learn-this-week",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nRole of IDA\nTechniques for\n\ndata screening\ndata cleaning\nimputation\nvalidation\n\nChecking assumptions for hypothesis testing and fitting linear models"
  },
  {
    "objectID": "week3/index.html#lecture-slides",
    "href": "week3/index.html#lecture-slides",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week3/index.html#tutorial-instructions",
    "href": "week3/index.html#tutorial-instructions",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week3/index.html#assignments",
    "href": "week3/index.html#assignments",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week3/index.html#assignments-1",
    "href": "week3/index.html#assignments-1",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Monday 05 August."
  },
  {
    "objectID": "week2/slides.html#birth-of-eda",
    "href": "week2/slides.html#birth-of-eda",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Birth of EDA",
    "text": "Birth of EDA\n\nThe field of exploratory data analysis came of age when this book appeared in 1977.\n\nTukey held that too much emphasis in statistics was placed on statistical hypothesis testing (confirmatory data analysis); more emphasis needed to be placed on using data to suggest hypotheses to test."
  },
  {
    "objectID": "week2/slides.html#john-w.-tukey",
    "href": "week2/slides.html#john-w.-tukey",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "John W. Tukey",
    "text": "John W. Tukey\n\n\n\n\n\n\n Image source: wikimedia.org\n\n\nBorn in 1915, in New Bedford, Massachusetts.\nMum was a private tutor who home-schooled John. Dad was a Latin teacher.\nBA and MSc in Chemistry, and PhD in Mathematics\nAwarded the National Medal of Science in 1973, by President Nixon\nBy some reports, his home-schooling was unorthodox and contributed to his thinking and working differently."
  },
  {
    "objectID": "week2/slides.html#taking-a-glimpse-back-in-time",
    "href": "week2/slides.html#taking-a-glimpse-back-in-time",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Taking a glimpse back in time",
    "text": "Taking a glimpse back in time\nis possible with the American Statistical Association video lending library.\n We‚Äôre going to watch John Tukey talking about exploring high-dimensional data with an amazing new computer in 1973, four years before the EDA book.\n\nLook out for these things:\nTukey‚Äôs expertise is described as for trial and error learning and the computing equipment.\n\n\nFirst 4.25 minutes"
  },
  {
    "objectID": "week2/slides.html#setting-the-frame-of-mind",
    "href": "week2/slides.html#setting-the-frame-of-mind",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Setting the frame of mind",
    "text": "Setting the frame of mind\nExcerpt from the introduction\n\nThis book is based on an important principle.\n It is important to understand what you CAN DO before you learn to measure how WELL you seem to have DONE it.\n Learning first what you can do will help you to work more easily and effectively.\n This book is about exploratory data analysis, about looking at data to see what it seems to say. It concentrates on simple arithmetic and easy-to-draw pictures. It regards whatever appearances we have recognized as partial descriptions, and tries to look beneath them for new insights. Its concern is with appearance, not with confirmation.\n Examples, NOT case histories\n The book does not exist to make the case that exploratory data analysis is useful. Rather it exists to expose its readers and users to a considerable variety of techniques for looking more effectively at one‚Äôs data. The examples are not intended to be complete case histories. Rather they should isolated techniques in action on real data. The emphasis is on general techniques, rather than specific problems. \nA basic problem about any body of data is to make it more easily and effectively handleable by minds ‚Äì our minds, her mind, his mind. To this general end:\n\nanything that make a simpler description possible makes the description more easily handleable.\nanything that looks below the previously described surface makes the description more effective.\n\n\nSo we shall always be glad (a) to simplify description and (b) to describe one layer deeper. In particular,\n\nto be able to say that we looked one layer deeper, and found nothing, is a definite step forward ‚Äì though not as far as to be able to say that we looked deeper and found thus-and-such.\nto be able to say that ‚Äúif we change our point of view in the following way ‚Ä¶ things are simpler‚Äù is always a gain‚Äìthough not quite so much as to be able to say ‚Äúif we don‚Äôt bother to change out point of view (some other) things are equally simple.‚Äù\n\n ‚Ä¶\n Consistent with this view, we believe, is a clear demand that pictures based on exploration of data should force their messages upon us. Pictures that emphasize what we already know‚Äì‚Äúsecurity blankets‚Äù to reassure us‚Äìare frequently not worth the space they take. Pictures that have to be gone over with a reading glass to see the main point are wasteful of time and inadequate of effect. The greatest value of a picture is when it forces us to notice what we never expected to see.\n\n\nConfirmation\n\n\nThe principles and procedures of what we call confirmatory data analysis are both widely used and one of the great intellectual products of our century. In their simplest form, these principles and procedures look at a sample‚Äìand at what that sample has told us about the population from which it came‚Äìand assess the precision with which our inference from sample to population is made. We can no longer get along without confirmatory data analysis. But we need not start with it.\n\nThe best way to understand what CAN be done is not longer‚Äìif it ever was‚Äìto ask what things could, in the current state of our skill techniques, be confirmed (positively or negatively). Even more understanding is lost if we consider each thing we can do to data only in terms of some set of very restrictive assumptions under which that thing is best possible‚Äìassumptions we know we CANNOT check in practice.\n\nExploration AND confirmation\n\nOnce upon a time, statisticians only explored. Then they learned to confirm exactly‚Äìto confirm a few things exactly, each under very specific circumstances. As they emphasized exact confirmation, their techniques inevitably became less flexible. The connection of the most used techniques with past insights was weakened. Anything to which confirmatory procedure was not explicitly attached was decried as ‚Äúmere descriptive statistics‚Äù, no matter how much we learned from it.\n\nToday, the flexibility of (approximate) confirmation by the jacknife makes it relatively easy to ask, for almost any clearly specified exploration, ‚ÄúHow far is it confirmed?‚Äù\n\nToday, exploratory and confirmatory can‚Äìand should‚Äìproceed side by side. This book, of course, considers only exploratory techniques, leaving confirmatory techniques to other accounts.\n\n\n About the problems \n\n The teacher needs to be careful about assigning problems. Not too many, please. They are likely to take longer than you think. The number supplied is to accommodate diversity of interest, not to keep everybody busy.\n Besides the length of our problems, both teacher and student need to realise that many problems do not have a single ‚Äúright answer‚Äù. There can be many ways to approach a body of data. Not all are equally good. For some bodies of data this may be clear, but for others we may not be able to tell from a single body of data which approach is preferred. Even several bodies of data about very similar situations may not be enough to show which approach should be preferred. Accordingly, it will often be quite reasonable for different analysts to reach somewhat different analyses.\n Yet more‚Äìto unlock the analysis of a body of day, to find the good way to approach it, may require a key, whose finding is a creative act. Not everyone can be expected to create the key to any one situation. And to continue to paraphrase Barnum, no one can be expected to create a key to each situation he or she meets.\n To learn about data analysis, it is right that each of us try many things that do not work‚Äìthat we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed‚Äìat least until we were told about it‚Äìan opportunity to learn more. Each teacher needs to recognize this in grading and commenting on problems.\n\n\n Precision\n\nThe teacher who heeds these words and admits that there need be no one correct approach may, I regret to contemplate, still want whatever is done to be digit perfect. (Under such a requirement, the write should still be able to pass the course, but it is not clear whether she would get an ‚ÄúA‚Äù.) One does, from time to time, have to produce digit-perfect, carefully checked results, but forgiving techniques that are not too distributed by unusual data are also, usually, little disturbed by SMALL arithmetic errors. The techniques we discuss here have been chosen to be forgiving. It is hoped, then, that small arithmetic errors will take little off the problem‚Äôs grades, leaving severe penalties for larger errors, either of arithmetic or concept."
  },
  {
    "objectID": "week2/slides.html#outline",
    "href": "week2/slides.html#outline",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Outline",
    "text": "Outline\n\n\n\nScratching down numbers\nSchematic summary\nEasy re-expression\nEffective comparison\nPlots of relationship\nStraightening out plots (using three points)\nSmoothing sequences\nParallel and wandering schematic plots\nDelineations of batches of points\nUsing two-way analyses\n\n\n\n\n\nMaking two-way analyses\nAdvanced fits\nThree way fits\nLooking in two or more ways at batched of points\nCounted fractions\nBetter smoothing\nCounts in bin after bin\nProduct-ratio plots\nShapes of distributions\nMathematical distributions"
  },
  {
    "objectID": "week2/slides.html#looking-at-numbers-with-tukey",
    "href": "week2/slides.html#looking-at-numbers-with-tukey",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Looking at numbers with Tukey",
    "text": "Looking at numbers with Tukey"
  },
  {
    "objectID": "week2/slides.html#scratching-down-numbers",
    "href": "week2/slides.html#scratching-down-numbers",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scratching down numbers",
    "text": "Scratching down numbers\n\n\nPrices of Chevrolet in the local used car newspaper ads of 1968.\n\noptions(width=20)\nchevrolets &lt;- tibble(\n  prices = c(250, 150, 795, 895, 695, \n               1699, 1499, 1099, 1693, 1166,\n               688, 1333, 895, 1775, 895,\n               1895, 795))\n#chevrolets$prices\n\n\nStem-and-leaf plot: still seen in introductory statistics texts"
  },
  {
    "objectID": "week2/slides.html#section-1",
    "href": "week2/slides.html#section-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "First stem-and-leaf, first digit on stem, second digit on leaf\n\n\nOrder any leaves which need it, eg stem 6\n\n\n\n\nA benefit is that the numbers can be read off the plot, but the focus is still on the pattern. Also quantiles like the median, can be computed easily."
  },
  {
    "objectID": "week2/slides.html#section-2",
    "href": "week2/slides.html#section-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Shrink the stem\n\n\nShrink the stem more"
  },
  {
    "objectID": "week2/slides.html#and-in-r",
    "href": "week2/slides.html#and-in-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "And, in R ‚Ä¶",
    "text": "And, in R ‚Ä¶\n\nchevrolets$prices\n\n [1]  250  150  795\n [4]  895  695 1699\n [7] 1499 1099 1693\n[10] 1166  688 1333\n[13]  895 1775  895\n[16] 1895  795\n\nstem(chevrolets$prices)\n\n\n  The decimal point is 3 digit(s) to the right of the |\n\n  0 | 23\n  0 | 7788999\n  1 | 123\n  1 | 57789"
  },
  {
    "objectID": "week2/slides.html#remember-the-tips-data",
    "href": "week2/slides.html#remember-the-tips-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üîñ Remember the tips data",
    "text": "üîñ Remember the tips data\n\n\n [1] 1.01 1.66 3.50 3.31 3.61 4.71 2.00 3.12 1.96 3.23 1.71 5.00 1.57 3.00 3.02\n[16] 3.92 1.67 3.71 3.50 3.35 4.08 2.75 2.23 7.58 3.18 2.34 2.00 2.00 4.30 3.00\n[31] 1.45 2.50 3.00 2.45 3.27 3.60 2.00 3.07 2.31 5.00 2.24 2.54 3.06 1.32 5.60\n[46] 3.00 5.00 6.00 2.05 3.00\n\n\n\nstem(tips$tip, scale=0.5, width=120)\n\n\n  The decimal point is at the |\n\n   1 | 000001233334445555555555556666667777788889\n   2 | 000000000000000000000000000000000000000001122222223333555555555555556666677788899\n   3 | 00000000000000000000000011111112222222333344445555555555555666778889\n   4 | 0000000000001112233335777\n   5 | 00000000001122226799\n   6 | 05577\n   7 | 6\n   8 | \n   9 | 0\n  10 | 0"
  },
  {
    "objectID": "week2/slides.html#refining-the-size",
    "href": "week2/slides.html#refining-the-size",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Refining the size",
    "text": "Refining the size\n\n\nFive digits per stem\n\n\n\nWhat is the number in parentheses? And why might this be useful?\n\n\n\nTwo digits per stem\n\n\n\n\n\nstem(tips$tip, scale=2)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n   10 | 0000107\n   12 | 55526\n   14 | 44578000000000678\n   16 | 1346781356\n   18 | 032678\n   20 | 00000000000000000000000000000000011233598\n   22 | 0033440114\n   24 | 5700000000002456\n   26 | 01412455\n   28 | 382\n   30 | 00000000000000000000000267891245688\n   32 | 133557159\n   34 | 0188800000000015\n   36 | 0181566\n   38 | 2\n   40 | 0000000000006889\n   42 | 09004\n   44 | 0\n   46 | 713\n   48 | \n   50 | 000000000074567\n   52 | 0\n   54 | \n   56 | 05\n   58 | 52\n   60 | 0\n   62 | \n   64 | 00\n   66 | 03\n   68 | \n   70 | \n   72 | \n   74 | 8\n   76 | \n   78 | \n   80 | \n   82 | \n   84 | \n   86 | \n   88 | \n   90 | 0\n   92 | \n   94 | \n   96 | \n   98 | \n  100 | 0\n\n\n\n\nWhy no number in parentheses?\n\n\n\nmedian(tips$tip)\n\n[1] 2.9"
  },
  {
    "objectID": "week2/slides.html#summary",
    "href": "week2/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\nStem-and-leaf plots are similar information to the histogram.\nGenerally it is possible to also read off the numbers, and to then easily calculate median or Q1 or Q3.\nIt‚Äôs great for small data sets, when you only have pencil and paper.\nAlternatives are a histogram, (jittered) dotplot, density plot, box plot, violin plot, letter value plot."
  },
  {
    "objectID": "week2/slides.html#a-different-style-of-number-scratching",
    "href": "week2/slides.html#a-different-style-of-number-scratching",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "a different style of number scratching",
    "text": "a different style of number scratching\nfor categorical variables\n\n\nWe know about\n\nbut its too easy to\n\nmake a mistake\n\nIs this easier?\n\n\nor harder"
  },
  {
    "objectID": "week2/slides.html#count-this-data-using-the-squares-approach.",
    "href": "week2/slides.html#count-this-data-using-the-squares-approach.",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Count this data using the squares approach.",
    "text": "Count this data using the squares approach.\n\n\n\n\n [1] \"F\" \"M\" \"M\" \"M\" \"F\" \"M\"\n [7] \"M\" \"M\" \"M\" \"M\" \"M\" \"F\"\n[13] \"M\" \"M\" \"F\" \"M\" \"F\" \"M\"\n[19] \"F\" \"M\" \"M\" \"F\" \"F\" \"M\"\n[25] \"M\" \"M\" \"M\" \"M\" \"M\" \"F\"\n[31] \"M\" \"M\" \"F\" \"F\" \"M\" \"M\"\n[37] \"M\" \"F\" \"M\" \"M\" \"M\" \"M\"\n[43] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n[49] \"M\" \"M\" \"M\" \"F\" \"F\" \"M\"\n[55] \"M\" \"M\" \"M\" \"F\" \"M\" \"M\"\n[61] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n[67] \"F\" \"F\" \"M\" \"M\" \"M\" \"F\""
  },
  {
    "objectID": "week2/slides.html#what-does-it-mean-to-feel-what-the-data-are-like",
    "href": "week2/slides.html#what-does-it-mean-to-feel-what-the-data-are-like",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What does it mean to ‚Äúfeel what the data are like?‚Äù",
    "text": "What does it mean to ‚Äúfeel what the data are like?‚Äù"
  },
  {
    "objectID": "week2/slides.html#section-3",
    "href": "week2/slides.html#section-3",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "This is a stem and leaf of the height of the highest peak in each of the 50 US states.\n\nThe states roughly fall into three groups.\n\nIt‚Äôs not really surprising, but we can imagine this grouping. Alaska is in a group of its own, with a much higher high peak. Then the Rocky Mountain states, California, Washington and Hawaii also have high peaks, and the rest of the states lump together."
  },
  {
    "objectID": "week2/slides.html#section-4",
    "href": "week2/slides.html#section-4",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Exploratory data analysis is detective work ‚Äì in the purest sense ‚Äì finding and revealing the clues."
  },
  {
    "objectID": "week2/slides.html#section-5",
    "href": "week2/slides.html#section-5",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "More summaries of numerical values"
  },
  {
    "objectID": "week2/slides.html#hinges-and-5-number-summaries",
    "href": "week2/slides.html#hinges-and-5-number-summaries",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hinges and 5-number summaries",
    "text": "Hinges and 5-number summaries\n\n\n\n\n [1] -3.2 -1.7 -0.4  0.1\n [5]  0.3  1.2  1.5  1.8\n [9]  2.4  3.0  4.3  6.4\n[13]  9.8\n\n\nYou know the median is the middle number. What‚Äôs a hinge?\nThere are 13 data values here, provided already sorted. We are going to write them into a Tukey named down-up-down-up pattern, evenly.\nMedian will be 7th, hinge will be 4th from each end."
  },
  {
    "objectID": "week2/slides.html#hinges-and-5-number-summary",
    "href": "week2/slides.html#hinges-and-5-number-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hinges and 5-number summary",
    "text": "Hinges and 5-number summary\n\n\n\n\n\nHinges are almost always the same as Q1 and Q3"
  },
  {
    "objectID": "week2/slides.html#box-and-whisker-display",
    "href": "week2/slides.html#box-and-whisker-display",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "box-and-whisker display",
    "text": "box-and-whisker display\n\n\n\n\nStarting with a 5-number summary"
  },
  {
    "objectID": "week2/slides.html#box-and-whisker-display-1",
    "href": "week2/slides.html#box-and-whisker-display-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "box-and-whisker display",
    "text": "box-and-whisker display\n\n\nStarting with a 5-number summary"
  },
  {
    "objectID": "week2/slides.html#identified-end-values",
    "href": "week2/slides.html#identified-end-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Identified end values",
    "text": "Identified end values\n\n\n\nWhy are some individual points singled out?\n\n\nRules for this one may be clearer?"
  },
  {
    "objectID": "week2/slides.html#section-6",
    "href": "week2/slides.html#section-6",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Isn‚Äôt this imposing a belief?"
  },
  {
    "objectID": "week2/slides.html#section-7",
    "href": "week2/slides.html#section-7",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "There is no excuse for failing to plot and look\nAnother Tukey wisdom drop"
  },
  {
    "objectID": "week2/slides.html#fences-and-outside-values",
    "href": "week2/slides.html#fences-and-outside-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Fences and outside values",
    "text": "Fences and outside values\n\n\nH-spread: difference between the hinges (we would call this Inter-Quartile Range)\nstep: 1.5 times H-spread\ninner fences: 1 step outside the hinges\nouter fences: 2 steps outside the hinges\nthe value at each end closest to, but still inside the inner fence are ‚Äúadjacent‚Äù\nvalues between an inner fence and its neighbouring outer fence are ‚Äúoutside‚Äù\nvalues beyond outer fences are ‚Äúfar out‚Äù\nthese rules produce a SCHEMATIC PLOT"
  },
  {
    "objectID": "week2/slides.html#new-statistics-trimeans",
    "href": "week2/slides.html#new-statistics-trimeans",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "New statistics: trimeans",
    "text": "New statistics: trimeans\nThe number that comes closest to\n\\[\\frac{\\text{lower hinge} + 2\\times \\text{median} + \\text{upper hinge}}{4}\\] is the trimean.\n \nThink about trimmed means, where we might drop the highest and lowest 5% of observations."
  },
  {
    "objectID": "week2/slides.html#letter-value-plots",
    "href": "week2/slides.html#letter-value-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Letter value plots",
    "text": "Letter value plots\n\n\nWhy break the data into quarters? Why not eighths, sixteenths? k-number summaries?\nWhat does a 7-number summary look like?\n\nHow would you make an 11-number summary?\n\n\nlibrary(lvplot)\np &lt;- ggplot(mpg, \n            aes(class, hwy))\np + geom_lv(aes(fill=..LV..)) + \n  scale_fill_brewer() + \n  coord_flip() + \n  xlab(\"\")"
  },
  {
    "objectID": "week2/slides.html#box-plots-are-ubiquitous-in-use-today.",
    "href": "week2/slides.html#box-plots-are-ubiquitous-in-use-today.",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Box plots are ubiquitous in use today.",
    "text": "Box plots are ubiquitous in use today.\n - üêàüê© Mostly used to compare distributions, multiple subsets of the data.\n\nPuts the emphasis on the middle 50% of observations, although variations can put emphasis on other aspects."
  },
  {
    "objectID": "week2/slides.html#easy-re-expression",
    "href": "week2/slides.html#easy-re-expression",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Easy re-expression",
    "text": "Easy re-expression"
  },
  {
    "objectID": "week2/slides.html#logs-square-roots-reciprocals",
    "href": "week2/slides.html#logs-square-roots-reciprocals",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Logs, square roots, reciprocals",
    "text": "Logs, square roots, reciprocals\n\n\nWhat you need to know about logs?\n\nhow to find good enough logs fast and easily\nthat equal differences in logs correspond to equal ratios of raw values.\n\n(This means that wherever you find people using products or ratios‚Äì even in such things as price indexes‚Äìusing logs‚Äìthus converting producers to sums and ratios to differences‚Äìis likely to help.)\n\n\nThe most common transformations are logs, sqrt root, reciprocals, reciprocals of square roots\n\n-1, -1/2, +1/2, +1\n\nWhat happened to ZERO?\n\n\nIt turns out that the role of a zero power, is for the purposes of re-expression, neatly solved by the logarithm."
  },
  {
    "objectID": "week2/slides.html#re-express-to-symmetrize-the-distribution",
    "href": "week2/slides.html#re-express-to-symmetrize-the-distribution",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Re-express to symmetrize the distribution",
    "text": "Re-express to symmetrize the distribution"
  },
  {
    "objectID": "week2/slides.html#power-ladder",
    "href": "week2/slides.html#power-ladder",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Power ladder",
    "text": "Power ladder\n \n‚¨ÖÔ∏è fix RIGHT-skewed values  \n-2, -1, -1/2, 0 (log), 1/3, 1/2, 1, 2, 3, 4\n\nfix LEFT-skewed values ‚û°Ô∏è"
  },
  {
    "objectID": "week2/slides.html#section-8",
    "href": "week2/slides.html#section-8",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "We now regard re-expression as a tool, something to let us do a better job of grasping. The grasping is done with the eye and the better job is through a more symmetric appearance.\nAnother Tukey wisdom drop"
  },
  {
    "objectID": "week2/slides.html#linearising-bivariate-relationships",
    "href": "week2/slides.html#linearising-bivariate-relationships",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linearising bivariate relationships",
    "text": "Linearising bivariate relationships\n  \n\nSurprising observation: The small fluctuations in later years.\nWhat might be possible reasons?"
  },
  {
    "objectID": "week2/slides.html#linearising-bivariate-relationships-1",
    "href": "week2/slides.html#linearising-bivariate-relationships-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linearising bivariate relationships",
    "text": "Linearising bivariate relationships\n  \n\nSee some fluctuations in the early years, too. Note that the log transformation couldn‚Äôt linearise."
  },
  {
    "objectID": "week2/slides.html#section-9",
    "href": "week2/slides.html#section-9",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Whatever the data, we can try to gain by straightening or by flattening.\nWhen we succeed in doing one or both, we almost always see more clearly what is going on."
  },
  {
    "objectID": "week2/slides.html#rules-and-advice",
    "href": "week2/slides.html#rules-and-advice",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Rules and advice",
    "text": "Rules and advice\n\n\n\nGraphics are friendly.\nArithmetic often exists to make graphs possible.\nGraphs force us to notice the unexpected; nothing could be more important.\nDifferent graphs show us quite different aspects of the same data.\nThere is no more reason to expect one graph to ‚Äútell all‚Äù than to expect one number to do the same.\n‚ÄúPlotting \\(y\\) against \\(x\\)‚Äù involves significant choices‚Äìhow we express one or both variables can be crucial.\n\n\n\nThe first step in penetrating plotting is to straighten out the dependence or point scatter as much as reasonable.\nPlotting \\(y^2\\), \\(\\sqrt{y}\\), \\(log(y)\\), \\(-1/y\\) or the like instead of \\(y\\) is one plausible step to take in search of straightness.\nPlotting \\(x^2\\), \\(\\sqrt{x}\\), \\(log(x)\\), \\(-1/x\\) or the like instead of \\(x\\) is another.\nOnce the plot is straightened, we can usually gain much by flattening it, usually by plotting residuals.\nWhen plotting scatters, we may need to be careful about how we express \\(x\\) and \\(y\\) in order to avoid concealment by crowding."
  },
  {
    "objectID": "week2/slides.html#section-10",
    "href": "week2/slides.html#section-10",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The book is a digest of üåü tricks and treats üåü of massaging numbers and drafting displays.\nMany of the tools have made it into today‚Äôs analyses in various ways. Many have not.\nNotice the word developments too: froots, fences. Tukey brought you the word ‚Äúsoftware‚Äù\nThe temperament of the book is an inspiration for the mind-set for this unit. There is such delight in working with numbers!\n‚ÄúWe love data!‚Äù"
  },
  {
    "objectID": "week2/slides.html#take-aways",
    "href": "week2/slides.html#take-aways",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Take-aways",
    "text": "Take-aways\n\nTukey‚Äôs approach was a reaction to many years of formalising data analysis using statistical hypothesis testing.\nMethodology development in statistical testing was a reaction to the ad-hoc nature of data analysis.\nComplex machine learning models like neural networks are in reaction to the inability of statistical models to capture highly non-linear relationships, and depend heavily on the data provided.\nExploring data today is in reaction to the need to explain complex models, to support organisations against legal challenges to decisions made from the model\nIt is much easier to accomplish computers.\n‚ÄúExploratory data analysis‚Äù as commonly used today term is unfortunately synonymous with ‚Äúdescriptive statistics‚Äù, but it is truly much more. Understanding its history from Tukey‚Äôs advocation helps you see it is the tooling to discover what you don‚Äôt know."
  },
  {
    "objectID": "week2/slides.html#resources",
    "href": "week2/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nwikipedia\nJohn W. Tukey (1977) Exploratory data analysis\nData coding using tidyverse suite of R packages\nSketching canvases made using fabricerin\n\n\n\n\nETC5521 Lecture 2 | ddde.numbat.space"
  },
  {
    "objectID": "week12/index.html#what-you-will-learn-this-week",
    "href": "week12/index.html#what-you-will-learn-this-week",
    "title": "Week 12: Extending beyond the data, what can and cannot be inferred more generally, given the data collection",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week12/index.html#lecture-slides",
    "href": "week12/index.html#lecture-slides",
    "title": "Week 12: Extending beyond the data, what can and cannot be inferred more generally, given the data collection",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week12/index.html#tutorial-instructions",
    "href": "week12/index.html#tutorial-instructions",
    "title": "Week 12: Extending beyond the data, what can and cannot be inferred more generally, given the data collection",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week12/index.html#assignments",
    "href": "week12/index.html#assignments",
    "title": "Week 12: Extending beyond the data, what can and cannot be inferred more generally, given the data collection",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week12/index.html#assignments-1",
    "href": "week12/index.html#assignments-1",
    "title": "Week 12: Extending beyond the data, what can and cannot be inferred more generally, given the data collection",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 4 Part 2 is due on Monday 28 October."
  },
  {
    "objectID": "week10/index.html",
    "href": "week10/index.html",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "",
    "text": "cubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data"
  },
  {
    "objectID": "week10/index.html#main-reference",
    "href": "week10/index.html#main-reference",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "",
    "text": "cubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data"
  },
  {
    "objectID": "week10/index.html#what-you-will-learn-this-week",
    "href": "week10/index.html#what-you-will-learn-this-week",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week10/index.html#lecture-slides",
    "href": "week10/index.html#lecture-slides",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week10/index.html#tutorial-instructions",
    "href": "week10/index.html#tutorial-instructions",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week10/index.html#assignments",
    "href": "week10/index.html#assignments",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week10/index.html#assignments-1",
    "href": "week10/index.html#assignments-1",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 4 Part 1 is due on Monday 07 October.\nAssignment 4 Part 2 is due on Monday 28 October."
  },
  {
    "objectID": "week1/slides.html#about-this-unit",
    "href": "week1/slides.html#about-this-unit",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "About this unit",
    "text": "About this unit"
  },
  {
    "objectID": "week1/slides.html#teaching-team-12",
    "href": "week1/slides.html#teaching-team-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Teaching team 1/2",
    "text": "Teaching team 1/2\n\n\n\nDi Cook  Distinguished Professor  Monash University \n\nüåê https://dicook.org/\n‚úâÔ∏è ETC5521.Clayton-x@monash.edu\nü¶£ @visnut@aus.social\n\n\nI have a PhD from Rutgers University, NJ, and a Bachelor of Science from University of New England\nI am a Fellow of the American Statistical Association, elected member of the the R Foundation and International Statistical Institute, Past-Editor of the Journal of Computational and Graphical Statistics, and the R Journal.\nMy research is in data visualisation, statistical graphics and computing, with application to sports, ecology and bioinformatics. I likes to develop new methodology and software.\nMy students always work on methods and software that is generally useful for the world. They have been responsible for bringing you the tidyverse suite, knitr, plotly, and many other R packages we regularly use."
  },
  {
    "objectID": "week1/slides.html#teaching-team-22",
    "href": "week1/slides.html#teaching-team-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Teaching team 2/2",
    "text": "Teaching team 2/2\n\n\n\nKrisanat Anukarnsakulchularp  Master of Business Analytics  Monash University \n\nüåê https://github.com/KrisanatA\n‚úâÔ∏è ETC5521.Clayton-x@monash.edu\n\n\nHe has a Bachelor of Actuarial Science, Monash University, 2018 - 2021\nand a Master of Business Analytics, Monash University | 2022 - 2023.\nHe has published the R package animbook\nand hopes to be a PhD student at Monash from 2025.\nThis is his second semester tutoring at Monash, and one of several units working on this semester."
  },
  {
    "objectID": "week1/slides.html#got-a-question-or-a-comment",
    "href": "week1/slides.html#got-a-question-or-a-comment",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Got a question, or a comment?",
    "text": "Got a question, or a comment?\n\n‚úã üî° You can ask directly by unmuting yourself, or typing in the chat, of the live lecture.\n\nüíª If watching the recording, please post in the discussion (ED) forum."
  },
  {
    "objectID": "week1/slides.html#welcome",
    "href": "week1/slides.html#welcome",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Welcome!",
    "text": "Welcome!\n\n\nBeyond modelling and prediction, data might have many more stories to tell. Exploring data to uncover patterns and structures, involves both numerical and visual techniques designed to reveal interesting information that may be unexpected. However, an analyst must be cautious not to over-interpret apparent patterns, and to use randomisation tools to assess whether the patterns are real or spurious.\n\n\nlearn to use modern data exploration tools with many different types of contemporary data to uncover interesting structures, unusual relationships and anomalies.\nunderstand how to map out appropriate analyses for a given data set and description, define what we would expect to see in the data, and whether what we see is contrary to expectations.\nbe able to compute null samples in order to test apparent patterns, and to interpret the results using computational methods for statistical inference.\ncritically assess the strength and adequacy of data analysis."
  },
  {
    "objectID": "week1/slides.html#unit-structure",
    "href": "week1/slides.html#unit-structure",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üìÖ Unit Structure",
    "text": "üìÖ Unit Structure\n\n\n2 hour lecture üë©‚Äçüè´ Tue 10.00am - noon, on zoom (see moodle for the link) Class is more fun if you can attend live!\n2 x 1.5 hour on-campus tutorial üõ†Ô∏è Wed 9:30-11:00 and Wed 7:30-9:00pm CL_Anc-19.LTB_134 Attendance is expected - this is the chance to practice what is explained in lecture under your tutor‚Äôs guidance."
  },
  {
    "objectID": "week1/slides.html#resources",
    "href": "week1/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üìö Resources",
    "text": "üìö Resources\nüè° Course homepage: this is where you find the course materials  (lecture slides, tutorials and tutorial solutions) https://ddde.numbat.space/\n\nüà¥ Moodle: this is where you find discussion forum, zoom links, and marks https://learning.monash.edu/course/view.php?id=18864\n\nüß∞ GitHub classroom: this is where you will find assignments, but links to each will be available in moodle. https://classroom.github.com/classrooms/175896553-etc5521-2024-classroom-29a96a"
  },
  {
    "objectID": "week1/slides.html#assessment-part-12",
    "href": "week1/slides.html#assessment-part-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üíØ Assessment Part 1/2",
    "text": "üíØ Assessment Part 1/2\n\nWeekly quizzes (5%) There will be a weekly quiz starting week 2 provided through Moodle. These are a great chance to check your knowledge, and help you prepare for the tutorial and to keep up to date with the weekly course material. Your best 10 scores will be used for your final quiz total. \nAssignment 1 (15%), through GitHub classroom, Due: Aug 5, 11:55pm. This is an individual assessment. \nAssignment 2 (20%), through GitHub classroom, Due: Aug 26, 11:55pm. This is an individual assessment. \nAssignment 3 (20%): through GitHub classroom, Due: Sep 16, 11:55pm. This is an individual assessment. \nAssignment 4, parts 1 and 2 (20% each), through GitHub classroom, Due: Oct 7, 11:55pm and Oct 28, 11:55pm."
  },
  {
    "objectID": "week1/slides.html#github-classroom",
    "href": "week1/slides.html#github-classroom",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nWe are going to use GitHub Classroom (etc5521 2024: Diving Deeper into Data Exploration) to distribute assignment templates and keep track of your assignment progress.\n\nClone the first assignment by clicking on the link given in Moodle.\nOnce you have accepted it, you will get a cloned copy on your own GitHub account. It is a private repo, which means you and the teaching staff will be the only people with access.\nIf you need some help getting started, check this information.\nThe week 1 tutorial is the best place to get help with GitHub."
  },
  {
    "objectID": "week1/slides.html#why-exploratory-data-analysis",
    "href": "week1/slides.html#why-exploratory-data-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Why exploratory data analysis?",
    "text": "Why exploratory data analysis?\n What‚Äôs special about exploring data, in contrast to confirmatory data analysis?\n\nLet‚Äôs look at some common definitions and quotes of ‚Äúexploratory data analysis‚Äù."
  },
  {
    "objectID": "week1/slides.html#section",
    "href": "week1/slides.html#section",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n\n\n\nhttps://en.wikipedia.org/wiki/Exploratory_data_analysis"
  },
  {
    "objectID": "week1/slides.html#section-1",
    "href": "week1/slides.html#section-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends.\n\n\n\nhttps://r4ds.had.co.nz/exploratory-data-analysis.html"
  },
  {
    "objectID": "week1/slides.html#section-2",
    "href": "week1/slides.html#section-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to (1) maximize insight into a data set; (2) uncover underlying structure; (3) extract important variables; (4) detect outliers and anomalies; (5) test underlying assumptions; (6) develop parsimonious models; and (7) determine optimal factor settings.\n\n\n\nhttps://www.itl.nist.gov/div898/handbook/eda/section1/eda11.htm"
  },
  {
    "objectID": "week1/slides.html#section-3",
    "href": "week1/slides.html#section-3",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "What is Exploratory Data Analysis (EDA)? (1) How to ensure you are ready to use machine learning algorithms in a project? (2) How to choose the most suitable algorithms for your data set? (3) How to define the feature variables that can potentially be used for machine learning?\n\n\n\nhttps://www.kaggle.com/pavansanagapati/a-simple-tutorial-on-exploratory-data-analysis"
  },
  {
    "objectID": "week1/slides.html#section-4",
    "href": "week1/slides.html#section-4",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "EDA is necessary for the next stage of data research. If there was an analogy to exploratory data analysis, it would be that of a painter examining their tools and available time, before deciding on what best to paint.\n\n\n\nhttps://seleritysas.com/blog/2020/05/08/exploratory-data-analysis-and-its-role-in-improving-business-operations/"
  },
  {
    "objectID": "week1/slides.html#section-5",
    "href": "week1/slides.html#section-5",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data.\n\n\n\nhttps://www.coursera.org/learn/exploratory-data-analysis#syllabus"
  },
  {
    "objectID": "week1/slides.html#section-6",
    "href": "week1/slides.html#section-6",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The purpose of doing the Exploratory Data Analysis or EDA is to find new information in data. The understanding of EDA that practitioners may not aware of, is the EDA uses a visually-examined dataset to understand and summarize the main characteristics of the dataset without having a prior hypothesis or relying upon statistical models.\n\n\n\nhttps://towardsdatascience.com/if-you-dont-find-anything-new-you-don-t-do-eda-right-d356f9995098"
  },
  {
    "objectID": "week1/slides.html#none-of-these-capture-what-this-course-is-about",
    "href": "week1/slides.html#none-of-these-capture-what-this-course-is-about",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "None of these capture what this course is about",
    "text": "None of these capture what this course is about\n\n\n\n\nhttps://www.gocomics.com/calvinandhobbes/2015/08/26"
  },
  {
    "objectID": "week1/slides.html#a-simple-example-to-illustrate-exploratory-data-analysis-contrasted-with-a-confirmatory-data-analysis",
    "href": "week1/slides.html#a-simple-example-to-illustrate-exploratory-data-analysis-contrasted-with-a-confirmatory-data-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "A simple example to illustrate ‚Äúexploratory data analysis‚Äù contrasted with a ‚Äúconfirmatory data analysis‚Äù",
    "text": "A simple example to illustrate ‚Äúexploratory data analysis‚Äù contrasted with a ‚Äúconfirmatory data analysis‚Äù"
  },
  {
    "objectID": "week1/slides.html#what-are-the-factors-that-affect-tipping-behaviour",
    "href": "week1/slides.html#what-are-the-factors-that-affect-tipping-behaviour",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What are the factors that affect tipping behaviour?",
    "text": "What are the factors that affect tipping behaviour?\n\n\nIn one restaurant, a food server recorded the following data on all customers they served during an interval of two and a half months in early 1990.\nFood servers‚Äô tips in restaurants may be influenced by many factors, including the nature of the restaurant, size of the party, and table locations in the restaurant. Restaurant managers need to know which factors matter when they assign tables to food servers.\n\n\n\nlibrary(tidyverse)\ntips &lt;- read_csv(\"http://ggobi.org/book/data/tips.csv\")"
  },
  {
    "objectID": "week1/slides.html#what-is-tipping",
    "href": "week1/slides.html#what-is-tipping",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is tipping?",
    "text": "What is tipping?\n\nWhen you‚Äôre dining at a full-service restaurant\n\nTip 20 percent of your full bill.\n\nWhen you grab a cup of coffee\n\nRound up or add a dollar if you‚Äôre a regular or ordered a complicated drink.\n\nWhen you have lunch at a food truck\n\nDrop a few dollars into the tip jar, but a little less than you would at a dine-in spot.\n\nWhen you use a gift card\n\nTip on the total value of the meal, not just what you paid out of pocket.\n\n\n\n\nThe basic rules of tipping that everyone should know about"
  },
  {
    "objectID": "week1/slides.html#recommended-procedure-in-the-book",
    "href": "week1/slides.html#recommended-procedure-in-the-book",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Recommended procedure in the book",
    "text": "Recommended procedure in the book\n\nStep 1: Develop a model\n\nShould the response be tip alone and use the total bill as a predictor?\nShould you create a new variable tip rate and use this as the response?\n\nStep 2: Fit the full model with sex, smoker, day, time and size as predictors\nStep 3: Refine model: Should some variables should be dropped?\nStep 4: Check distribution of residuals\nStep 5: Summarise the model, if X=something, what would be the expected tip"
  },
  {
    "objectID": "week1/slides.html#step-1",
    "href": "week1/slides.html#step-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 1",
    "text": "Step 1\nCalculate tip % as tip/total bill \\(\\times\\) 100\n  \n\ntips &lt;- tips %&gt;%\n  mutate(tip_pct = tip/totbill * 100) \n\n\n\nNote: Creating new variables (sometimes called feature engineering), is a common step in any data analysis."
  },
  {
    "objectID": "week1/slides.html#step-2-fit",
    "href": "week1/slides.html#step-2-fit",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 2 Fit",
    "text": "Step 2 Fit\nFit the full model with all variables\n \n\ntips_lm &lt;- tips %&gt;%\n  select(tip_pct, sex, smoker, day, time, size) %&gt;%\n  lm(tip_pct ~ ., data=.)"
  },
  {
    "objectID": "week1/slides.html#step-2-model-summary",
    "href": "week1/slides.html#step-2-model-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 2 Model summary",
    "text": "Step 2 Model summary\n\n\n\nlibrary(broom)\nlibrary(kableExtra)\ntidy(tips_lm) %&gt;% \n  kable(digits=2) %&gt;% \n  kable_styling() \n\n\nglance(tips_lm) %&gt;% \n  select(r.squared, statistic, \n         p.value) %&gt;% \n  kable(digits=3)\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n20.66\n2.49\n8.29\n0.00\n\n\nsexM\n-0.85\n0.83\n-1.02\n0.31\n\n\nsmokerYes\n0.36\n0.85\n0.43\n0.67\n\n\ndaySat\n-0.18\n1.83\n-0.10\n0.92\n\n\ndaySun\n1.67\n1.90\n0.88\n0.38\n\n\ndayThu\n-1.82\n2.32\n-0.78\n0.43\n\n\ntimeNight\n-2.34\n2.61\n-0.89\n0.37\n\n\nsize\n-0.96\n0.42\n-2.28\n0.02\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nstatistic\np.value\n\n\n\n\n0.042\n1.5\n0.17\n\n\n\n\n\nü§î Which variable(s) would be considered important for predicting tip %?"
  },
  {
    "objectID": "week1/slides.html#step-3-refine-model",
    "href": "week1/slides.html#step-3-refine-model",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 3: Refine model",
    "text": "Step 3: Refine model\n\n\n\ntips_lm &lt;- tips %&gt;%\n  select(tip_pct, size) %&gt;% \n  lm(tip_pct ~ ., data=.) \ntidy(tips_lm) %&gt;% \n  kable(digits=2) %&gt;% \n  kable_styling() \n\n\nglance(tips_lm) %&gt;% \n  select(r.squared, statistic, p.value) %&gt;% \n  kable(digits=3)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n18.44\n1.12\n16.5\n0.00\n\n\nsize\n-0.92\n0.41\n-2.2\n0.03\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nstatistic\np.value\n\n\n\n\n0.02\n5\n0.026"
  },
  {
    "objectID": "week1/slides.html#model-summary",
    "href": "week1/slides.html#model-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model summary",
    "text": "Model summary\n \n\\[\\widehat{tip %} = 18.44 - 0.92 \\times size\\]\n\n \nAs the size of the dining party increases by one person the tip decreases by approximately 1%."
  },
  {
    "objectID": "week1/slides.html#model-assessment",
    "href": "week1/slides.html#model-assessment",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model assessment",
    "text": "Model assessment\n   \\(R^2 = 0.02\\).\n\n  This dropped by half from the full model, even though no other variables contributed significantly to the model. It might be a good step to examine interaction terms.\nWhat does \\(R^2 = 0.02\\) mean?"
  },
  {
    "objectID": "week1/slides.html#model-assessment-1",
    "href": "week1/slides.html#model-assessment-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model assessment",
    "text": "Model assessment\n\\(R^2 = 0.02\\) means that size explains just 2% of the variance in tip %. This is a very weak model.\n\nAnd \\(R^2 = 0.04\\) is also a very weak model.\nWhat do the \\(F\\) statistic and \\(p\\)-value mean?\nWhat do the \\(t\\) statistics and \\(p\\)-value associated with model coefficients mean?"
  },
  {
    "objectID": "week1/slides.html#overall-model-significance",
    "href": "week1/slides.html#overall-model-significance",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Overall model significance",
    "text": "Overall model significance\nAssume that we have a random sample from a population. Assume that the model for the population is\n\\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed\n\\[ \\widehat{tip %} = b_0 + b_1  sexM + ... + b_7 size \\] The \\(F\\) statistic refers to\n\\[ H_o: \\beta_1 = ... = \\beta_7 = 0 ~~ vs ~~ H_a: \\text{at least one is not 0}\\] The \\(p\\)-value is the probability that we observe the given \\(F\\) value or larger, computed assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week1/slides.html#term-significance",
    "href": "week1/slides.html#term-significance",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Term significance",
    "text": "Term significance\nAssume that we have a random sample from a population. Assume that the model for the population is\n\\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed\n\\[ \\widehat{tip %} = b_0 + b_1  sexM + ... + b_7 size \\]\nThe \\(t\\) statistics in the coefficient summary refer to\n\\[ H_o: \\beta_k = 0 ~~ vs ~~ H_a: \\beta_k \\neq 0 \\] The \\(p\\)-value is the probability that we observe the given \\(t\\) value or more extreme, computed assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week1/slides.html#model-diagnostics-md",
    "href": "week1/slides.html#model-diagnostics-md",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model diagnostics (MD)",
    "text": "Model diagnostics (MD)\nNormally, the final model summary would be accompanied diagnostic plots\n\nobserved vs fitted values to check strength and appropriateness of the fit\nunivariate plot, and normal probability plot, of residuals to check for normality\nin the simple final model like this, the observed vs predictor, with model overlaid would be advised to assess the model relative to the variability around the model\nwhen the final model has more terms, using a partial dependence plot to check the relative relationship between the response and predictors would be recommended."
  },
  {
    "objectID": "week1/slides.html#residual-plots",
    "href": "week1/slides.html#residual-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plots",
    "text": "Residual plots\n\n\n\ntips_aug &lt;- augment(tips_lm)\nggplot(tips_aug, \n    aes(x=.resid)) + \n  geom_histogram() +\n  xlab(\"residuals\")"
  },
  {
    "objectID": "week1/slides.html#residual-normal-probability-plots",
    "href": "week1/slides.html#residual-normal-probability-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual normal probability plots",
    "text": "Residual normal probability plots\n\n\n\nggplot(tips_aug, \n    aes(sample=.resid)) + \n  stat_qq() +\n  stat_qq_line() +\n  xlab(\"residuals\") +\n  theme(aspect.ratio=1)"
  },
  {
    "objectID": "week1/slides.html#fitted-vs-observed",
    "href": "week1/slides.html#fitted-vs-observed",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Fitted vs observed",
    "text": "Fitted vs observed\n\n\n\nggplot(tips_aug, \n    aes(x=.fitted, y=tip_pct)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  xlab(\"observed\") +\n  ylab(\"fitted\")"
  },
  {
    "objectID": "week1/slides.html#model-in-the-data-space",
    "href": "week1/slides.html#model-in-the-data-space",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "‚ÄúModel-in-the-data-space‚Äù",
    "text": "‚ÄúModel-in-the-data-space‚Äù\n\n\n\nggplot(tips_aug, \n    aes(x=size, y=tip_pct)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  ylab(\"tip %\")\n\n\nThe fitted model is overlaid on a plot of the data. This is called ‚Äúmodel-in-the-data-space‚Äù (Wickham et al, 2015).\n\nAll the plots on the previous three slides: histogram of residuals, normal probability plot, fitted vs residuals are considered to be ‚Äúdata-in-the-model-space‚Äù. Stay tuned for more discussion on this later."
  },
  {
    "objectID": "week1/slides.html#section-7",
    "href": "week1/slides.html#section-7",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The result of this work might leave us with\n\na model that could be used to impose a dining/tipping policy in restaurants (see here)\n\n\nbut it should also leave us with an unease that this policy is based on weak support."
  },
  {
    "objectID": "week1/slides.html#summary",
    "href": "week1/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\n\n\nPlots as we have just seen, associated with pursuit of an answer to a specific question may be best grouped into the category of ‚Äúmodel diagnostics (MD)‚Äù.\n\nThere are additional categories of plots for data analysis that include initial data analysis (IDA), descriptive statistics. Stay tuned for more on these.\n\n\nA separate and big area for plots of data is for communication, where we already know what is in the data and we want to communicate the information as best possible.\n\nWhen exploring data, we are using data plots to discover things we didn‚Äôt already know."
  },
  {
    "objectID": "week1/slides.html#what-did-this-analysis-miss",
    "href": "week1/slides.html#what-did-this-analysis-miss",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What did this analysis miss?",
    "text": "What did this analysis miss?"
  },
  {
    "objectID": "week1/slides.html#general-strategy-for-exploring-data",
    "href": "week1/slides.html#general-strategy-for-exploring-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "General strategy for EXPLORING DATA",
    "text": "General strategy for EXPLORING DATA\n\n\nIts a good idea to examine the data description, the explanation of the variables, and how the data was collected.\n\nYou need to know what type of variables are in the data in order to decide appropriate choice of plots, and calculations to make.\nData description should have information about data collection methods, so that the extent of what we learn from the data might apply to new data.\n\n\n\nWhat does that look like here?\n\nglimpse(tips)\n\nRows: 244\nColumns: 9\n$ obs     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1‚Ä¶\n$ totbill &lt;dbl&gt; 17.0, 10.3, 21.0, 23.7, 24.6, 25‚Ä¶\n$ tip     &lt;dbl&gt; 1.0, 1.7, 3.5, 3.3, 3.6, 4.7, 2.‚Ä¶\n$ sex     &lt;chr&gt; \"F\", \"M\", \"M\", \"M\", \"F\", \"M\", \"M‚Ä¶\n$ smoker  &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"N‚Ä¶\n$ day     &lt;chr&gt; \"Sun\", \"Sun\", \"Sun\", \"Sun\", \"Sun‚Ä¶\n$ time    &lt;chr&gt; \"Night\", \"Night\", \"Night\", \"Nigh‚Ä¶\n$ size    &lt;dbl&gt; 2, 3, 3, 2, 4, 4, 2, 4, 2, 2, 2,‚Ä¶\n$ tip_pct &lt;dbl&gt; 5.9, 16.1, 16.7, 14.0, 14.7, 18.‚Ä¶\n\n\nLook at the distribution of quantitative variables tips, total bill.\n\n\nExamine the distributions across categorical variables.\nExamine quantitative variables relative to categorical variables"
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips",
    "href": "week1/slides.html#distributions-of-tips",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) + \n  geom_histogram(\n    colour=\"white\")"
  },
  {
    "objectID": "week1/slides.html#because-one-binwidth-is-never-enough",
    "href": "week1/slides.html#because-one-binwidth-is-never-enough",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Because, one binwidth is never enough ‚Ä¶",
    "text": "Because, one binwidth is never enough ‚Ä¶"
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips-1",
    "href": "week1/slides.html#distributions-of-tips-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) +\n  geom_histogram(\n    breaks=seq(0.5,10.5,1),  \n    colour=\"white\") + \n  scale_x_continuous(\n    breaks=seq(0,11,1))\n\nBig fat bins. Tips are skewed, which means most tips are relatively small."
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips-2",
    "href": "week1/slides.html#distributions-of-tips-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) + \n  geom_histogram(\n    breaks=seq(0.5,10.5,0.1), \n    colour=\"white\") +\n  scale_x_continuous(\n    breaks=seq(0,11,1))\n\nSkinny bins. Tips are multimodal, and occurring at the full dollar and 50c amounts."
  },
  {
    "objectID": "week1/slides.html#we-could-also-look-at-total-bill-this-way",
    "href": "week1/slides.html#we-could-also-look-at-total-bill-this-way",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "We could also look at total bill this way",
    "text": "We could also look at total bill this way\nbut I‚Äôve already done this, and we don‚Äôt learn anything more about the multiple peaks than waht is learned by plotting tips."
  },
  {
    "objectID": "week1/slides.html#relationship-between-tip-and-total",
    "href": "week1/slides.html#relationship-between-tip-and-total",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Relationship between tip and total",
    "text": "Relationship between tip and total\n\n\n\np &lt;- ggplot(tips, \n    aes(x= totbill, y=tip)) + \n  geom_point() + \n  scale_y_continuous(\n    breaks=seq(0,11,1))\np\n\nWhy is total on the x axis? \nShould we add a guideline?"
  },
  {
    "objectID": "week1/slides.html#add-a-guideline-indicating-common-practice",
    "href": "week1/slides.html#add-a-guideline-indicating-common-practice",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Add a guideline indicating common practice",
    "text": "Add a guideline indicating common practice\n\n\n\np &lt;- p + geom_abline(intercept=0, \n              slope=0.2) + \n  annotate(\"text\", x=45, y=10, \n           label=\"20% tip\") \np\n\n\n\n\nMost tips less than 20%: Skin flints vs generous diners\nA couple of big tips\nBanding horizontally is the rounding seen previously"
  },
  {
    "objectID": "week1/slides.html#we-should-examine-bar-charts-and-mosaic-plots-of-the-categorical-variables-next",
    "href": "week1/slides.html#we-should-examine-bar-charts-and-mosaic-plots-of-the-categorical-variables-next",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "We should examine bar charts and mosaic plots of the categorical variables next",
    "text": "We should examine bar charts and mosaic plots of the categorical variables next\nbut I‚Äôve already done that, and there‚Äôs not too much of interest there."
  },
  {
    "objectID": "week1/slides.html#relative-to-categorical-variables",
    "href": "week1/slides.html#relative-to-categorical-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Relative to categorical variables",
    "text": "Relative to categorical variables\n\n\n\np + facet_grid(smoker~sex) \n\n\n\n\n\n\n\n\n\n\n\n\nThe bigger bills tend to be paid by men (and females that smoke).\nExcept for three diners, female non-smokers are very consistent tippers, probably around 15-18% though.\nThe variability in the smokers is much higher than for the non-smokers."
  },
  {
    "objectID": "week1/slides.html#isnt-this-interesting",
    "href": "week1/slides.html#isnt-this-interesting",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Isn‚Äôt this interesting?",
    "text": "Isn‚Äôt this interesting?"
  },
  {
    "objectID": "week1/slides.html#procedure-of-eda",
    "href": "week1/slides.html#procedure-of-eda",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Procedure of EDA",
    "text": "Procedure of EDA\n\nWe gained a wealth of insight in a short time.\nUsing nothing but graphical methods we investigated univariate, bivariate, and multivariate relationships.\nWe found both global features and local detail. We saw that\n\ntips were rounded; then we saw the obvious\n\ncorrelation between the tip and the size of the bill, noting the scarcity of generous tippers; finally we\ndiscovered differences in the tipping behavior of male and female smokers and non-smokers.\n\n\nThese are unexpected insights were missed from the analysis that focused solely on the primary question."
  },
  {
    "objectID": "week1/slides.html#what-can-go-wrong",
    "href": "week1/slides.html#what-can-go-wrong",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What can go wrong?",
    "text": "What can go wrong?"
  },
  {
    "objectID": "week1/slides.html#how-was-data-collected",
    "href": "week1/slides.html#how-was-data-collected",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How was data collected?",
    "text": "How was data collected?\n\n\nIn one restaurant, a food server recorded the following data on all customers they served during an interval of two and a half months in early 1990.\n\nHow much can you infer about tipping more broadly?\n\n\nTip has a weak but significant relationship with total bill?\nTips have a skewed distribution? (More small tips and fewer large tips?)\nTips tend to be made in nice round numbers.\nPeople generally under-tip?\nSmokers are less reliable tippers."
  },
  {
    "objectID": "week1/slides.html#ways-to-verify-support-or-refute-generalisations",
    "href": "week1/slides.html#ways-to-verify-support-or-refute-generalisations",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Ways to verify, support or refute generalisations",
    "text": "Ways to verify, support or refute generalisations\n\n\n\nexternal information\nother studies/samples\ngood choice of calculations and plots\nall the permutations and subsets of measured variables\ncomputational re-sampling methods (we‚Äôll see these soon)\n\n\n\nPoor data collection methods affects every analysis, including statistical or computational modeling.\n\n\nFor this waiter and the restaurant manager, there is some useful information. Like what?\n\n\nService fee for smokers to ensure consistency?\nAssign waiter to variety of party sizes and composition.\nShifts on different days or time of day (not shown)."
  },
  {
    "objectID": "week1/slides.html#words-of-wisdom",
    "href": "week1/slides.html#words-of-wisdom",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Words of wisdom",
    "text": "Words of wisdom\nFalse discovery is the lesser danger when compared to non-discovery. Non-discovery is the failure to identify meaningful structure, and it may result in false or incomplete modeling. In a healthy scientific enterprise, the fear of non-discovery should be at least as great as the fear of false discovery."
  },
  {
    "objectID": "week1/slides.html#where-do-we-go-from-here",
    "href": "week1/slides.html#where-do-we-go-from-here",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Where do we go from here?",
    "text": "Where do we go from here?\n\nMethods for single, bivariate, multivariate\n\nnumerical variables\ncategorical variables\n\nMethods to accommodate temporal and spatial context\nHow to make effective comparisons\nUtilising computational methods to assess what you see is ‚Äúreal‚Äù"
  },
  {
    "objectID": "week1/slides.html#resources-1",
    "href": "week1/slides.html#resources-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nCook and Swayne (2007) Interactive and Dynamic Graphics for Data Analysis, Introduction\nDonoho (2017) 50 Years of Data Science\nStaniak and Biecek (2019) The Landscape of R Packages for Automated Exploratory Data Analysis\n\n\n\n\nETC5521 Lecture 1 | ddde.numbat.space"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "ETC5521 Resources",
    "section": "",
    "text": "Books and articles\n\nCook and Swayne (2007) Interactive and Dynamic Graphics for Data Analysis, Introduction\nDonoho (2017) 50 Years of Data Science\nStaniak and Biecek (2019) The Landscape of R Packages for Automated Exploratory Data Analysis\nHuebner et al (2018) A Contemporary Conceptual Framework for Initial Data Analysis\nHuebner et al (2020) Hidden analyses\nChatfield (1985) The Initial Examination of Data. Journal of the Royal Statistical Society. Series A (General) 148 \nCox & Snell (1981) Applied Statistics. London: Chapman and Hall.\nvan der Loo and de Jonge (2018). Statistical Data Cleaning with Applications in R. John Wiley and Sons Ltd.\nHyndman (2014) Explaining the ABS unemployment fluctuations\nBuja et al.¬†(2009). Statistical Inference for Exploratory Data Analysis and Model Diagnostics. Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences 367 (1906): 4361‚Äì83.\nWickham et al (2010) Graphical Inference for Infovis. IEEE Transactions on Visualization and Computer Graphics 16 (6): 973‚Äì79.\nHofmann et al (2012) Graphical Tests for Power Comparison of Competing Designs. IEEE Transactions on Visualization and Computer Graphics 18 (12): 2441‚Äì48.\nMajumder et al (2013) Validation of Visual Statistical Inference, Applied to Linear Models. Journal of the American Statistical Association 108 (503): 942‚Äì56.\nJosse et al (2022) R-miss-tastic\nFriendly and Denis ‚ÄúMilestones in History of Thematic Cartography, Statistical Graphics and Data Visualisation‚Äù available at http://www.datavis.ca/milestones/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Professor Di Cook\n\nEmail: etc5521.clayton-x@monash.edu\nConsultation: Thu 11:00-12:00, 352 Education Blg, and zoom"
  },
  {
    "objectID": "index.html#lecturerchief-examiner",
    "href": "index.html#lecturerchief-examiner",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Professor Di Cook\n\nEmail: etc5521.clayton-x@monash.edu\nConsultation: Thu 11:00-12:00, 352 Education Blg, and zoom"
  },
  {
    "objectID": "index.html#tutors",
    "href": "index.html#tutors",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Tutors",
    "text": "Tutors\n\nKrisanat Anukarnsakulchularp\n\nTutorials: Wed 9:30-11:00am and 7:30-9:00pm CL_Anc-19.LTB_134\nConsultation: Mon 9.30-11:00 am, Menzies W9.20"
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Weekly schedule",
    "text": "Weekly schedule\n\nLecture: Tues 10-12 on zoom (link in Moodle)\nTutorial: 1.5 hours\nWeekly learning quizzes due each Wednesday 9am, from week 2\n\n\n\n\nWeek\nTopic\nReference\nAssessments\n\n\n\n\n22 Jul\nOverview. Why this course? What is EDA?\nThe Landscape of R Packages for Automated Exploratory Data Analysis\n\n\n\n29 Jul\nLearning from history\nEDA Case Study: Bay area blues\n\n\n\n05 Aug\nInitial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA\nThe initial examination of data\nAssignment 1\n\n\n12 Aug\nUsing computational tools to determine whether what is seen in the data can be assumed to apply more broadly\nWickham et al.¬†(2010) Graphical inference for Infovis\n\n\n\n19 Aug\nWorking with a single variable, making transformations, detecting outliers, using robust statistics\nWilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions\n\n\n\n26 Aug\nBivariate dependencies and relationships, transformations to linearise\nWilke (2019) Ch 12 Visualising associations\nAssignment 2\n\n\n02 Sep\nMaking comparisons between groups and strata\nWilke (2019) Ch 9, 10.2-4, 11.2\n\n\n\n09 Sep\nGoing beyond two variables, exploring high dimensions\nCook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1\n\n\n\n16 Sep\nExploring data having a space and time context Part I\nbrolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R\nAssignment 3\n\n\n23 Sep\nMid-semester break\n\n\n\n\n30 Sep\nExploring data having a space and time context Part II\ncubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data\n\n\n\n07 Oct\nSculpting data using models, checking assumptions, co-dependency and performing diagnostics\nHow to use a tour to check if your model suffers from multicollinearity\nAssignment 4 Part 1\n\n\n14 Oct\nExtending beyond the data, what can and cannot be inferred more generally, given the data collection\n\n\n\n\n28 Oct\n\n\nAssignment 4 Part 2"
  },
  {
    "objectID": "index.html#assessments",
    "href": "index.html#assessments",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Assessments",
    "text": "Assessments\n\nWeekly learning quizzes: 5%\nAssignment 1: Instructions (15%)\nAssignment 2: Instructions (20%)\nAssignment 3: Instructions (20%)\nAssignment 4 part 1: Instructions (20%)\nAssignment 4 part 2: Instructions (20%)"
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Software",
    "text": "Software\nWe will be using the latest versions of R and RStudio.\nHere is the code to install (most of) the R packages we will be using in this unit.\ninstall.packages(c(\"tidyr\", \"dplyr\", \"readr\", \"readxl\", \"readabs\", \"forcats\", \"tsibble\", \"cubble\", \"lubridate\", \"ggplot2\", \"GGally\", \"ggthemes\", \"sugrrants\", \"ggbeeswarm\", \"plotly\", \"gganimate\", \"tourr\", \"sugarbag\", \"tsibbletalk\", \"visdat\", \"inspectdf\", \"naniar\", \"validate\", \"vcd\", \"mvtnorm\", \"nullabor\", \"visage\", \"forecast\", \"cassowaryr\", \"brolgar\", \"palmerpenguins\", \"housingData\",  \"broom\", \"kableExtra\", \"lvplot\", \"colorspace\", \"patchwork\"), dependencies=TRUE)\nFrom GitHub, install\nremotes::install_github(\"casperhart/detourr\")\nIf you are relatively new to R, working through the materials at https://learnr.numbat.space is an excellent way to up-skill. You are epsecially encouraged to work through Chapter 3, on Troubleshooting and asking for help, because at some point you will need help with your coding, and how you go about this matters and impacts the ability of others to help you.\nThese materials are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "week1/index.html",
    "href": "week1/index.html",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "",
    "text": "The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week1/index.html#reading",
    "href": "week1/index.html#reading",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "",
    "text": "The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week1/index.html#what-you-will-learn-this-week",
    "href": "week1/index.html#what-you-will-learn-this-week",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nHow exploring data is different from a confirmatory analysis\nGet up and running with GitHub Classroom"
  },
  {
    "objectID": "week1/index.html#lecture-slides",
    "href": "week1/index.html#lecture-slides",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week1/index.html#tutorial-instructions",
    "href": "week1/index.html#tutorial-instructions",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week1/index.html#assignments",
    "href": "week1/index.html#assignments",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Monday 05 August."
  },
  {
    "objectID": "week1/tutorialsol.html",
    "href": "week1/tutorialsol.html",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you‚Äôve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorialsol.html#objectives",
    "href": "week1/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you‚Äôve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorialsol.html#preparation",
    "href": "week1/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 1",
    "section": "üîß Preparation",
    "text": "üîß Preparation\n\nComplete the weekly quiz, before the deadline!\nHave git installed on your laptop so that you can access the test classroom.\nHave the latest versions of RStudio and R installed on your laptop.\nInstall this list of R packages:\n\n\nCreate an RStudio Project for this unit, called ETC5521. All your work in the tutorials should be conducted in this project. Ideally, your project is organised into folders, one for data, one for tutorial_XX, ‚Ä¶ Each week when you begin your tutorial, open the project."
  },
  {
    "objectID": "week1/tutorialsol.html#exercises",
    "href": "week1/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 1",
    "section": "üì• Exercises",
    "text": "üì• Exercises"
  },
  {
    "objectID": "week1/tutorialsol.html#ice-breaker",
    "href": "week1/tutorialsol.html#ice-breaker",
    "title": "ETC5521 Tutorial 1",
    "section": "1. Ice breaker",
    "text": "1. Ice breaker\n\nGrab your name tag\n\nFollow the instructions of your tutor to get to know your classmates.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nWhich R package is your favourite? ggplot2 or plotly\nDo you prefer to use geom_boxplot or geom_quasirandom to compare subsets of nuemrical variables?\nHave you used the R package purrr before? Y or N\nWhat does the package profvis do? Visualisation data or Profiling code?"
  },
  {
    "objectID": "week1/tutorialsol.html#how-good-are-your-detective-skills",
    "href": "week1/tutorialsol.html#how-good-are-your-detective-skills",
    "title": "ETC5521 Tutorial 1",
    "section": "2. How good are your detective skills?",
    "text": "2. How good are your detective skills?\nBeing good at noticing something unexpected or unusual is an important skills for exploratory data analysis. This exercise is designed to practice your detective skills.\nPlay the game alzheimer_test from the fun package by running this code:\nYou will be given 6 tasks to complete. Each one is to find a specific letter hidden among a \\(10\\times 30\\) grid of letters. When you are finished, answer these questions:\n\nWhich task did you THINK was the most difficult?\nWhich task does the DATA say was most difficult based, based on the time taken to answer, tm1.1.j. in your results data?\nSave the dataset to an .rda file.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n           char1.1.j. char2.1.j.  tm1.1.j.\nans.user.2          M          N 30.839718\nans.user.3          I          T 19.695932\nans.user.5          D          O 17.189302\nans.user.1          O          C 16.534676\nans.user.4          F          E  4.424869\nans.user            9          6  3.812386"
  },
  {
    "objectID": "week1/tutorialsol.html#get-started-using-github-classroom",
    "href": "week1/tutorialsol.html#get-started-using-github-classroom",
    "title": "ETC5521 Tutorial 1",
    "section": "3. Get started using GitHub Classroom",
    "text": "3. Get started using GitHub Classroom\n\nIn Moodle go to the Assignment 1 instructions to find the invitation to a GitHub Classroom. Accept this invitation.\nClone the assignment repo to your computer.\nOpen the assign01.html instructions.\nMake a start on loading the data into R."
  },
  {
    "objectID": "week1/tutorialsol.html#finishing-up",
    "href": "week1/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 1",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week1/tutorial.html",
    "href": "week1/tutorial.html",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you‚Äôve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorial.html#objectives",
    "href": "week1/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you‚Äôve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorial.html#preparation",
    "href": "week1/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 1",
    "section": "üîß Preparation",
    "text": "üîß Preparation\n\nComplete the weekly quiz, before the deadline!\nHave git installed on your laptop so that you can access the test classroom.\nHave the latest versions of RStudio and R installed on your laptop.\nInstall this list of R packages:\n\n\nCreate an RStudio Project for this unit, called ETC5521. All your work in the tutorials should be conducted in this project. Ideally, your project is organised into folders, one for data, one for tutorial_XX, ‚Ä¶ Each week when you begin your tutorial, open the project."
  },
  {
    "objectID": "week1/tutorial.html#exercises",
    "href": "week1/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 1",
    "section": "üì• Exercises",
    "text": "üì• Exercises"
  },
  {
    "objectID": "week1/tutorial.html#ice-breaker",
    "href": "week1/tutorial.html#ice-breaker",
    "title": "ETC5521 Tutorial 1",
    "section": "1. Ice breaker",
    "text": "1. Ice breaker\n\nGrab your name tag\n\nFollow the instructions of your tutor to get to know your classmates.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nWhich R package is your favourite? ggplot2 or plotly\nDo you prefer to use geom_boxplot or geom_quasirandom to compare subsets of nuemrical variables?\nHave you used the R package purrr before? Y or N\nWhat does the package profvis do? Visualisation data or Profiling code?"
  },
  {
    "objectID": "week1/tutorial.html#how-good-are-your-detective-skills",
    "href": "week1/tutorial.html#how-good-are-your-detective-skills",
    "title": "ETC5521 Tutorial 1",
    "section": "2. How good are your detective skills?",
    "text": "2. How good are your detective skills?\nBeing good at noticing something unexpected or unusual is an important skills for exploratory data analysis. This exercise is designed to practice your detective skills.\nPlay the game alzheimer_test from the fun package by running this code:\nYou will be given 6 tasks to complete. Each one is to find a specific letter hidden among a \\(10\\times 30\\) grid of letters. When you are finished, answer these questions:\n\nWhich task did you THINK was the most difficult?\nWhich task does the DATA say was most difficult based, based on the time taken to answer, tm1.1.j. in your results data?\nSave the dataset to an .rda file.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n           char1.1.j. char2.1.j.  tm1.1.j.\nans.user.2          M          N 30.839718\nans.user.3          I          T 19.695932\nans.user.5          D          O 17.189302\nans.user.1          O          C 16.534676\nans.user.4          F          E  4.424869\nans.user            9          6  3.812386"
  },
  {
    "objectID": "week1/tutorial.html#get-started-using-github-classroom",
    "href": "week1/tutorial.html#get-started-using-github-classroom",
    "title": "ETC5521 Tutorial 1",
    "section": "3. Get started using GitHub Classroom",
    "text": "3. Get started using GitHub Classroom\n\nIn Moodle go to the Assignment 1 instructions to find the invitation to a GitHub Classroom. Accept this invitation.\nClone the assignment repo to your computer.\nOpen the assign01.html instructions.\nMake a start on loading the data into R."
  },
  {
    "objectID": "week1/tutorial.html#finishing-up",
    "href": "week1/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 1",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week11/index.html",
    "href": "week11/index.html",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "",
    "text": "How to use a tour to check if your model suffers from multicollinearity"
  },
  {
    "objectID": "week11/index.html#main-reference",
    "href": "week11/index.html#main-reference",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "",
    "text": "How to use a tour to check if your model suffers from multicollinearity"
  },
  {
    "objectID": "week11/index.html#what-you-will-learn-this-week",
    "href": "week11/index.html#what-you-will-learn-this-week",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week11/index.html#lecture-slides",
    "href": "week11/index.html#lecture-slides",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week11/index.html#tutorial-instructions",
    "href": "week11/index.html#tutorial-instructions",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week11/index.html#assignments",
    "href": "week11/index.html#assignments",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week11/index.html#assignments-1",
    "href": "week11/index.html#assignments-1",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 4 Part 1 is due on Monday 07 October.\nAssignment 4 Part 2 is due on Monday 28 October."
  },
  {
    "objectID": "week2/index.html",
    "href": "week2/index.html",
    "title": "Week 2: Learning from history",
    "section": "",
    "text": "EDA Case Study: Bay area blues"
  },
  {
    "objectID": "week2/index.html#main-reference",
    "href": "week2/index.html#main-reference",
    "title": "Week 2: Learning from history",
    "section": "",
    "text": "EDA Case Study: Bay area blues"
  },
  {
    "objectID": "week2/index.html#what-you-will-learn-this-week",
    "href": "week2/index.html#what-you-will-learn-this-week",
    "title": "Week 2: Learning from history",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nThe historical origins of EDA\nPencil and paper methods like stem-and-leaf plots\nHow to symmetrise and linearise your data\nWhere EDA is relevant today"
  },
  {
    "objectID": "week2/index.html#lecture-slides",
    "href": "week2/index.html#lecture-slides",
    "title": "Week 2: Learning from history",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week2/index.html#tutorial-instructions",
    "href": "week2/index.html#tutorial-instructions",
    "title": "Week 2: Learning from history",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week2/index.html#assignments",
    "href": "week2/index.html#assignments",
    "title": "Week 2: Learning from history",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Monday 05 August."
  },
  {
    "objectID": "week2/tutorialsol.html",
    "href": "week2/tutorialsol.html",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "The purpose of this tutorial is to scope out the software reporting to do EDA in R. We want to understand the capabilities and the limitations."
  },
  {
    "objectID": "week2/tutorialsol.html#objectives",
    "href": "week2/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "The purpose of this tutorial is to scope out the software reporting to do EDA in R. We want to understand the capabilities and the limitations."
  },
  {
    "objectID": "week2/tutorialsol.html#preparation",
    "href": "week2/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 2",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is The Landscape of R Packages for Automated Exploratory Data Analysis. This is a lovely summary of software available that is considered to do exploratory data analysis (EDA). (Note: Dr Cook considers these to be mostly descriptive statistics packages, not exploratory data analysis in the true spirit of the term.) This reading will be the basis of the tutorial exercises today.\n\nComplete the weekly quiz, before the deadline!\nInstall this list of R packages, in addition to what you installed in the previous weeks:\n\n\ninstall.packages(c(\"arsenal\", \"autoEDA\", \"DataExplorer\", \"dataMaid\", \"dlookr\", \"ExPanDaR\", \"explore\", \"exploreR\", \"funModeling\", \"inspectdf\", \"RtutoR\", \"SmartEDA\", \"summarytools\", \"visdat\", \"xray\", \"cranlogs\", \"tidyverse\", \"nycflights13\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week2/tutorialsol.html#exercises",
    "href": "week2/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 2",
    "section": "üì• Exercises",
    "text": "üì• Exercises\nThe article lists a number of R packages that might be used for EDA: arsenal, autoEDA, DataExplorer, dataMaid, dlookr, ExPanDaR, explore, exploreR, funModeling, inspectdf, RtutoR, SmartEDA, summarytools, visdat, xray.\n\n1.\nWhat package had the highest number of CRAN downloads as of 12.07.2019? (Based on the paper.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummarytools with 84737\n\n\n\n\n\n\n2.\nOpen up the shiny server for checking download rates at https://hadley.shinyapps.io/cran-downloads/. Which of these packages has the highest download rate over the period Jan 1, 2024-today?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nlibrary(cranlogs)\neda_pkgs &lt;- cran_downloads(packages=c(\"arsenal\", \"autoEDA\", \"DataExplorer\", \"dataMaid\", \"dlookr\", \"ExPanDaR\", \"explore\", \"exploreR\", \"funModeling\", \"inspectdf\", \"RtutoR\", \"SmartEDA\", \"summarytools\", \"visdat\", \"xray\"), from=\"2024-01-01\", to=lubridate::today())\neda_pkgs |&gt; \n  group_by(package) |&gt;\n  summarise(m=mean(count)) |&gt;\n  arrange(desc(m))\n\n# A tibble: 15 √ó 2\n   package           m\n   &lt;chr&gt;         &lt;dbl&gt;\n 1 summarytools 595.  \n 2 SmartEDA     547.  \n 3 visdat       488.  \n 4 DataExplorer 363.  \n 5 arsenal      221.  \n 6 dlookr        96.9 \n 7 explore       56.2 \n 8 funModeling   46.8 \n 9 inspectdf     43.9 \n10 dataMaid      31.9 \n11 ExPanDaR      16.8 \n12 xray           9.71\n13 exploreR       7.29\n14 RtutoR         2.13\n15 autoEDA        0   \n\n\nsmartEDA was on a roll early in the year, but has virtually disappeared. summarytools is going strong. Interestingly, visdat, high on the list, was developed by Nick Tierney in the years he was at Monash.\n\n\n\n\n\n\n3.\nWhat is an interesting pattern to observe from the time series plot of all the downloads?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe weekly seasonality! There is a regular up/down pattern, that if you zoom in closely - try plotting just a couple of weeks of data - you can see corresponds to week day vs weekend.\n\n\n\n\n\n\n4.\nHow many functions does Staniak and Biecek (2019) say visdat has for doing EDA? Explore what each of them does, by running the example code for each function. What do you think are the features that make visdat a really popular package?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n6; Simple focus, useful functions that apply to a lot of problems.\nIt gives an overview of the variable types, and missing values. This is useful to start a data analysis so that you can start working out what methods might be applied to each, and also whether some variables may have too many missing values to be analysable.\n\nlibrary(visdat)\n# function 1\nvis_dat(airquality)\n\n\n\n\n\n\n\n# function 2\nmessy_vector &lt;- c(TRUE,\n                 \"TRUE\",\n                 \"T\",\n                 \"01/01/01\",\n                 \"01/01/2001\",\n                 NA,\n                 NaN,\n                 \"NA\",\n                 \"Na\",\n                 \"na\",\n                 \"10\",\n                 10,\n                 \"10.1\",\n                 10.1,\n                 \"abc\",\n                 \"$%TG\")\nset.seed(1114)\nmessy_df &lt;- data.frame(var1 = messy_vector,\n                       var2 = sample(messy_vector),\n                       var3 = sample(messy_vector))\nvis_guess(messy_df)\n\n\n\n\n\n\n\n# function 3\nvis_miss(airquality)\n\n\n\n\n\n\n\n# function 4\naq_diff &lt;- airquality\naq_diff[1:10, 1:2] &lt;- NA\nvis_compare(airquality, aq_diff)\n\n\n\n\n\n\n\n# function 5\ndat_test &lt;- tibble::tribble(\n            ~x, ~y,\n            -1,  \"A\",\n            0,  \"B\",\n            1,  \"C\",\n            NA, NA\n            )\n\nvis_expect(dat_test, ~.x == -1)\n\n\n\n\n\n\n\n# function 6\nvis_cor(airquality)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.\nThe package summarytools appears to becoming more favourable relative to visdat. Take a look at this package and explain what tools it has that are not available in visdat.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nlibrary(summarytools)\ndfSummary(tobacco)\n\nData Frame Summary  \ntobacco  \nDimensions: 1000 x 9  \nDuplicates: 2  \n\n-----------------------------------------------------------------------------------------------------------\nNo   Variable       Stats / Values         Freqs (% of Valid)    Graph                 Valid      Missing  \n---- -------------- ---------------------- --------------------- --------------------- ---------- ---------\n1    gender         1. F                   489 (50.0%)           IIIIIIIIII            978        22       \n     [factor]       2. M                   489 (50.0%)           IIIIIIIIII            (97.8%)    (2.2%)   \n\n2    age            Mean (sd) : 50 (18)    63 distinct values    .     .     . . . :   975        25       \n     [numeric]      min &lt; med &lt; max:                             : : : : : . : : : :   (97.5%)    (2.5%)   \n                    18 &lt; 50 &lt; 80                                 : : : : : : : : : :                       \n                    IQR (CV) : 32 (0.4)                          : : : : : : : : : :                       \n                                                                 : : : : : : : : : :                       \n\n3    age.gr         1. 18-34               258 (26.5%)           IIIII                 975        25       \n     [factor]       2. 35-50               241 (24.7%)           IIII                  (97.5%)    (2.5%)   \n                    3. 51-70               317 (32.5%)           IIIIII                                    \n                    4. 71 +                159 (16.3%)           III                                       \n\n4    BMI            Mean (sd) : 26 (4.5)   974 distinct values             :           974        26       \n     [numeric]      min &lt; med &lt; max:                                     : : :         (97.4%)    (2.6%)   \n                    8.8 &lt; 26 &lt; 39                                        : : :                             \n                    IQR (CV) : 5.7 (0.2)                               : : : : :                           \n                                                                     . : : : : : .                         \n\n5    smoker         1. Yes                 298 (29.8%)           IIIII                 1000       0        \n     [factor]       2. No                  702 (70.2%)           IIIIIIIIIIIIII        (100.0%)   (0.0%)   \n\n6    cigs.per.day   Mean (sd) : 6.8 (12)   37 distinct values    :                     965        35       \n     [numeric]      min &lt; med &lt; max:                             :                     (96.5%)    (3.5%)   \n                    0 &lt; 0 &lt; 40                                   :                                         \n                    IQR (CV) : 11 (1.8)                          :                                         \n                                                                 :   . . . . . .                           \n\n7    diseased       1. Yes                 224 (22.4%)           IIII                  1000       0        \n     [factor]       2. No                  776 (77.6%)           IIIIIIIIIIIIIII       (100.0%)   (0.0%)   \n\n8    disease        1. Hypertension        36 (16.2%)            III                   222        778      \n     [character]    2. Cancer              34 (15.3%)            III                   (22.2%)    (77.8%)  \n                    3. Cholesterol         21 ( 9.5%)            I                                         \n                    4. Heart               20 ( 9.0%)            I                                         \n                    5. Pulmonary           20 ( 9.0%)            I                                         \n                    6. Musculoskeletal     19 ( 8.6%)            I                                         \n                    7. Diabetes            14 ( 6.3%)            I                                         \n                    8. Hearing             14 ( 6.3%)            I                                         \n                    9. Digestive           12 ( 5.4%)            I                                         \n                    10. Hypotension        11 ( 5.0%)                                                      \n                    [ 3 others ]           21 ( 9.5%)            I                                         \n\n9    samp.wgts      Mean (sd) : 1 (0.1)    0.86!: 267 (26.7%)    IIIII                 1000       0        \n     [numeric]      min &lt; med &lt; max:       1.04!: 249 (24.9%)    IIII                  (100.0%)   (0.0%)   \n                    0.9 &lt; 1 &lt; 1.1          1.05!: 324 (32.4%)    IIIIII                                    \n                    IQR (CV) : 0.2 (0.1)   1.06!: 160 (16.0%)    III                                       \n                                           ! rounded                                                       \n-----------------------------------------------------------------------------------------------------------\n\n\nIt has a lot more standard statistical summaries, and outputs the summary in a form that can be embedded nicely in a quarto report.\nIf you go to the package website https://github.com/dcomtois/summarytools it‚Äôs not very promising. There is no package website only the GitHub. There have been no updates for 8 months.\n\n\n\n\n\n\n6.\nWhy do you think the package SmartEDA has gone out of favour?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe web site looks a little uncared for, missing images on the main GitHub page. It has a nice package pacge, but again pieces are missing.\n\nlibrary(SmartEDA)\nExpData(data=mtcars,type=1)\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)        32\n2                              No. of variables (ncol)        11\n3                    No. of numeric/interger variables        11\n4                              No. of factor variables         0\n5                                No. of text variables         0\n6                             No. of logical variables         0\n7                          No. of identifier variables         0\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         0\n10               %. of variables having complete cases 100% (11)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\nExpData(data=mtcars,type=2)\n\n   Index Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1           mpg       numeric       32             0              0\n2      2           cyl       numeric       32             0              0\n3      3          disp       numeric       32             0              0\n4      4            hp       numeric       32             0              0\n5      5          drat       numeric       32             0              0\n6      6            wt       numeric       32             0              0\n7      7          qsec       numeric       32             0              0\n8      8            vs       numeric       32             0              0\n9      9            am       numeric       32             0              0\n10    10          gear       numeric       32             0              0\n11    11          carb       numeric       32             0              0\n   No_of_distinct_values\n1                     25\n2                      3\n3                     27\n4                     22\n5                     22\n6                     29\n7                     30\n8                      2\n9                      2\n10                     3\n11                     6\n\n\nIt also has some standard summaries like summarytools which is possibly why it became attractive. And can be used to produce a detailed report on the data, although a report like the one in the next question took ages to complete.\nI‚Äôm not sure why it‚Äôs popularity has dropped. The documentation is not great, there‚Äôs quite a few spelling errors. The functions are (sort of) in CamelCase which is not as attractive as snake_case.\n\n\n\n\n\n\n7.\nThe SmartEDA::ExpReport() and DataExplorer::create_report() are functions that generates a long report when provided with a data set. Try this using the latter function, on the nycflights13 data, using this code:\n\n# DataExplorer\nlibrary(DataExplorer)\nlibrary(nycflights13)\nlibrary(tidyverse)\n\n# Create a big data set\nairlines_all &lt;- flights |&gt; \n  full_join(airlines, by = \"carrier\") |&gt;\n  full_join(planes, by = \"tailnum\", \n            suffix = c(\"_flights\", \"_planes\")) |&gt;\n  full_join(airports, by = c(\"origin\"=\"faa\"), \n            suffix = c(\"_carrier\", \"_origin\")) |&gt;\n  full_join(airports, by = c(\"dest\"=\"faa\"), \n            suffix = c(\"_origin\", \"_dest\"))\n\n\ncreate_report(airlines_all, y = \"arr_delay\")\n\nThe code instructs to use arr_delay as a ‚Äúresponse variable‚Äù. Give some reasons why this report is not very useful!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThere is a lot of information, and many of the plots and tables are particularly unreadable.\n\n\n\n\n\nHave your Generative AI assistant suggest what to look at when it is given the variable summary, from:\n\n\nglimpse(airlines_all)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nHere‚Äôs what I got from Claude:\nBased on the dataset you've provided, which appears to be about airline flights, I can suggest several plots and summaries that would be informative. Here are some recommendations:\n\nTime-based analysis:\n\nLine plot of average delays (dep_delay and arr_delay) by month and year\nHeatmap of delays by day of week and hour of day\nBox plots of air_time by month\n\n\nCarrier analysis:\n\nBar plot of flight counts by carrier\nBox plots of delays by carrier\nScatter plot of departure delay vs. arrival delay, colored by carrier\n\n\nRoute analysis:\n\nBar plot of most common routes (origin-dest pairs)\nMap visualization of routes with line thickness indicating frequency\n\n\nDistance analysis:\n\nScatter plot of distance vs. air_time\nHistogram of flight distances\n\n\nAircraft analysis:\n\nBar plot of most common aircraft models\nBox plot of delays by aircraft manufacturer\n\n\nAirport analysis:\n\nBar plot of busiest airports (by departure and arrival)\nMap of airports with point size indicating traffic volume\n\n\nSummaries:\n\nTable of average delays by carrier\nSummary statistics (mean, median, std dev) for air_time and distance\nCorrelation matrix of numeric variables\n\n\nTime series:\n\nTime series decomposition of daily flight counts\n\n\nCategorical data:\n\nStacked bar charts for engine types by manufacturer\n\n\nPerformance metrics:\n\nScatter plot of scheduled arrival time vs. actual arrival time\nIt‚Äôs nicely organise by different types of analysis activities that might be done. Each of these require different ways to aggregate the data, to make summaries or plots by various categorical variables. But it doesn‚Äôt suggest pre-processing that might be necessary, so I asked it to tell me what might be needed:\nBefore creating visualizations and summaries, some pre-processing steps would be beneficial for this dataset. Here are some recommendations:\n\nHandle missing values:\n\nCheck for NA or null values, especially in columns like speed, air_time, and delay fields.\nDecide whether to impute missing values or remove rows with missing data, depending on the extent and nature of missingness.\n\n\nDate and time formatting:\n\nCombine year_flights, month, and day into a single date column.\nEnsure time_hour is properly formatted as a datetime object.\n\n\nCategorical data:\n\nConvert appropriate columns to factors (e.g., carrier, origin, dest, manufacturer).\nCheck for and handle any inconsistent spellings or codings in categorical variables.\n\n\nDelay calculations:\n\nCreate a total_delay column by summing dep_delay and arr_delay.\nConsider creating a binary column for whether a flight was delayed (e.g., over 15 minutes).\n\n\nOutlier detection:\n\nCheck for and possibly handle extreme values in delay times, air_time, and distance.\n\n\nFeature engineering:\n\nCreate a day_of_week column from the date.\nCalculate flight speed from distance and air_time.\nCreate a season column based on month.\n\n\nData type conversions:\n\nEnsure numeric columns are of the appropriate type (int or float).\nConvert time columns (dep_time, arr_time, etc.) to a more usable format if they're not already.\n\n\nGeocoding:\n\nEnsure lat and lon columns for both origin and destination are numeric and contain valid coordinates.\n\n\nConsistency checks:\n\nVerify that arr_time is always later than dep_time.\nCheck that air_time is consistent with the difference between arr_time and dep_time.\n\n\nMerging data:\n\nIf the planes and airports data are in separate datasets, merge them carefully with the main flights data.\n\n\nCreating subsets:\n\nConsider creating smaller datasets for specific analyses (e.g., by year, by carrier) to improve processing speed.\nThis is quite useful!\n\n\n\n\n\n\n8.\nIn a limited fashion, lets work through some areas suggested by Claude.\n\nUse visdat to examine the variable types and missing values. You‚Äôll need to take a sample of the data because there are too many observations to reasonably plot. However, a sample should give reasonable insight on the reliability of most variables. What variables may not be useful because they have too many missing values?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nvis_dat(airlines_all[sample(1:nrow(airlines_all), 1000),])\n\n\n\n\n\n\n\n\nspeed is mostly missing. It definitely cannot be used.\ntype, manufacturer, model, engine, year_planes, engines, seats have a lot of missings, and so maybe unusable. It is possible that these missings are for some types of planes, and other types of planes have more complete records.\n\n\n\n\nFor the rest of these questions, you decide how to process the data, make summaries or plots to provide answers.\n\nWhich carrier had the most flights?\nIs this the same for each month? Or day of the week?\nAre there more departure delays for flights in the morning hours, or evening hours?\nFind an error in the data, e.g.¬†a flight that arrived before it left.\nWith your neighbour in the tutorial come up with one thing that is a bit surprising to you that you can learn from this data. Make sure you state what you expected to see, and why what you saw was then a surprise. (It is possible that you can use the DataExplorer report to look at something you had not thought to examine, as motivation.)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nairlines_all |&gt; count(carrier, sort=TRUE)\n\n# A tibble: 17 √ó 2\n   carrier     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 UA      58665\n 2 B6      54635\n 3 EV      54173\n 4 DL      48110\n 5 AA      32729\n 6 MQ      26397\n 7 US      20536\n 8 9E      18460\n 9 WN      12275\n10 VX       5162\n11 FL       3260\n12 &lt;NA&gt;     2812\n13 AS        714\n14 F9        685\n15 YV        601\n16 HA        342\n17 OO         32\n\nairlines_all |&gt; \n  filter(carrier %in% c(\"UA\", \"B6\", \"EV\", \"DL\")) |&gt;\n  mutate(carrier = factor(carrier, levels = c(\"UA\", \"B6\", \"EV\", \"DL\"))) |&gt;\n  count(month, carrier) |&gt;\n  mutate(month = factor(month, levels=1:12)) |&gt;\n  pivot_wider(names_from = carrier, \n              values_from = n, \n              values_fill=0)\n\n# A tibble: 12 √ó 5\n   month    UA    B6    EV    DL\n   &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 1      4637  4427  4171  3690\n 2 2      4346  4103  3827  3444\n 3 3      4971  4772  4726  4189\n 4 4      5047  4517  4561  4092\n 5 5      4960  4576  4817  4082\n 6 6      4975  4622  4456  4126\n 7 7      5066  4984  4641  4251\n 8 8      5124  4952  4563  4318\n 9 9      4694  4291  4725  3883\n10 10     5060  4361  4908  4093\n11 11     4854  4289  4471  3849\n12 12     4931  4741  4307  4093\n\nairlines_all |&gt; \n  filter(carrier %in% c(\"UA\", \"B6\", \"EV\", \"DL\")) |&gt;\n  mutate(carrier = factor(carrier, levels = c(\"UA\", \"B6\", \"EV\", \"DL\"))) |&gt;\n  count(month, carrier) |&gt;\n  ggplot(aes(x=month, y=n, colour=carrier)) + \n    geom_line() +\n    scale_x_continuous(\"\", breaks = 1:12)\n\n\n\n\n\n\n\nairlines_all |&gt;\n  group_by(hour) |&gt;\n  summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  ggplot(aes(x=hour, y=dep_delay)) +\n    geom_point() +\n    geom_smooth(se=F)\n\n\n\n\n\n\n\nairlines_all |&gt;\n  ggplot() +\n    geom_abline(intercept=0, slope=1, colour=\"red\") +\n    geom_point(aes(x=dep_delay, y=arr_delay)) +\n    theme(aspect.ratio=1)\n\n\n\n\n\n\n\nairlines_all |&gt;\n  ggplot() +\n    geom_abline(intercept=0, slope=1, colour=\"red\") +\n    geom_point(aes(x=dep_time, y=arr_time)) +\n    theme(aspect.ratio=1)\n\n\n\n\n\n\n\nairlines_all |&gt; \n  filter(origin == dest)\n\n# A tibble: 0 √ó 42\n# ‚Ñπ 42 variables: year_flights &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, dep_time &lt;int&gt;,\n#   sched_dep_time &lt;int&gt;, dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;,\n#   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name_carrier &lt;chr&gt;,\n#   year_planes &lt;int&gt;, type &lt;chr&gt;, manufacturer &lt;chr&gt;, model &lt;chr&gt;,\n#   engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;, name_origin &lt;chr&gt;, ‚Ä¶\n\n\n\nUA, United Airlines\nOoh, September EV had more flights than UA!\nDefinitely evening hours.\nThere are some flights that leave substantially early, maybe not a mistake but worrying for a traveller.\nWhat did you find?\n\n\n\n\n\n\n\n9.\nTable 2 of the Landscape paper summarises the activities of two early phases of the CRISP-DM standard. What does CRISP-DM mean? The implication is that EDA is related to ‚Äúdata understanding‚Äù and ‚Äúdata preparation‚Äù. Would you agree with this or disagree? Why?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nCross-Industry Standard Process for Data Mining; EDA techniques can be useful for some parts of these stages, for example finding outliers, or examining missing value patterns. Some of these steps are important for effective EDA, too, for example, you need to know what types of variables you have in order to decide what types of plots to make.\n\n\n\n\n\n\n10.\nTable 1 of the paper is summarising CRAN downloads and GitHub activity is hard to read. How are the rows sorted? What is the most important information communicated by the table? In what way(s) might revising this table make it easier to read and digest the most important information?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMost important information is the download rate because the purpose is to know which are the commonly used packages. Sorting rows by downloads makes the table easier to read."
  },
  {
    "objectID": "week2/tutorialsol.html#finishing-up",
    "href": "week2/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 2",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week2/tutorial.html",
    "href": "week2/tutorial.html",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "The purpose of this tutorial is to scope out the software reporting to do EDA in R. We want to understand the capabilities and the limitations."
  },
  {
    "objectID": "week2/tutorial.html#objectives",
    "href": "week2/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "The purpose of this tutorial is to scope out the software reporting to do EDA in R. We want to understand the capabilities and the limitations."
  },
  {
    "objectID": "week2/tutorial.html#preparation",
    "href": "week2/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 2",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is The Landscape of R Packages for Automated Exploratory Data Analysis. This is a lovely summary of software available that is considered to do exploratory data analysis (EDA). (Note: Dr Cook considers these to be mostly descriptive statistics packages, not exploratory data analysis in the true spirit of the term.) This reading will be the basis of the tutorial exercises today.\n\nComplete the weekly quiz, before the deadline!\nInstall this list of R packages, in addition to what you installed in the previous weeks:\n\n\ninstall.packages(c(\"arsenal\", \"autoEDA\", \"DataExplorer\", \"dataMaid\", \"dlookr\", \"ExPanDaR\", \"explore\", \"exploreR\", \"funModeling\", \"inspectdf\", \"RtutoR\", \"SmartEDA\", \"summarytools\", \"visdat\", \"xray\", \"cranlogs\", \"tidyverse\", \"nycflights13\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week2/tutorial.html#exercises",
    "href": "week2/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 2",
    "section": "üì• Exercises",
    "text": "üì• Exercises\nThe article lists a number of R packages that might be used for EDA: arsenal, autoEDA, DataExplorer, dataMaid, dlookr, ExPanDaR, explore, exploreR, funModeling, inspectdf, RtutoR, SmartEDA, summarytools, visdat, xray.\n\n1.\nWhat package had the highest number of CRAN downloads as of 12.07.2019? (Based on the paper.)\n\n\n2.\nOpen up the shiny server for checking download rates at https://hadley.shinyapps.io/cran-downloads/. Which of these packages has the highest download rate over the period Jan 1, 2024-today?\n\n\n3.\nWhat is an interesting pattern to observe from the time series plot of all the downloads?\n\n\n4.\nHow many functions does Staniak and Biecek (2019) say visdat has for doing EDA? Explore what each of them does, by running the example code for each function. What do you think are the features that make visdat a really popular package?\n\n\n5.\nThe package summarytools appears to becoming more favourable relative to visdat. Take a look at this package and explain what tools it has that are not available in visdat.\n\n\n6.\nWhy do you think the package SmartEDA has gone out of favour?\n\n\n7.\nThe SmartEDA::ExpReport() and DataExplorer::create_report() are functions that generates a long report when provided with a data set. Try this using the latter function, on the nycflights13 data, using this code:\n\n# DataExplorer\nlibrary(DataExplorer)\nlibrary(nycflights13)\nlibrary(tidyverse)\n\n# Create a big data set\nairlines_all &lt;- flights |&gt; \n  full_join(airlines, by = \"carrier\") |&gt;\n  full_join(planes, by = \"tailnum\", \n            suffix = c(\"_flights\", \"_planes\")) |&gt;\n  full_join(airports, by = c(\"origin\"=\"faa\"), \n            suffix = c(\"_carrier\", \"_origin\")) |&gt;\n  full_join(airports, by = c(\"dest\"=\"faa\"), \n            suffix = c(\"_origin\", \"_dest\"))\n\n\ncreate_report(airlines_all, y = \"arr_delay\")\n\nThe code instructs to use arr_delay as a ‚Äúresponse variable‚Äù. Give some reasons why this report is not very useful!\n\nHave your Generative AI assistant suggest what to look at when it is given the variable summary, from:\n\n\nglimpse(airlines_all)\n\n\n\n8.\nIn a limited fashion, lets work through some areas suggested by Claude.\n\nUse visdat to examine the variable types and missing values. You‚Äôll need to take a sample of the data because there are too many observations to reasonably plot. However, a sample should give reasonable insight on the reliability of most variables. What variables may not be useful because they have too many missing values?\n\nFor the rest of these questions, you decide how to process the data, make summaries or plots to provide answers.\n\nWhich carrier had the most flights?\nIs this the same for each month? Or day of the week?\nAre there more departure delays for flights in the morning hours, or evening hours?\nFind an error in the data, e.g.¬†a flight that arrived before it left.\nWith your neighbour in the tutorial come up with one thing that is a bit surprising to you that you can learn from this data. Make sure you state what you expected to see, and why what you saw was then a surprise. (It is possible that you can use the DataExplorer report to look at something you had not thought to examine, as motivation.)\n\n\n\n9.\nTable 2 of the Landscape paper summarises the activities of two early phases of the CRISP-DM standard. What does CRISP-DM mean? The implication is that EDA is related to ‚Äúdata understanding‚Äù and ‚Äúdata preparation‚Äù. Would you agree with this or disagree? Why?\n\n\n10.\nTable 1 of the paper is summarising CRAN downloads and GitHub activity is hard to read. How are the rows sorted? What is the most important information communicated by the table? In what way(s) might revising this table make it easier to read and digest the most important information?"
  },
  {
    "objectID": "week2/tutorial.html#finishing-up",
    "href": "week2/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 2",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/slides.html#the-role-of-initial-data-analysis",
    "href": "week3/slides.html#the-role-of-initial-data-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "The role of initial data analysis",
    "text": "The role of initial data analysis\n\n\nThe first thing to do with data is to look at them ‚Ä¶. usually means tabulating and plotting the data in many different ways to see what‚Äôs going on. With the wide availability of computer packages and graphics nowadays there is no excuse for ducking the labour of this preliminary phase, and it may save some red faces later.\nCrowder, M. J. & Hand, D. J. (1990) ‚ÄúAnalysis of Repeated Measures‚Äù"
  },
  {
    "objectID": "week3/slides.html#initial-data-analysis-and-confirmatory-analysis",
    "href": "week3/slides.html#initial-data-analysis-and-confirmatory-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Initial Data Analysis and Confirmatory Analysis",
    "text": "Initial Data Analysis and Confirmatory Analysis\n\n\n\nPrior to conducting a confirmatory data analysis, it is important to conduct an initial data analysis (IDA).\n\n\n\nConfirmatory data analysis (CDA) is focused on statistical inference and includes procedures for:\n\nhypothesis testing,\npredictive modelling,\nparameter estimation including uncertainty,\nmodel selection.\n\n\n\n\n\n\nIDA includes:\n\ndescribing the data and collection procedures\nscrutinise data for errors, outliers, missing observations\ncheck assumptions for confirmatory data analysis\n\n\n\nIDA is sometimes called preliminary data analysis.\n\n\n\n\nIDA is related to exploratory data analysis (EDA) in the sense that it is primarily conducted graphically, and there are few formal tests available."
  },
  {
    "objectID": "week3/slides.html#taxonomies-are-useful-but-rarely-perfect",
    "href": "week3/slides.html#taxonomies-are-useful-but-rarely-perfect",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Taxonomies are useful but rarely perfect",
    "text": "Taxonomies are useful but rarely perfect"
  },
  {
    "objectID": "week3/slides.html#objectives-of-ida",
    "href": "week3/slides.html#objectives-of-ida",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Objectives of IDA?",
    "text": "Objectives of IDA?\n\nThe main objective for IDA is to intercept any problems in the data that might adversely affect the confirmatory data analysis.\n\n\n\nThe role of CDA is to answer the intended question(s) that the data were collected for.\n\n\nIDA is often unreported in the data analysis reports or scientific papers, for various reasons. It might not have been done, or it may have been conducted but there was no space in the paper to report on it."
  },
  {
    "objectID": "week3/slides.html#ida-in-government-statistics",
    "href": "week3/slides.html#ida-in-government-statistics",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA in government statistics",
    "text": "IDA in government statistics\nThe purpose of data cleaning is to bring data up to a level of quality such that it can reliably be used for the production of statistical models or statements.\nA statistical value chain is constructed by defining a number of meaningful intermediate data products, for which a chosen set of quality attributes are well described.\n\n\n\n\n\nvan der Loo & de Jonge (2018) Statistical Data Cleaning with Applications in R"
  },
  {
    "objectID": "week3/slides.html#ida-in-health-and-medical-data",
    "href": "week3/slides.html#ida-in-health-and-medical-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA in health and medical data",
    "text": "IDA in health and medical data\n\nHuebner et al (2018)‚Äôs six steps of IDA: (1) Metadata setup, (2) Data cleaning, (3) Data screening, (4) Initial reporting, (5) Refining and updating the analysis plan, (6) Reporting IDA in documentation."
  },
  {
    "objectID": "week3/slides.html#heed-these-words",
    "href": "week3/slides.html#heed-these-words",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Heed these words",
    "text": "Heed these words\n\n\n\nIDA prepares an analyst for CDA. One needs to be careful about NOT compromising the inference.\n\nHow do you compromise inference?\n\n\nChange your inference or questions based on what you find in IDA.\nOutlier removal or not.\nMissing value imputation choices.\nTreatment of zeros.\nHandling of variable type, categorical temporal.\nLack of multivariate relationship checking, including subsets based on levels of categorical variables.\nChoosing variables and observations.\n\n\n\n\nHow do you avoid these errors?\n\nDocument ALL the IDA, using a reproducible analysis script.\nPre-register your CDA plan, so that your CDA questions do not change.\nDecisions made on outlier removal, variable selection, recoding, sampling, handling of zeros have known affects on results, and are justifiable.\n\nInsure yourself against accusations of data snooping, data dredging, data fishing."
  },
  {
    "objectID": "week3/slides.html#data-screening",
    "href": "week3/slides.html#data-screening",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data screening",
    "text": "Data screening"
  },
  {
    "objectID": "week3/slides.html#data-screening-1",
    "href": "week3/slides.html#data-screening-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data screening",
    "text": "Data screening\n\n\n\nIt‚Äôs important to check how the data are understood by the computer.\n\nthat is, checking for data type:\n\nWas the date read in as character?\nWas a factor read in as numeric?\n\n\n\nAlso important for making inference is to know whether the data supports making broader conclusions. How was the data collected? Is it clear what the population of interest is, and that the data is a representative sample of this population?"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-12",
    "href": "week3/slides.html#example-checking-the-data-type-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type (1/2)",
    "text": "Example: Checking the data type (1/2)\n\n\nlecture3-example.xlsx\n\n\n\n\n\nlibrary(readxl)\nlibrary(here)\ndf &lt;- read_excel(here(\"data/lecture3-example.xlsx\"))\ndf\n\n# A tibble: 5 √ó 4\n     id date                loc       temp\n  &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;    &lt;dbl&gt;\n1     1 2010-01-03 00:00:00 New York  42  \n2     2 2010-02-03 00:00:00 New York  41.4\n3     3 2010-03-03 00:00:00 New York  38.5\n4     4 2010-04-03 00:00:00 New York  41.1\n5     5 2010-05-03 00:00:00 New York  39.8\n\n\n\nWhat problems are there with the computer‚Äôs interpretation of data type?\nWhat context specific issues indicate incorrect computer interpretation?"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-22",
    "href": "week3/slides.html#example-checking-the-data-type-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type (2/2)",
    "text": "Example: Checking the data type (2/2)\n\n\n\nlibrary(lubridate)\ndf &lt;- read_excel(here(\"data/lecture3-example.xlsx\"), \n                 col_types = c(\"text\", \n                               \"date\", \n                               \"text\",\n                               \"numeric\"))\n\ndf |&gt; \n  mutate(id = as.factor(id),\n         date = ydm(date)) |&gt;\n  mutate(\n         day = day(date),\n         month = month(date),\n         year = year(date)) \n\n# A tibble: 5 √ó 7\n  id    date       loc       temp   day month  year\n  &lt;fct&gt; &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1     2010-03-01 New York  42       1     3  2010\n2 2     2010-03-02 New York  41.4     2     3  2010\n3 3     2010-03-03 New York  38.5     3     3  2010\n4 4     2010-03-04 New York  41.1     4     3  2010\n5 5     2010-03-05 New York  39.8     5     3  2010\n\n\n\n\nid is now a factor instead of integer\nday, month and year are now extracted from the date\nIs it okay now?\n\n\n\n\nIn the United States, it‚Äôs common to use the date format MM/DD/YYYY (gasps) while the rest of the world commonly uses DD/MM/YYYY or better still YYYY/MM/DD.\n\n\n\n\nIt‚Äôs highly probable that the dates are 1st-5th March and not 3rd of Jan-May.\n\n\n\n\nYou can validate interpretation of temperature using weather database."
  },
  {
    "objectID": "week3/slides.html#example-specifying-the-data-type-with-r",
    "href": "week3/slides.html#example-specifying-the-data-type-with-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Specifying the data type with R",
    "text": "Example: Specifying the data type with R\n\n\n\nYou can robustify your workflow by ensuring you have a check for the expected data type in your code.\n\n\nxlsx_df &lt;- read_excel(here(\"data/lecture3-example.xlsx\"),\n                 col_types = c(\"text\", \"date\", \"text\", \"numeric\"))  |&gt; \n  mutate(id = as.factor(id), \n         date = as.character(date),\n         date = as.Date(date, format = \"%Y-%d-%m\"))\n\n\n\nread_csv has a broader support for col_types\n\n\ncsv_df &lt;- read_csv(here::here(\"data/lecture3-example.csv\"),\n                 col_types = cols(\n                      id = col_factor(),\n                      date = col_date(format = \"%m/%d/%y\"),\n                      loc = col_character(),\n                      temp = col_double()))\n\n\nThe checks (or coercions) ensure that even if the data are updated, you can have some confidence that any data type error will be picked up before further analysis."
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-with-r",
    "href": "week3/slides.html#example-checking-the-data-type-with-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type with R",
    "text": "Example: Checking the data type with R\nYou can have a quick glimpse of the data type with:\n\ndplyr::glimpse(xlsx_df)\n\nRows: 5\nColumns: 4\n$ id   &lt;fct&gt; 1, 2, 3, 4, 5\n$ date &lt;date&gt; 2010-03-01, 2010-03-02, 2010-03-03, 2010-03-0‚Ä¶\n$ loc  &lt;chr&gt; \"New York\", \"New York\", \"New York\", \"New Yor‚Ä¶\n$ temp &lt;dbl&gt; 42, 41, 38, 41, 40\n\ndplyr::glimpse(csv_df)\n\nRows: 5\nColumns: 4\n$ id   &lt;fct&gt; 1, 2, 3, 4, 5\n$ date &lt;date&gt; 2010-03-01, 2010-03-02, 2010-03-03, 2010-03-0‚Ä¶\n$ loc  &lt;chr&gt; \"New York\", \"New York\", \"New York\", \"New Yor‚Ä¶\n$ temp &lt;dbl&gt; 42, 41, 38, 41, 40"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-visually",
    "href": "week3/slides.html#example-checking-the-data-type-visually",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type visually",
    "text": "Example: Checking the data type visually\n\n\nYou can also visualise the data type with:\n\nlibrary(visdat)\nvis_dat(xlsx_df)\n\n\n\n\n\n\n\n\n\n\nlibrary(inspectdf)\ninspect_types(xlsx_df)  |&gt; \n  show_plot()"
  },
  {
    "objectID": "week3/slides.html#data-cleaning",
    "href": "week3/slides.html#data-cleaning",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning",
    "text": "Data cleaning"
  },
  {
    "objectID": "week3/slides.html#data-cleaning-12",
    "href": "week3/slides.html#data-cleaning-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning (1/2)",
    "text": "Data cleaning (1/2)\nData quality checks should be one of the first steps in the data analysis to assess any problems with the data.\nThese include using common or domain knowledge to check if the recorded data have sensible values.\n\n\nAre positive values, e.g.¬†height and weight, recorded as positive values with a plausible range?\nIf the data are counts, do the recorded values contain non-integer values?\nFor compositional data, do the values add up to 100% (or 1)? If not, is that a measurement error or due to rounding? Or is another variable missing?\nDoes the data contain only positives, ie disease occurrences, or warranty claims? If so, what would the no report group look like?"
  },
  {
    "objectID": "week3/slides.html#data-cleaning-22",
    "href": "week3/slides.html#data-cleaning-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning (2/2)",
    "text": "Data cleaning (2/2)\nIn addition, numerical or graphical summaries may reveal that there is unwanted structure in the data, for example,\n\n\n\nDoes the treatment group have different demographic characteristics to the control group?\nAre the distributions similar between the or training and test sets?\nAre there sufficient measurements for each level of categorical variable, or across the range of numerical variables?\n\n\n\n\nDoes the distribution of the data imply violations of assumptions for the CDA, such as\n\nnon-normality,\ndiscrete rather real-valued, or\ndifferent variance in different domains?\n\n\n\n\n\nData scrutinizing is a process that you get better at with practice and have familiarity with the domain area."
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-quality",
    "href": "week3/slides.html#example-checking-the-data-quality",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data quality",
    "text": "Example: Checking the data quality\n\n\n\n\n# A tibble: 9 √ó 4\n  id    date       loc        temp\n  &lt;fct&gt; &lt;date&gt;     &lt;chr&gt;     &lt;dbl&gt;\n1 1     2010-03-01 New York   42  \n2 2     2010-03-02 New York   41.4\n3 3     2010-03-03 New York   38.5\n4 4     2010-03-04 New York   41.1\n5 5     2010-03-05 New York   39.8\n6 6     2020-03-01 Melbourne  30.6\n7 7     2020-03-02 Melbourne  17.9\n8 8     2020-03-03 Melbourne  18.6\n9 9     2020-03-04 &lt;NA&gt;       21.3\n\n\n\n\nNumerical or graphical summaries or even just eye-balling the data helps to uncover some data quality issues.\nAny issues here?\n\n\n\n\nThere‚Äôs a missing value in loc.\nTemperature is in Farenheit for New York but Celsius in Melbourne (you can validate this again using external sources)."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-17",
    "href": "week3/slides.html#case-study-world-development-indicators-17",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (1/7)",
    "text": "Case study: World development indicators (1/7)\n\n\n\noptions(width=80)\nraw_dat &lt;- read_csv(here(\"data/world-development-indicators.csv\"), \n                    na = \"..\", n_max = 11935)\nglimpse(raw_dat)\n\nRows: 11,935\nColumns: 54\n$ `Country Name`  &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Argentina\", \"A‚Ä¶\n$ `Country Code`  &lt;chr&gt; \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\"‚Ä¶\n$ `Series Name`   &lt;chr&gt; \"Adolescent fertility rate (births per 1,000 women age‚Ä¶\n$ `Series Code`   &lt;chr&gt; \"SP.ADO.TFRT\", \"NV.AGR.TOTL.ZS\", \"ER.H2O.FWTL.ZS\", \"SH‚Ä¶\n$ `1969 [YR1969]` &lt;dbl&gt; 6.4e+01, 9.2e+00, NA, NA, 3.3e+00, NA, 2.2e+01, NA, NA‚Ä¶\n$ `1970 [YR1970]` &lt;dbl&gt; 6.5e+01, 9.6e+00, NA, NA, 3.5e+00, NA, 2.5e+01, NA, NA‚Ä¶\n$ `1971 [YR1971]` &lt;dbl&gt; 6.7e+01, 1.1e+01, NA, NA, 3.7e+00, NA, 2.4e+01, 8.7e+0‚Ä¶\n$ `1972 [YR1972]` &lt;dbl&gt; 6.8e+01, 1.1e+01, NA, NA, 3.6e+00, NA, 1.9e+01, 9.2e+0‚Ä¶\n$ `1973 [YR1973]` &lt;dbl&gt; 7.1e+01, 1.2e+01, NA, NA, 3.7e+00, NA, 2.7e+01, 9.6e+0‚Ä¶\n$ `1974 [YR1974]` &lt;dbl&gt; 7.5e+01, 1.0e+01, NA, NA, 3.7e+00, NA, 3.0e+01, 9.9e+0‚Ä¶\n$ `1975 [YR1975]` &lt;dbl&gt; 7.8e+01, 6.6e+00, NA, NA, 3.6e+00, NA, 2.9e+01, 1.0e+0‚Ä¶\n$ `1976 [YR1976]` &lt;dbl&gt; 8.1e+01, 8.2e+00, NA, NA, 3.8e+00, NA, 2.0e+01, 1.0e+0‚Ä¶\n$ `1977 [YR1977]` &lt;dbl&gt; 8.4e+01, 8.1e+00, 9.5e+00, NA, 3.7e+00, NA, 2.6e+01, 1‚Ä¶\n$ `1978 [YR1978]` &lt;dbl&gt; 8.2e+01, 7.5e+00, NA, NA, 3.8e+00, NA, 2.9e+01, 1.1e+0‚Ä¶\n$ `1979 [YR1979]` &lt;dbl&gt; 8.0e+01, 7.8e+00, NA, NA, 4.0e+00, NA, 3.1e+01, 1.2e+0‚Ä¶\n$ `1980 [YR1980]` &lt;dbl&gt; 7.8e+01, 6.4e+00, NA, NA, 3.9e+00, NA, 3.3e+01, 1.2e+0‚Ä¶\n$ `1981 [YR1981]` &lt;dbl&gt; 7.6e+01, 6.5e+00, NA, NA, 3.6e+00, NA, 4.8e+01, 1.2e+0‚Ä¶\n$ `1982 [YR1982]` &lt;dbl&gt; 7.4e+01, 9.6e+00, NA, NA, 3.6e+00, NA, 4.6e+01, 1.2e+0‚Ä¶\n$ `1983 [YR1983]` &lt;dbl&gt; 7.4e+01, 8.7e+00, NA, NA, 3.6e+00, NA, 4.6e+01, 1.3e+0‚Ä¶\n$ `1984 [YR1984]` &lt;dbl&gt; 7.4e+01, 8.3e+00, NA, NA, 3.6e+00, NA, 4.2e+01, 1.3e+0‚Ä¶\n$ `1985 [YR1985]` &lt;dbl&gt; 7.4e+01, 7.6e+00, NA, NA, 3.3e+00, NA, 3.3e+01, 1.3e+0‚Ä¶\n$ `1986 [YR1986]` &lt;dbl&gt; 7.4e+01, 7.8e+00, NA, NA, 3.4e+00, NA, 3.3e+01, 1.3e+0‚Ä¶\n$ `1987 [YR1987]` &lt;dbl&gt; 7.3e+01, 8.1e+00, NA, NA, 3.7e+00, NA, 4.8e+01, 1.4e+0‚Ä¶\n$ `1988 [YR1988]` &lt;dbl&gt; 7.3e+01, 9.0e+00, NA, NA, 3.8e+00, NA, 4.3e+01, 1.4e+0‚Ä¶\n$ `1989 [YR1989]` &lt;dbl&gt; 7.3e+01, 9.6e+00, NA, NA, 3.6e+00, NA, 8.0e+01, 1.3e+0‚Ä¶\n$ `1990 [YR1990]` &lt;dbl&gt; 7.3e+01, 8.1e+00, NA, 9.7e+01, 3.4e+00, NA, 3.2e+01, 1‚Ä¶\n$ `1991 [YR1991]` &lt;dbl&gt; 7.3e+01, 6.7e+00, NA, NA, 3.5e+00, NA, 2.3e+01, 1.3e+0‚Ä¶\n$ `1992 [YR1992]` &lt;dbl&gt; 7.3e+01, 6.0e+00, NA, 9.6e+01, 3.6e+00, NA, 2.2e+01, 1‚Ä¶\n$ `1993 [YR1993]` &lt;dbl&gt; 7.3e+01, 5.1e+00, NA, NA, 3.5e+00, NA, 2.6e+01, 1.5e+0‚Ä¶\n$ `1994 [YR1994]` &lt;dbl&gt; 7.2e+01, 5.1e+00, NA, NA, 3.5e+00, NA, 2.7e+01, 1.6e+0‚Ä¶\n$ `1995 [YR1995]` &lt;dbl&gt; 7.1e+01, 5.4e+00, NA, 9.8e+01, 3.7e+00, NA, 2.8e+01, 1‚Ä¶\n$ `1996 [YR1996]` &lt;dbl&gt; 7.0e+01, 5.6e+00, NA, NA, 3.8e+00, NA, 2.8e+01, 1.7e+0‚Ä¶\n$ `1997 [YR1997]` &lt;dbl&gt; 7.0e+01, 5.2e+00, 9.8e+00, 9.7e+01, 3.9e+00, NA, 3.0e+‚Ä¶\n$ `1998 [YR1998]` &lt;dbl&gt; 6.9e+01, 5.3e+00, NA, 9.8e+01, 3.9e+00, NA, 3.3e+01, 2‚Ä¶\n$ `1999 [YR1999]` &lt;dbl&gt; 6.8e+01, 4.5e+00, NA, 9.8e+01, 4.0e+00, NA, 3.6e+01, 2‚Ä¶\n$ `2000 [YR2000]` &lt;dbl&gt; 6.7e+01, 4.7e+00, NA, 9.9e+01, 3.8e+00, NA, 3.4e+01, 2‚Ä¶\n$ `2001 [YR2001]` &lt;dbl&gt; 6.6e+01, 4.6e+00, NA, 9.8e+01, 3.6e+00, 6.5e+01, 3.7e+‚Ä¶\n$ `2002 [YR2002]` &lt;dbl&gt; 6.5e+01, 1.0e+01, NA, 9.9e+01, 3.3e+00, NA, 6.2e+01, 2‚Ä¶\n$ `2003 [YR2003]` &lt;dbl&gt; 6.5e+01, 1.0e+01, NA, 9.9e+01, 3.5e+00, NA, 5.1e+01, 2‚Ä¶\n$ `2004 [YR2004]` &lt;dbl&gt; 6.4e+01, 8.4e+00, NA, 9.9e+01, 4.1e+00, NA, 4.2e+01, 2‚Ä¶\n$ `2005 [YR2005]` &lt;dbl&gt; 6.4e+01, 7.9e+00, NA, 9.9e+01, 4.1e+00, 7.9e+01, 3.5e+‚Ä¶\n$ `2006 [YR2006]` &lt;dbl&gt; 6.4e+01, 6.9e+00, NA, 9.9e+01, 4.4e+00, NA, 2.8e+01, 2‚Ä¶\n$ `2007 [YR2007]` &lt;dbl&gt; 6.4e+01, 7.5e+00, NA, 9.9e+01, 4.4e+00, NA, 2.6e+01, 2‚Ä¶\n$ `2008 [YR2008]` &lt;dbl&gt; 6.4e+01, 7.3e+00, NA, 9.5e+01, 4.7e+00, NA, 2.2e+01, 2‚Ä¶\n$ `2009 [YR2009]` &lt;dbl&gt; 6.4e+01, 5.3e+00, NA, 9.8e+01, 4.4e+00, NA, 2.6e+01, 2‚Ä¶\n$ `2010 [YR2010]` &lt;dbl&gt; 6.4e+01, 7.1e+00, NA, 9.5e+01, 4.6e+00, NA, 2.5e+01, 2‚Ä¶\n$ `2011 [YR2011]` &lt;dbl&gt; 6.4e+01, 7.0e+00, NA, 9.7e+01, 4.6e+00, NA, 2.6e+01, 2‚Ä¶\n$ `2012 [YR2012]` &lt;dbl&gt; 6.4e+01, 5.8e+00, 1.3e+01, 9.8e+01, 4.6e+00, 5.5e+01, ‚Ä¶\n$ `2013 [YR2013]` &lt;dbl&gt; 6.4e+01, 6.1e+00, NA, 9.7e+01, 4.5e+00, 8.1e+01, 3.3e+‚Ä¶\n$ `2014 [YR2014]` &lt;dbl&gt; 6.4e+01, 6.7e+00, 1.3e+01, 1.0e+02, 4.7e+00, NA, 3.4e+‚Ä¶\n$ `2015 [YR2015]` &lt;dbl&gt; 6.3e+01, 5.2e+00, NA, 1.0e+02, NA, NA, 4.0e+01, NA, NA‚Ä¶\n$ `2016 [YR2016]` &lt;dbl&gt; 6.3e+01, 6.4e+00, NA, NA, NA, NA, 3.8e+01, NA, NA, 1.3‚Ä¶\n$ `2017 [YR2017]` &lt;dbl&gt; NA, 5.6e+00, NA, NA, NA, NA, 3.9e+01, NA, NA, 1.1e+01,‚Ä¶\n$ `2018 [YR2018]` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n\n\nWorld Development Indicators (WDI), sourced from the World Bank Group (2019)\n\n\n\nWhat are the data types?\nWhat are the variables?\nWhat are the observations?\nIs the data in tidy form?"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-27",
    "href": "week3/slides.html#case-study-world-development-indicators-27",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (2/7)",
    "text": "Case study: World development indicators (2/7)\n\n\n\ncountry_code_df &lt;- raw_dat  |&gt;\n  distinct(`Country Name`, `Country Code`)  |&gt;\n  rename_all(janitor::make_clean_names)  |&gt;\n  left_join(\n    countrycode::codelist |&gt; select(iso3c, region, continent),\n    by = c(\"country_code\" = \"iso3c\")\n  )  |&gt;\n  arrange(continent, region) \n\n\n\n\nRows: 217\nColumns: 4\n$ country_name &lt;chr&gt; \"Algeria\", \"Djibouti\", \"Egypt, Arab Rep.\", \"Libya\", \"Moro‚Ä¶\n$ country_code &lt;chr&gt; \"DZA\", \"DJI\", \"EGY\", \"LBY\", \"MAR\", \"TUN\", \"AGO\", \"BEN\", \"‚Ä¶\n$ region       &lt;chr&gt; \"Middle East & North Africa\", \"Middle East & North Africa‚Ä¶\n$ continent    &lt;chr&gt; \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa‚Ä¶\n\n\n# A tibble: 6 √ó 2\n  continent     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Africa       54\n2 Americas     46\n3 Asia         50\n4 Europe       46\n5 Oceania      19\n6 &lt;NA&gt;          2\n\n\n# A tibble: 8 √ó 2\n  region                         n\n  &lt;chr&gt;                      &lt;int&gt;\n1 East Asia & Pacific           37\n2 Europe & Central Asia         56\n3 Latin America & Caribbean     42\n4 Middle East & North Africa    21\n5 North America                  3\n6 South Asia                     8\n7 Sub-Saharan Africa            48\n8 &lt;NA&gt;                           2\n\n\n\n\n\n\nHow many countries are included\nHow many continents, regions?\nWhy are there NAs here?\n\n\n\ncountry_code_df |&gt; filter(is.na(continent))\n\n# A tibble: 2 √ó 4\n  country_name    country_code region continent\n  &lt;chr&gt;           &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;    \n1 Channel Islands CHI          &lt;NA&gt;   &lt;NA&gt;     \n2 Kosovo          XKX          &lt;NA&gt;   &lt;NA&gt;"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-37",
    "href": "week3/slides.html#case-study-world-development-indicators-37",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (3/7)",
    "text": "Case study: World development indicators (3/7)\n\n\n\nwdi_vars &lt;- raw_dat  |&gt;\n  select(`Series Name`, `Series Code`) |&gt;\n  distinct() |&gt;\n  rename_all(janitor::make_clean_names) \n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis will use the short name (series_code) for variables.\nStore full variable name (series_name) and short name (series_code) in a separate table.\nThe series_code will be used as the key whenever the full name is needed."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-47",
    "href": "week3/slides.html#case-study-world-development-indicators-47",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (4/7)",
    "text": "Case study: World development indicators (4/7)\n\n\n\nwdi &lt;- raw_dat  |&gt;\n  select(`Country Code`, `Series Code`, `1969 [YR1969]`:`2018 [YR2018]`) |&gt;\n  rename_all(janitor::make_clean_names) |&gt;\n  pivot_longer(x1969_yr1969:x2018_yr2018,\n               names_to = \"year\", \n               values_to = \"value\") |&gt;\n  mutate(year = as.numeric(str_sub(year, 2, 5)) ) |&gt;\n  pivot_wider(names_from = series_code,\n              values_from = value)\n\nwdi2017 &lt;- wdi  |&gt; filter(year == 2017)\n\n\nOrganise data into tidy form\nCheck missing value distribution"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-57",
    "href": "week3/slides.html#case-study-world-development-indicators-57",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (5/7)",
    "text": "Case study: World development indicators (5/7)\n\n\nCheck missings by\n\nvariable\ncountry"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-67",
    "href": "week3/slides.html#case-study-world-development-indicators-67",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (6/7)",
    "text": "Case study: World development indicators (6/7)\n\n\nLook at Costa Rica (CRI), most complete country\n\n\n\n\n\n\n\n\n\n\nTo illustrate imputation, we‚Äôll show one of the variables, that is relatively complete.\n\n\n\n\n\n\n\n\n\nImpute a few temporal missings using nearest neighbours."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-67-1",
    "href": "week3/slides.html#case-study-world-development-indicators-67-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (6/7)",
    "text": "Case study: World development indicators (6/7)\n\n\nMissings imputed using imputeTS using the moving average method.\n\n\n\n\n\n\n\n\n\n\n\nDon‚Äôt have to impute before scrutinizing data\nWhat are these numbers supposed to be?\n\nSE.PRM.CMPT.ZS is ‚ÄúPrimary completion rate, total (% of relevant age group)‚Äù\nDo we have any problems?\n\nYes. The explanation of the variable suggests the numbers should range between 0-100."
  },
  {
    "objectID": "week3/slides.html#summary-of-the-process",
    "href": "week3/slides.html#summary-of-the-process",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üìã Summary of the process",
    "text": "üìã Summary of the process\nThe steps we took roughly followed these:\n\n\n\n\n\n\n\n\nAt the end of this stage we would have:\n\n3 tables of data: country name/code, variables name/key, time series of multiple variables for many countries\nWhat would you like to learn from this data? What sort of models might be fitted? What types of hypotheses might be tested?\nHave we done anything that might have compromised the later analysis?"
  },
  {
    "objectID": "week3/slides.html#data-collection",
    "href": "week3/slides.html#data-collection",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data collection",
    "text": "Data collection"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-13",
    "href": "week3/slides.html#case-study-employment-data-in-australia-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (1/3)",
    "text": "Case study: Employment Data in Australia (1/3)\nBelow is the data from ABS that shows the total number of people employed in a given month from February 1976 to December 2019 using the original time series.\n\n\nload(here(\"data/employed.rda\"))\nglimpse(employed)\n\nRows: 557\nColumns: 4\n$ date  &lt;date&gt; 1978-02-01, 1978-03-01, 1978-04-01, 1978-05-01, 1978-06-01, 197‚Ä¶\n$ month &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1‚Ä¶\n$ year  &lt;dbl&gt; 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978‚Ä¶\n$ value &lt;dbl&gt; 5986, 6041, 6054, 6038, 6031, 6036, 6005, 6024, 6046, 6034, 6125‚Ä¶\n\n\nAustralian Bureau of Statistics, Labour force, Australia, Table 01. Labour force status by Sex, Australia - Trend, Seasonally adjusted and Original"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-23",
    "href": "week3/slides.html#case-study-employment-data-in-australia-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (2/3)",
    "text": "Case study: Employment Data in Australia (2/3)\n\n\nDo you notice anything?\n\n\n\n\n\n\n\n\n\n\n\nWhy do you think the number of people employed is going up each year?\n\n\n\nAustralian population is 25.39 million in 2019\n1.5% annual increase in population\nVic population is 6.681 million (Sep 2020) - 26%\nNSW population is 8.166 (Sep 2020) - 32%"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-33",
    "href": "week3/slides.html#case-study-employment-data-in-australia-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (3/3)",
    "text": "Case study: Employment Data in Australia (3/3)\n\n\n\nThere‚Äôs a suspicious change in August numbers from 2014.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA potential explanation for this is that there was a change in the survey from 2014.\n\nSee discussion on this at Hyndsight blog (10 October 2014)."
  },
  {
    "objectID": "week3/slides.html#case-study-2014-data-mining-cup-winners",
    "href": "week3/slides.html#case-study-2014-data-mining-cup-winners",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: 2014 Data Mining Cup winners",
    "text": "Case study: 2014 Data Mining Cup winners\n\n\n\nUgly plot of all observations provided in training sample, with response variable in colour, and test sample to predict.\nWhat does this tell you about the test sample?"
  },
  {
    "objectID": "week3/slides.html#case-study-french-frieshot-chips-12",
    "href": "week3/slides.html#case-study-french-frieshot-chips-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: french fries/hot chips (1/2)",
    "text": "Case study: french fries/hot chips (1/2)\n\n\n\n\nRows: 696\nColumns: 9\n$ time      &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ treatment &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ subject   &lt;fct&gt; 3, 3, 10, 10, 15, 15, 16, 16, ‚Ä¶\n$ rep       &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, ‚Ä¶\n$ potato    &lt;dbl&gt; 2.9, 14.0, 11.0, 9.9, 1.2, 8.8‚Ä¶\n$ buttery   &lt;dbl&gt; 0.0, 0.0, 6.4, 5.9, 0.1, 3.0, ‚Ä¶\n$ grassy    &lt;dbl&gt; 0.0, 0.0, 0.0, 2.9, 0.0, 3.6, ‚Ä¶\n$ rancid    &lt;dbl&gt; 0.0, 1.1, 0.0, 2.2, 1.1, 1.5, ‚Ä¶\n$ painty    &lt;dbl&gt; 5.5, 0.0, 0.0, 0.0, 5.1, 2.3, ‚Ä¶\n\n\n\n10 week sensory experiment, 12 individuals assessed taste of french fries on several scales (how potato-y, buttery, grassy, rancid, paint-y do they taste?), fried in one of 3 different oils, replicated twice.\n\nIs the design complete?\nAre replicates like each other?\nHow do the ratings on the different scales differ?\nAre raters giving different scores on average?\nDo ratings change over the weeks?"
  },
  {
    "objectID": "week3/slides.html#case-study-french-frieshot-chips-22",
    "href": "week3/slides.html#case-study-french-frieshot-chips-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: french fries/hot chips (2/2)",
    "text": "Case study: french fries/hot chips (2/2)\n\n\n\nIs the design complete?\n\n\nfrench_fries |&gt; count(subject)\n\n# A tibble: 12 √ó 2\n   subject     n\n   &lt;fct&gt;   &lt;int&gt;\n 1 3          54\n 2 10         60\n 3 15         60\n 4 16         60\n 5 19         60\n 6 31         54\n 7 51         60\n 8 52         60\n 9 63         60\n10 78         60\n11 79         54\n12 86         54\n\n\n\n\n\nfrench_fries |&gt; count(time)\n\n# A tibble: 10 √ó 2\n   time      n\n   &lt;fct&gt; &lt;int&gt;\n 1 1        72\n 2 2        72\n 3 3        72\n 4 4        72\n 5 5        72\n 6 6        72\n 7 7        72\n 8 8        72\n 9 9        60\n10 10       60\n\nfrench_fries |&gt; count(treatment)\n\n# A tibble: 3 √ó 2\n  treatment     n\n  &lt;fct&gt;     &lt;int&gt;\n1 1           232\n2 2           232\n3 3           232\n\nfrench_fries |&gt; count(rep)\n\n# A tibble: 2 √ó 2\n    rep     n\n  &lt;dbl&gt; &lt;int&gt;\n1     1   348\n2     2   348"
  },
  {
    "objectID": "week3/slides.html#case-study-warranty-claims",
    "href": "week3/slides.html#case-study-warranty-claims",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Warranty claims",
    "text": "Case study: Warranty claims\n\n\n\n\nRows: 4,561\nColumns: 14\n$ Region           &lt;chr&gt; \"East\", \"West\", \"North ‚Ä¶\n$ State            &lt;chr&gt; \"Delhi\", \"Gujarat\", \"We‚Ä¶\n$ Area             &lt;chr&gt; \"Urban\", \"Rural\", \"Urba‚Ä¶\n$ City             &lt;chr&gt; \"New Delhi\", \"Ahmedabad‚Ä¶\n$ Consumer_profile &lt;chr&gt; \"Personal\", \"Personal\",‚Ä¶\n$ TV_2001_Issue    &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 1, 1,‚Ä¶\n$ TV_2002_Issue    &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1,‚Ä¶\n$ TV_2003_Issue    &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 0,‚Ä¶\n$ Claim_Value      &lt;dbl&gt; 25000, 4216, 4000, 5000‚Ä¶\n$ Service_Centre   &lt;dbl&gt; 13, 10, 10, 12, 10, 10,‚Ä¶\n$ Product_Age      &lt;dbl&gt; 60, 672, 275, 10, 4, 34‚Ä¶\n$ Purchased_from   &lt;chr&gt; \"Dealer\", \"Dealer\", \"De‚Ä¶\n$ Call_details     &lt;dbl&gt; 1.3, 25.0, 11.0, 1.6, 0‚Ä¶\n$ Purpose          &lt;chr&gt; \"Complaint\", \"Other\", \"‚Ä¶\n\n\n\nTV_2001_Issue: failure of power supply\nTV_2002_Issue: failure of inverter\nTV_2003_Issue: failure of motherboard\n\n\n\n\nWhat is the population that this data is measuring?\nWhat is not measured?\n\n\n\n\n# A tibble: 2 √ó 2\n  City          n\n  &lt;chr&gt;     &lt;int&gt;\n1 Delhi       106\n2 Bangalore   320\n\n\nCan we say that Delhi has fewer problems with TVs than Bangalore?\n\n\n\nSource: ExcelR Projects. (2019). Warranty Claims. Kaggle."
  },
  {
    "objectID": "week3/slides.html#summary-of-checks-for-data-collection",
    "href": "week3/slides.html#summary-of-checks-for-data-collection",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üìã Summary of checks for data collection",
    "text": "üìã Summary of checks for data collection\n‚úÖ Has the collection process been consistent?\n‚úÖ Does the set to be predicted match the training set?\n‚úÖ Is the experimental design correctly applied?\n‚úÖ Have treatments been appropriately randomised or assigned comprehensively across subjects?\n‚úÖ What is the population that the collected data describes?\n‚úÖ If the data is observational, can you group them into comparison sets?"
  },
  {
    "objectID": "week3/slides.html#imputing-missing-values",
    "href": "week3/slides.html#imputing-missing-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Imputing missing values",
    "text": "Imputing missing values"
  },
  {
    "objectID": "week3/slides.html#example-1-olympic-medals",
    "href": "week3/slides.html#example-1-olympic-medals",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 1: Olympic medals",
    "text": "Example 1: Olympic medals\n\n\n\n\n             country totalmedal\n1       UnitedStates        104\n2              China         88\n3             Russia         82\n4       GreatBritain         65\n5            Germany         44\n6              Japan         38\n7          Australia         35\n8             France         34\n9         SouthKorea         28\n10             Italy         28\n11       Netherlands         20\n12           Ukraine         20\n13            Canada         18\n14           Hungary         17\n15             Spain         17\n16            Brazil         17\n17              Cuba         14\n18        Kazakhstan         13\n19        NewZealand         13\n20              Iran         12\n21           Jamaica         12\n22           Belarus         12\n23             Kenya         11\n24     CzechRepublic         10\n25            Poland         10\n26        Azerbaijan         10\n27           Romania          9\n28           Denmark          9\n29            Sweden          8\n30          Colombia          8\n31          Ethiopia          7\n32            Mexico          7\n33           Georgia          7\n34        NorthKorea          6\n35       SouthAfrica          6\n36           Croatia          6\n37             India          6\n38            Turkey          5\n39         Lithuania          5\n40           Ireland          5\n41          Mongolia          5\n42       Switzerland          4\n43            Norway          4\n44          Slovenia          4\n45            Serbia          4\n46         Argentina          4\n47        Uzbekistan          4\n48 TrinidadandTobago          4\n49          Slovakia          4\n50           Tunisia          3\n51          Thailand          3\n52           Finland          3\n53           Belgium          3\n54           Armenia          3\n55 DominicanRepublic          2\n56            Latvia          2\n57             Egypt          2\n58        PuertoRico          2\n59          Malaysia          2\n60         Indonesia          2\n61           Estonia          2\n62            Taiwan          2\n63          Bulgaria          2\n64         Singapore          2\n65             Qatar          2\n66           Moldova          2\n67            Greece          2\n68         Venezuela          1\n69            Uganda          1\n70           Grenada          1\n71           Bahamas          1\n72           Algeria          1\n73          Portugal          1\n74        Montenegro          1\n75         Guatemala          1\n76             Gabon          1\n77            Cyprus          1\n78          Botswana          1\n79        Tajikistan          1\n80       SaudiArabia          1\n81           Morocco          1\n82            Kuwait          1\n83          HongKong          1\n84           Bahrain          1\n85       Afghanistan          1\n\n\n Is the average number of medals equal to 962/85 = 11.32?\n\n\nWhat is missing?\n\n\nWhat is the correct average number of medals?\n\n\n962/204 = 4.72\n Working out what is missing can be hard!"
  },
  {
    "objectID": "week3/slides.html#example-2-el-nino",
    "href": "week3/slides.html#example-2-el-nino",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 2: El Nino",
    "text": "Example 2: El Nino\n\n\nExplore missings\n\nplotting on edge of plots, or\nusing simple imputation like mean\n\n\noceanbuoys |&gt;\n  ggplot(aes(x=air_temp_c, y=humidity)) +\n  geom_miss_point()\n\n\n\n\n\n\n\n\n\nImpute and check\n\nImpute using regression or simulation\nCheck distribution relative to complete cases\n\n\n\n\nCode\nlibrary(simputation)\nocean_imp_yr &lt;- oceanbuoys %&gt;%\n  bind_shadow() %&gt;%\n  impute_lm(air_temp_c ~ wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  impute_lm(humidity ~  wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  impute_lm(sea_temp_c ~  wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  add_label_shadow()\n\nggplot(ocean_imp_yr,\n       aes(x = air_temp_c,\n           y = humidity,\n           color = any_missing)) + \n  geom_point() +\n  theme(legend.title = element_blank())"
  },
  {
    "objectID": "week3/slides.html#validators",
    "href": "week3/slides.html#validators",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Validators",
    "text": "Validators\nAutomating some checks"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-13",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (1/3)",
    "text": "Case study: Dutch supermarket revenue and cost (1/3)\n\nData contains the revenue and cost (in Euros) for 60 supermarkets\nData has been anonymised and distorted\n\n\ndata(\"SBS2000\", package = \"validate\")\ndplyr::glimpse(SBS2000)\n\nRows: 60\nColumns: 11\n$ id          &lt;fct&gt; RET01, RET02, RET03, RET04, ‚Ä¶\n$ size        &lt;fct&gt; sc0, sc3, sc3, sc3, sc3, sc0‚Ä¶\n$ incl.prob   &lt;dbl&gt; 0.02, 0.14, 0.14, 0.14, 0.14‚Ä¶\n$ staff       &lt;int&gt; 75, 9, NA, NA, NA, 1, 5, 3, ‚Ä¶\n$ turnover    &lt;int&gt; NA, 1607, 6886, 3861, NA, 25‚Ä¶\n$ other.rev   &lt;int&gt; NA, NA, -33, 13, 37, NA, NA,‚Ä¶\n$ total.rev   &lt;int&gt; 1130, 1607, 6919, 3874, 5602‚Ä¶\n$ staff.costs &lt;int&gt; NA, 131, 324, 290, 314, NA, ‚Ä¶\n$ total.costs &lt;int&gt; 18915, 1544, 6493, 3600, 553‚Ä¶\n$ profit      &lt;int&gt; 20045, 63, 426, 274, 72, 3, ‚Ä¶\n$ vat         &lt;int&gt; NA, NA, NA, NA, NA, NA, 1346‚Ä¶"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-23",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (2/3)",
    "text": "Case study: Dutch supermarket revenue and cost (2/3)\n\nChecking for completeness of records\n\n\nlibrary(validate)\nrules &lt;- validator(\n          is_complete(id),\n          is_complete(id, turnover),\n          is_complete(id, turnover, profit))\nout &lt;- confront(SBS2000, rules)\nsummary(out)\n\n  name items passes fails nNA error warning\n1   V1    60     60     0   0 FALSE   FALSE\n2   V2    60     56     4   0 FALSE   FALSE\n3   V3    60     52     8   0 FALSE   FALSE\n                         expression\n1                   is_complete(id)\n2         is_complete(id, turnover)\n3 is_complete(id, turnover, profit)"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-33",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (3/3)",
    "text": "Case study: Dutch supermarket revenue and cost (3/3)\n\nSanity check derived variables\n\n\nlibrary(validate)\nrules &lt;- validator(\n    total.rev - profit == total.costs,\n    turnover + other.rev == total.rev,\n    profit &lt;= 0.6 * total.rev\n)\nout &lt;- confront(SBS2000, rules)\nsummary(out)\n\n  name items passes fails nNA error warning\n1   V1    60     39    14   7 FALSE   FALSE\n2   V2    60     19     4  37 FALSE   FALSE\n3   V3    60     49     6   5 FALSE   FALSE\n                                      expression\n1 abs(total.rev - profit - total.costs) &lt;= 1e-08\n2 abs(turnover + other.rev - total.rev) &lt;= 1e-08\n3              profit - 0.6 * total.rev &lt;= 1e-08"
  },
  {
    "objectID": "week3/slides.html#ida-for-hypothesis-testing",
    "href": "week3/slides.html#ida-for-hypothesis-testing",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA for hypothesis testing",
    "text": "IDA for hypothesis testing"
  },
  {
    "objectID": "week3/slides.html#hypothesis-testing-13",
    "href": "week3/slides.html#hypothesis-testing-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hypothesis testing (1/3)",
    "text": "Hypothesis testing (1/3)\n\nState the hypothesis (pair), e.g.¬†\\(H_o: \\mu_1 = \\mu_2\\) vs \\(H_a: \\mu_1 &lt; \\mu_2\\).\nTest statistic depends on assumption about the distribution, e.g.¬†\n\n\\(t\\)-test will assume that distributions are normal, or small departures from if we have a large sample.\ntwo-sample might assume both groups have the same variance\n\n\n\n\nSteps to complete:\n\nCompute the test statistic\nMeasure it against a standard distribution\nIf it is extreme, \\(p\\)-value is small, decision is to reject \\(H_o\\)\n\\(p\\)-value is the probability of observing a value as large as this, or large, assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week3/slides.html#example-1-checking-variance-and-distribution-23",
    "href": "week3/slides.html#example-1-checking-variance-and-distribution-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 1: Checking variance and distribution (2/3)",
    "text": "Example 1: Checking variance and distribution (2/3)\n\n\n\n\n\nCode\ndata(sleep)\nggplot(sleep, aes(x=group, y=extra)) + \n  geom_boxplot() +\n  geom_point(colour=\"#D55E00\")\n\n\n\n\n\n\n\n\n\n\nCushny, A. R. and Peebles, A. R. (1905) The action of optical isomers: II hyoscines. The Journal of Physiology 32, 501‚Äì510.\n\n\nFew observations. Nothing strongly suggests violation of normality and spread of points is similar for each group.\n\ntt &lt;- with(sleep,\n     t.test(extra[group == 1],\n            extra[group == 2], \n            paired = TRUE))\ntt\n\n\n    Paired t-test\n\ndata:  extra[group == 1] and extra[group == 2]\nt = -4, df = 9, p-value = 0.003\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.5 -0.7\nsample estimates:\nmean difference \n           -1.6"
  },
  {
    "objectID": "week3/slides.html#example-2-checking-distribution-and-variance-33",
    "href": "week3/slides.html#example-2-checking-distribution-and-variance-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 2: Checking distribution and variance (3/3)",
    "text": "Example 2: Checking distribution and variance (3/3)\n\n\n\n\n\nCode\nInsectSprays  |&gt; \n  ggplot(aes(x=fct_reorder(spray, count), \n             y=count)) + \n  geom_jitter(width=0.1, height=0, colour=\"#D55E00\", size=3, alpha=0.8) +\n  xlab(\"\") \n\n\n\n\n\n\n\n\n\n\nIs it plausible that the samples are from a normal population? Do they have equal variance?\n\n\n\nfm1 &lt;- aov(count ~ spray, data = InsectSprays)\nsummary(fm1)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nspray        5   2669     534    34.7 &lt;2e-16 ***\nResiduals   66   1015      15                   \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhat hypothesis being tested? What would the decision be?\n\n\nWhy does equal variance matter in this test?"
  },
  {
    "objectID": "week3/slides.html#ida-for-inferential-modeling",
    "href": "week3/slides.html#ida-for-inferential-modeling",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA for inferential modeling",
    "text": "IDA for inferential modeling"
  },
  {
    "objectID": "week3/slides.html#linear-models-13",
    "href": "week3/slides.html#linear-models-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (1/3)",
    "text": "Linear models (1/3)\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nggplot(cars, aes(speed, dist)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + e_i\\]\nAssumptions:\n\nForm is linear\nError is normally distributed around 0\n\n\n\nCheck using residual plots\n\n\n\nCode\ncars_model &lt;- lm(dist ~ speed, data = cars)\ncars_fit &lt;- augment(cars_model)\n\ncars_p1 &lt;- ggplot(cars_fit, aes(x=.fitted, \n                                y=.resid)) + \n  geom_hline(yintercept = 0, colour=\"grey70\") +\n  geom_point() \ncars_p2 &lt;- ggplot(cars_fit, aes(x=.resid)) +\n  geom_density()\ncars_p1 + cars_p2 + plot_layout(ncol=2)"
  },
  {
    "objectID": "week3/slides.html#linear-models-23",
    "href": "week3/slides.html#linear-models-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (2/3)",
    "text": "Linear models (2/3)\n\n\n\nData and loess smoother\n\n\nCode\nggplot(diamonds, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(se=F)\n\n\n\n\n\n\n\n\n\n\nForm is not linear!\nAlso, insufficient data on large diamonds.\n\n\n\nFix 1: fit polynomial form\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + e_i.\\]\n\n\nCode\ndiamonds_sub &lt;- diamonds |&gt;\n  filter(carat &lt; 3)\n\ndiamonds_model &lt;- lm(price ~ poly(carat, 2),\n                     data=diamonds_sub)\ndiamonds_fit &lt;- diamonds_sub |&gt;\n  mutate(.fitted = diamonds_model$fitted.values, \n         .resid = diamonds_model$residuals)\n\ndiamonds_p1 &lt;- ggplot(diamonds_fit) +\n  geom_point(aes(x=carat, y=price)) +\n  geom_point(aes(x=carat, y=.fitted),\n             colour=\"#D55E00\")\ndiamonds_p2 &lt;- ggplot(diamonds_fit, \n                      aes(x=.fitted, \n                          y=.resid)) + \n  geom_hline(yintercept = 0, colour=\"grey70\") +\n  geom_point() \ndiamonds_p1 + diamonds_p2 + plot_layout(ncol=2)\n\n\n\n\n\n\n\n\n\n\nForm is not quadratic, continue to explore additional polynomial terms."
  },
  {
    "objectID": "week3/slides.html#linear-models-33",
    "href": "week3/slides.html#linear-models-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (3/3)",
    "text": "Linear models (3/3)\n\n\n\nData and loess smoother\n\n\nCode\nggplot(diamonds, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(se=F)\n\n\n\n\n\n\n\n\n\n\nForm is not linear!\nAlso, insufficient data on large diamonds.\n\n\n\nFix 2: linearise\n\nThe log transformation of both variables linearises the relationship, so that a simple linear model can be used, and can correct heteroskedasticity.\n\n\nCode\nggplot(diamonds_sub, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(method = lm) +\n  scale_x_log10() +\n  scale_y_log10()"
  },
  {
    "objectID": "week3/slides.html#cautions",
    "href": "week3/slides.html#cautions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Cautions",
    "text": "Cautions\n\nNotice that there was no formal statistical inference when trying to determine an appropriate model form.\n\n\nDiscarded models are hardly ever reported. Consequently, majority of reported statistics give a distorted view and it‚Äôs important to remind yourself what might not be reported."
  },
  {
    "objectID": "week3/slides.html#summary",
    "href": "week3/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\n\n\nIDA is a model-focused exploration to support a CDA with:\n\ndata description and collection\ndata quality checking, and\nchecking assumptions\nmodel fit without any formal statistical inference.\n\nIDA is part of EDA, even when no CDA is planned.\n\nIDA may never see the limelight BUT it forms the foundation that the main analysis is built upon. Document it! Do it well!\n\n\n\n\nThe Census Bureau tabulates same-sex couples in both the American Community Survey (ACS) and the Decennial Census. Two questions are used to identify same-sex couples: relationship and sex. The agency follows edit rules that are used to change data values for seemingly contradictory answers. The edit rules for combining information from relationship and sex have evolved since the category of unmarried partner was added in 1990. In that census, if a household consisted of a married couple and both spouses reported the same sex, the relationship category remained husband or wife, but the sex of the partner who reported being a spouse to the householder was changed. Humans all the way down\n\n\n\n\nHuman actions are ubiquitous in every part of data analysis! The most objective methods often have had subjective actions before and after."
  },
  {
    "objectID": "week3/slides.html#further-reading",
    "href": "week3/slides.html#further-reading",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Further reading",
    "text": "Further reading\n\nHuebner et al (2018) A Contemporary Conceptual Framework for Initial Data Analysis\nHuebner et al (2020) Hidden analyses\nChatfield (1985) The Initial Examination of Data. Journal of the Royal Statistical Society. Series A (General) 148 \nCox & Snell (1981) Applied Statistics. London: Chapman and Hall.\nvan der Loo and de Jonge (2018). Statistical Data Cleaning with Applications in R. John Wiley and Sons Ltd.\nHyndman (2014) Explaining the ABS unemployment fluctuations\n\n\n\n\nETC5521 Lecture 3 | ddde.numbat.space"
  },
  {
    "objectID": "week4/index.html",
    "href": "week4/index.html",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "",
    "text": "Wickham et al.¬†(2010) Graphical inference for Infovis"
  },
  {
    "objectID": "week4/index.html#main-reference",
    "href": "week4/index.html#main-reference",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "",
    "text": "Wickham et al.¬†(2010) Graphical inference for Infovis"
  },
  {
    "objectID": "week4/index.html#what-you-will-learn-this-week",
    "href": "week4/index.html#what-you-will-learn-this-week",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nrevision of statistical inference\nusing re-sampling methods to calibrate reading patterns\ngenerating lineups of plots\nhow to specify the null hypothesis\ncalculating p-value and power"
  },
  {
    "objectID": "week4/index.html#lecture-slides",
    "href": "week4/index.html#lecture-slides",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week4/index.html#tutorial-instructions",
    "href": "week4/index.html#tutorial-instructions",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week4/index.html#assignments",
    "href": "week4/index.html#assignments",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 2 is due on Tuesday 27 August."
  },
  {
    "objectID": "week4/tutorialsol.html",
    "href": "week4/tutorialsol.html",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorialsol.html#objectives",
    "href": "week4/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorialsol.html#preparation",
    "href": "week4/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 4",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is The initial examination of data. It is authored by Chris Chatfield, and is a classic paper explaining the role of initial data analysis. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"palmerpenguins\", \"ggbeeswarm\", \"broom\", \"nullabor\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week4/tutorialsol.html#exercises",
    "href": "week4/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 4",
    "section": "üì• Exercises",
    "text": "üì• Exercises\n\nExercise 1: IDA on penguins data\n\nTake a glimpse of the penguins data. What types are variables are present in the data?\n\n\nlibrary(palmerpenguins)\n\n\nHow was this data collected? You will need to read the documentation for the palmerpenguins package.\nUsing the visdat package make an overview plot to examine types of variables and for missing values.\nCheck the distributions of each species on each of the size variables, using a jittered dotplot, using the geom_quasirandom() function in the ggbeeswarm package. There seems to be some bimodality in some species on some variables eg bill_length_mm. Why do you think this might be? Check your thinking by making a suitable plot.\nIs there any indication of outliers from the jittered dotplots of different variables?\nMake a scatterplot of body_mass_g vs flipper_length_mm for all the penguins. What do the vertical stripes indicate? Are there any other unusual patterns to note, such as outliers or clustering or nonlinearity?\nHow well can penguin body mass be predicted based on the flipper length? Fit a linear model to check. Report the equation, the \\(R^2\\), \\(\\sigma\\), and make a residual plot of residuals vs flipper_length_mm. From the residual plot, are there any concerns about the model fit?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39, 40, 40, NA, 37, 39, 39, 39, 34, 42, 38, 38, 41, ‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 19, 17, 18, NA, 19, 21, 18, 20, 18, 20, 17, 17, 18, ‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186‚Ä¶\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ‚Ä¶\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male‚Ä¶\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007‚Ä¶\n\n\n\nDetails are at https://allisonhorst.github.io/palmerpenguins/articles/intro.html, and you learn ‚ÄúThese data were collected from 2007 - 2009 by Dr.¬†Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. The data were imported directly from the Environmental Data Initiative (EDI) Data Portal.‚Äù It is necessary to also read the original data collection article https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081 to obtain details of the sampling. Breeding pairs of penguins were included based on sampling of nests where pairs of adults were present, were chosen and marked, before onset of egg-laying.\nThere are three factor variables - species, island, sex - and three integer variables - flipper_length_mm, body_mass_g and year - and two numeric variables - bill_length_mm and bill_depth_mm.\n\nIt is interesting that flipper_length_mm and body_mass_g are reported without decimal places, and thus are integers, whereas bill_length_mm and bill_depth_mm are reported with one decimal place, and thus are doubles. Both are numeric variables, though.\nFour variables have missing values: sex, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g. There are more missings on sex. The other missings occur together, the penguins are missing on all five variables.\n\nlibrary(visdat)\nvis_dat(penguins)\n\n\n\n\n\n\n\n\n\nParticularly on bill_length_mm multimodality can be seen in the Chinstrap and Gentoo penguins. This corresponds to differences in the two species. Differences can be seen in the sexes for all of the variables and species, but it is only big enough in bill_length_mm to be noticeable as bimodality.\n\n\nlibrary(ggbeeswarm)\nggplot(penguins, aes(x=species, \n                     y=bill_length_mm)) +\n  geom_quasirandom() \n\n\n\n\n\n\n\nggplot(penguins, aes(x=species, \n                     y=bill_length_mm, \n                     colour=sex)) +\n  geom_quasirandom() +\n  scale_color_brewer(\"\", palette=\"Dark2\")\n\n\n\n\n\n\n\n\n\nOutliers can be seen on bill_length_mm for Chinstrap and Gentoo. Interestingly, there is one female penguins with a really big bill, bigger than all the males even.\n\nAlso on flipper_length_mm there is one female penguins with a much smaller value than others.\n\nThe striping corresponds to rounded values of flipper length, that have been reported to the nearest mm. It appears that it is done across all measuring as it is present for both sexes, for all species, for each island and each year. Otherwise there is nothing much to report as unusual.\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_length_mm,\n             y=body_mass_g)) +\n  geom_point() +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_length_mm,\n             y=body_mass_g)) +\n  geom_point() +\n  facet_wrap(~island, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_length_mm,\n             y=body_mass_g)) +\n  geom_point() +\n  facet_wrap(~sex, ncol=2, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_length_mm,\n             y=body_mass_g)) +\n  geom_point() +\n  facet_wrap(~species, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_length_mm,\n             y=body_mass_g)) +\n  geom_point() +\n  facet_wrap(~year, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nThe model fit statistics suggest it is a reasonably good model, with flipper length explaining about 76% of body mass. From the estimated standard deviation of the error, \\(\\sigma=393\\), we could say that the estimated body mass is likely accurate to within 800g. (Assuming normal distribution and 95% of observations within two \\(\\sigma\\).)\n\nThe residual suggests no major problems. There is a little heteroskedasticity. Perhaps if you look carefully, though, it might indicate that different models should have been fitted for the smaller penguins and the bigger penguins, perhaps models separately for sex and species would be advised.\n\nlibrary(broom)\npenguins_nona &lt;- penguins %&gt;%\n  na.omit()\npenguins_fit &lt;- lm(body_mass_g~flipper_length_mm, data=penguins_nona)\ntidy(penguins_fit)\n\n# A tibble: 2 √ó 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -5872.     310.       -18.9 1.18e- 54\n2 flipper_length_mm     50.2      1.54      32.6 3.13e-105\n\nglance(penguins_fit)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.762         0.761  393.     1060. 3.13e-105     1 -2461. 4928. 4940.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\npenguins_m &lt;- augment(penguins_fit)\nggplot(penguins_m, aes(x=flipper_length_mm, y=.resid)) +\n  geom_hline(yintercept=1, colour=\"grey70\") +\n  geom_point() +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Can we believe what we see?\nThis question uses material from this week‚Äôs lecture, from a few hours ago.\n\nIn the previous question we made subjective statements about the residual plot to determine if the model was a good fit or not. We‚Äôll use randomisation to check any observations we made from the residual plot. The code below makes a lineup of the true plot against plots made with rotation residuals (nulls/good). When you run the code you will get a line decrypt(\"....\"), which you can copy and paste back in to the console window to get the location of the true plot (in case you forgot which it is). Does the true plot look like the null plots? If not, describe how it differs.\n\n\nlibrary(nullabor)\nggplot(lineup(null_lm(body_mass_g~flipper_length_mm, method=\"rotate\"),\n              penguins_m),\n       aes(x=flipper_length_mm, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n\nPick one group, males or females, and one of Adelie, Chinstrap or Gentoo, and choose two of the four measurements. Fit a linear model, and do a lineup of the residuals. Can you tell which is the true plot? Show your lineup to your tutorial partner or someone else nearby and ask them\n\n\nto pick the plot that is most different.\nexplain why they picked that plot.\n\nUsing your decrypt() code locate the true plot. Is the true plot different from the nulls?\nDid you or your friend choose the data plot? Was it identifiable from the lineup or indistinguishable from the null plots?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe true plot looks a little different from the nulls. It has more of a V shape, which might suggest that the model fits poorly, that the smaller penguins have a different relationship than the larger penguins. It is evidence to support fitting separate models to the sexes and species.\nSomething like the following\n\n\nlibrary(nullabor)\npenguins_f_adelie &lt;- penguins_nona %&gt;%\n  filter(species == \"Adelie\",\n         sex == \"female\")\npenguins_f_adelie_bl_bd_fit &lt;- \n  lm(bill_depth_mm ~ bill_length_mm,\n     data=penguins_f_adelie)\npenguins_f_adelie_bl_bd_m &lt;-\n  augment(penguins_f_adelie_bl_bd_fit)\nggplot(lineup(null_lm(bill_depth_mm ~ \n                        bill_length_mm,\n                      method=\"rotate\"),\n              penguins_f_adelie_bl_bd_m),\n       aes(x=bill_length_mm, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n\n\n\n\n\n\n\nI would not be able to distinguish which is the true plot in this lineup."
  },
  {
    "objectID": "week4/tutorialsol.html#finishing-up",
    "href": "week4/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 4",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week4/tutorial.html",
    "href": "week4/tutorial.html",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorial.html#objectives",
    "href": "week4/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorial.html#preparation",
    "href": "week4/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 4",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is The initial examination of data. It is authored by Chris Chatfield, and is a classic paper explaining the role of initial data analysis. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"palmerpenguins\", \"ggbeeswarm\", \"broom\", \"nullabor\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week4/tutorial.html#exercises",
    "href": "week4/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 4",
    "section": "üì• Exercises",
    "text": "üì• Exercises\n\nExercise 1: IDA on penguins data\n\nTake a glimpse of the penguins data. What types are variables are present in the data?\n\n\nlibrary(palmerpenguins)\n\n\nHow was this data collected? You will need to read the documentation for the palmerpenguins package.\nUsing the visdat package make an overview plot to examine types of variables and for missing values.\nCheck the distributions of each species on each of the size variables, using a jittered dotplot, using the geom_quasirandom() function in the ggbeeswarm package. There seems to be some bimodality in some species on some variables eg bill_length_mm. Why do you think this might be? Check your thinking by making a suitable plot.\nIs there any indication of outliers from the jittered dotplots of different variables?\nMake a scatterplot of body_mass_g vs flipper_length_mm for all the penguins. What do the vertical stripes indicate? Are there any other unusual patterns to note, such as outliers or clustering or nonlinearity?\nHow well can penguin body mass be predicted based on the flipper length? Fit a linear model to check. Report the equation, the \\(R^2\\), \\(\\sigma\\), and make a residual plot of residuals vs flipper_length_mm. From the residual plot, are there any concerns about the model fit?\n\n\n\nExercise 2: Can we believe what we see?\nThis question uses material from this week‚Äôs lecture, from a few hours ago.\n\nIn the previous question we made subjective statements about the residual plot to determine if the model was a good fit or not. We‚Äôll use randomisation to check any observations we made from the residual plot. The code below makes a lineup of the true plot against plots made with rotation residuals (nulls/good). When you run the code you will get a line decrypt(\"....\"), which you can copy and paste back in to the console window to get the location of the true plot (in case you forgot which it is). Does the true plot look like the null plots? If not, describe how it differs.\n\n\nlibrary(nullabor)\nggplot(lineup(null_lm(body_mass_g~flipper_length_mm, method=\"rotate\"),\n              penguins_m),\n       aes(x=flipper_length_mm, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n\nPick one group, males or females, and one of Adelie, Chinstrap or Gentoo, and choose two of the four measurements. Fit a linear model, and do a lineup of the residuals. Can you tell which is the true plot? Show your lineup to your tutorial partner or someone else nearby and ask them\n\n\nto pick the plot that is most different.\nexplain why they picked that plot.\n\nUsing your decrypt() code locate the true plot. Is the true plot different from the nulls?\nDid you or your friend choose the data plot? Was it identifiable from the lineup or indistinguishable from the null plots?"
  },
  {
    "objectID": "week4/tutorial.html#finishing-up",
    "href": "week4/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 4",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week5/slides.html#quantitative-variables",
    "href": "week5/slides.html#quantitative-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Quantitative variables",
    "text": "Quantitative variables"
  },
  {
    "objectID": "week5/slides.html#features-of-a-single-quantitative-variable",
    "href": "week5/slides.html#features-of-a-single-quantitative-variable",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Features of a single quantitative variable",
    "text": "Features of a single quantitative variable\n\n\n\n\n\nFeature\nExample\nDescription\n\n\n\n\nAsymmetry\n\nThe distribution is not symmetrical.\n\n\nOutliers\n\nSome observations are that are far from the rest.\n\n\nMultimodality\n\nThere are more than one \"peak\" in the observations.\n\n\nGaps\n\nSome continuous interval that are contained within the range but no observations exists.\n\n\nHeaping\n\nSome values occur unexpectedly often.\n\n\nDiscretized\n\nOnly certain values are found, e.g. due to rounding.\n\n\nImplausible\n\nValues outside of plausible or likely range."
  },
  {
    "objectID": "week5/slides.html#numerical-features-of-a-single-quantitative-variables",
    "href": "week5/slides.html#numerical-features-of-a-single-quantitative-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Numerical features of a single quantitative variables",
    "text": "Numerical features of a single quantitative variables\n\n\nA measure of central tendency, e.g.¬†mean, median and mode\nA measure of dispersion (also called variability or spread), e.g.¬†variance, standard deviation and interquartile range\nThere are other measures, e.g.¬†skewness and kurtosis that measures ‚Äútailedness‚Äù, but these are not as common as the measures of first two\nThe mean is also the first moment and variance, skewness and kurtosis are second, third, and fourth central moments\n\nSignificance tests or hypothesis tests\n\nTesting for \\(H_0: \\mu = \\mu_0\\) vs.¬†\\(H_1: \\mu \\neq \\mu_0\\) (often \\(\\mu_0 = 0\\))\nThe \\(t\\)-test is commonly used if the underlying data are believed to be normally distributed"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-18",
    "href": "week5/slides.html#australian-federal-election-18",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (1/8)",
    "text": "2019 Australian Federal Election (1/8)\n\n\nContext\n\nThere are 151 seats in the House of Representative for the 2019 Australian federal election\nThe major parties in Australia are:\n\nthe Coalition, comprising of the:\n\nLiberal,\nLiberal National (Qld),\nNational, and\nCountry Liberal (NT) parties, and\n\nthe Australian Labor party\n\nThe Greens party is a small but notable party\n\n\n\nSource: PRObono"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-28",
    "href": "week5/slides.html#australian-federal-election-28",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (2/8)",
    "text": "2019 Australian Federal Election (2/8)\n\n\n\n\n\n\n\n\nData source: Australian Electoral Commission. (2019)"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-38",
    "href": "week5/slides.html#australian-federal-election-38",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (3/8)",
    "text": "2019 Australian Federal Election (3/8)\nWhat is the number of the seats won in the House of Representatives by parties?\n\nüìädataR\n\n\n\n\n\n\n\n\n\nParty\n# of seats\n\n\n\n\nCoalition\n77\n\n\nLiberal\n44\n\n\nLiberal National Party Of Queensland\n23\n\n\nThe Nationals\n10\n\n\nAustralian Labor Party\n68\n\n\nThe Greens\n1\n\n\nCentre Alliance\n1\n\n\nKatter's Australian Party (Kap)\n1\n\n\nIndependent\n3\n\n\n\n\n\n\n\n\nWhat does this table tell you?\n\n\nThe Coalition won the government\nLabor and Coalition hold majority of the seats in the House of Representatives (lower house)\nParties such as The Greens, Centre Alliance and Katter‚Äôs Australian Party (KAP) won only a single seat\n\nOnly?\n\n\nWait‚Ä¶ Did the parties compete in all electoral districts?\n\n\n\n\n\n\nskimr::skim(df1)\n\n‚îÄ‚îÄ Data Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                           Values\nName                       df1   \nNumber of rows             1207  \nNumber of columns          18    \n_______________________          \nColumn type frequency:           \n  character                11    \n  numeric                  7     \n________________________         \nGroup variables            None  \n\n‚îÄ‚îÄ Variable type: character ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   skim_variable   n_missing complete_rate min max empty\n 1 StateAb                 0         1       2   3     0\n 2 DivisionID              0         1       3   3     0\n 3 DivisionNm              0         1       4  15     0\n 4 CandidateID             0         1       3   5     0\n 5 Surname                 0         1       2  18     0\n 6 GivenNm                 0         1       1  25     0\n 7 BallotPosition          0         1       1   3     0\n 8 Elected                 0         1       1   1     0\n 9 HistoricElected         0         1       1   1     0\n10 PartyAb               151         0.875   2   4     0\n11 PartyNm                 2         0.998   5  61     0\n   n_unique whitespace\n 1        8          0\n 2      151          0\n 3      151          0\n 4     1057          0\n 5      890          0\n 6      613          0\n 7       14          0\n 8        2          0\n 9        2          0\n10       40          0\n11       45          0\n\n‚îÄ‚îÄ Variable type: numeric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  skim_variable    n_missing complete_rate     mean       sd\n1 OrdinaryVotes            0             1 10401.   12446.  \n2 AbsentVotes              0             1   511.     569.  \n3 ProvisionalVotes         0             1    41.4     51.7 \n4 PrePollVotes             0             1   514.     607.  \n5 PostalVotes              0             1  1033.    1476.  \n6 TotalVotes               0             1 12501.   14860.  \n7 Swing                    0             1     1.07     4.26\n     p0     p25     p50      p75    p100 hist \n1 167   1867    4317    14768.   54535   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n2  13    117     246      711     3287   ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n3   0      8      20       56      444   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n4  11    108.    211      761     5248   ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n5  14    181     317     1216.    9837   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n6 250   2348    5196    18142    61202   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n7 -28.1   -0.73    1.21     2.75    43.5 ‚ñÅ‚ñÜ‚ñá‚ñÅ‚ñÅ\n\n\n\n\n\ndf1 &lt;- read_csv(here::here(\"data/HouseFirstPrefsByCandidateByVoteTypeDownload-24310.csv\"),\n  skip = 1,\n  col_types = cols(\n    .default = col_character(),\n    OrdinaryVotes = col_double(),\n    AbsentVotes = col_double(),\n    ProvisionalVotes = col_double(),\n    PrePollVotes = col_double(),\n    PostalVotes = col_double(),\n    TotalVotes = col_double(),\n    Swing = col_double()\n  )\n)\n\n\nrecode_party_names &lt;- c(\n  \"Australian Labor Party (Northern Territory) Branch\" = \"Australian Labor Party\",\n  \"Labor\" = \"Australian Labor Party\",\n  \"The Greens (Vic)\" = \"The Greens\",\n  \"The Greens (Wa)\" = \"The Greens\",\n  \"Katter's Australian Party (KAP)\" = \"Katter's Australian Party\",\n  \"Country Liberals (Nt)\" = \"Country Liberals (NT)\"\n)\n\n\ntdf1 &lt;- df1 |&gt;\n  filter(Elected == \"Y\") |&gt;\n  mutate(\n    PartyNm = str_to_title(PartyNm),\n    PartyNm = recode(PartyNm, !!!recode_party_names)\n  ) |&gt;\n  count(PartyNm, sort = TRUE) |&gt;\n  slice(2:4, 1, 8, 6, 7, 5)\n\n\ndata.frame(PartyNm = \"Coalition\", n = sum(tdf1$n[1:3])) |&gt;\n  rbind(tdf1) |&gt;\n  knitr::kable(col.names = c(\"Party\", \"# of seats\")) |&gt;\n  kableExtra::kable_classic() |&gt;\n  kableExtra::kable_styling(    \n    full_width = FALSE,\n    font_size = 24\n  ) |&gt;\n  kableExtra::add_indent(2:4) |&gt;\n  kableExtra::row_spec(2:4, color = \"#C8C8C8\")"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-48",
    "href": "week5/slides.html#australian-federal-election-48",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (4/8)",
    "text": "2019 Australian Federal Election (4/8)\n\nüìädataR\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you notice from this table?\n\n\n\nThe Greens are represented in every electoral districts\nUnited Australia Party is the only other non-major party to be represented in every electoral district\nKAP is represented in 7 electoral districts\nCentre Alliance is only represented in 3 electoral districts!\n\nLet‚Äôs have a closer look at the Greens party‚Ä¶\n\n\n\n\n\n\n\ntdf2 &lt;- df1 |&gt;\n  mutate(\n    PartyNm = str_to_title(PartyNm),\n    PartyNm = recode(PartyNm, !!!recode_party_names)\n  ) |&gt;\n  count(PartyNm, sort = TRUE)\n\n\n\nYou can omit table_options and toggle_select or have a look at the source Rmd to find out what it is\n\ntdf2 |&gt;\n  DT::datatable(\n    rownames = FALSE,\n    escape = FALSE,\n    width = \"900px\",\n    options = table_options(\n      scrollY = \"400px\",\n      title = \"Australian Federal Election 2019 - Party Distribution\",\n      csv = \"aus-election-2019-party-dist\"\n    ),\n    elementId = \"tab1B\",\n    colnames = c(\"Party\", \"# of electorates\"),\n    callback = toggle_select\n  )"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-58",
    "href": "week5/slides.html#australian-federal-election-58",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (5/8)",
    "text": "2019 Australian Federal Election (5/8)\n\nüìädataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does this graph tell you?\n\n\n\nMajority of the country does not have first preference for the Greens\nSome constituents are slightly more supportive than the others\n\nWhat further questions does it raise?\nNotes:\n\nAustralia uses full-preference instant-runoff voting in single member seats\nFollowing the full allocation of preferences, it is possible to derive a two-party-preferred figure, where the votes have been allocated between the two main candidates in the election.\nIn Australia, this is usually between the candidates from the Coalition parties and the Australian Labor Party.\n\n\n\n\n\n\n\n\nskimr::skim(tdf3)\n\n‚îÄ‚îÄ Data Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                           Values\nName                       tdf3  \nNumber of rows             151   \nNumber of columns          6     \n_______________________          \nColumn type frequency:           \n  character                3     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n‚îÄ‚îÄ Variable type: character ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  skim_variable n_missing complete_rate min max empty\n1 DivisionID            0             1   3   3     0\n2 DivisionNm            0             1   4  15     0\n3 State                 0             1   2   3     0\n  n_unique whitespace\n1      151          0\n2      151          0\n3        8          0\n\n‚îÄ‚îÄ Variable type: numeric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  skim_variable n_missing complete_rate     mean      sd\n1 votes_GRN             0             1  9821.   5581.  \n2 votes_total           0             1 99925.   9801.  \n3 perc_GRN              0             1     9.87    5.63\n        p0      p25       p50      p75     p100 hist \n1  2744     6555      8676     11532.   45876   ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n2 51009    96372.   100936    105588   116216   ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÖ\n3     2.89     6.43      8.55     11.4     47.8 ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\ntdf3 &lt;- df1 |&gt;\n  group_by(DivisionID) |&gt;\n  summarise(\n    DivisionNm = unique(DivisionNm),\n    State = unique(StateAb),\n    votes_GRN = TotalVotes[which(PartyAb == \"GRN\")],\n    votes_total = sum(TotalVotes)\n  ) |&gt;\n  mutate(perc_GRN = votes_GRN / votes_total * 100)\n\n\ntdf3 |&gt;\n  ggplot(aes(perc_GRN)) +\n  geom_histogram(color = \"white\", fill = \"#00843D\") +\n  labs(\n    x = \"First preference votes %\",\n    y = \"Count\",\n    title = \"Greens party\"\n  )"
  },
  {
    "objectID": "week5/slides.html#formulating-questions-for-eda-vs-making-observations-from-a-plot",
    "href": "week5/slides.html#formulating-questions-for-eda-vs-making-observations-from-a-plot",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Formulating questions for EDA vs making observations from a plot",
    "text": "Formulating questions for EDA vs making observations from a plot\n\n\n\nBEFORE plotting or making summaries think broad (open-ended) questions about the distribution of values\nQuestions with simple answers (i.e.¬†yes or no) less helpful in encouraging exploration using graphics\nFor example,\n\nWhat is the distribution of the first preference vote percentages for the Labor party across Australia?\nIs it evenly spread across electorates or are there clusters of popularity?\n\n\n\n\n\nAFTER plotting or making summaries think  was this what you expected, are there any surprises. Detail what you learn, and how you should follow up on these observations.\n\n\n\n\n\n\n\n\n\n\nIs the outlying observation the electoral district that won the seat?"
  },
  {
    "objectID": "week5/slides.html#visual-inference",
    "href": "week5/slides.html#visual-inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference",
    "text": "Visual inference\n\n\nSuitable null models for a single variable all focus on potential distributions.\nTypical plot description:\n\nggplot(data, aes(x=var1)) +\n  geom_histogram()\n\n Is the distribution consistent with a sample from a\n\nnormal distribution?\nuniform distribution?\nskewed distribution?\nMANY OTHER POTENTIAL DISTRIBUTIONS\n\n\nPotential simulation methods from specific distributions\n\n# Symmetric, unimodal, bell-shaped\nnull_dist(\"var1\", \"norm\")\nnull_dist(\"var1\", \"cauchy\")\nnull_dist(\"var1\", \"t\")\n\n# Skewed right\nnull_dist(\"var1\", \"exp\")\nnull_dist(\"var1\", \"chisq\")\nnull_dist(\"var1\", \"gamma\")\n\n# Constant \nnull_dist(\"var1\", \"uniform\")"
  },
  {
    "objectID": "week5/slides.html#lineup-of-greens-first-preference-percentages",
    "href": "week5/slides.html#lineup-of-greens-first-preference-percentages",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Lineup of Greens first preference percentages",
    "text": "Lineup of Greens first preference percentages\n\nüìäExplanationR\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the exponential distribution as the null says that we expect most electorates to have small tallies for Greens, and only a few electorates will have large, potentially winnable tallies.\nNOTE: We‚Äôve already seen the data so we can‚Äôt be impartial judges for choosing the most different plot. We can use the null plots to check whether the small mode of moderately high tallies is unusual if the tallies really are samples from an exponential distribution.\n\n\n\nlibrary(nullabor)\nset.seed(241)\nggplot(lineup(null_dist(\"perc_GRN\", \"exp\"), tdf3, n=10),\n       aes(x=perc_GRN)) +\n  geom_histogram(color = \"white\", fill = \"#00843D\", bins = 30) +\n  facet_wrap(~.sample, ncol=5, scales=\"free\") +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.grid.major = element_blank())"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-68",
    "href": "week5/slides.html#australian-federal-election-68",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (6/8)",
    "text": "2019 Australian Federal Election (6/8)\n\nüìädataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% of first preference for the Greens\n\n\n\n\nState\nMean\nMedian\nSD\nIQR\nSkewness\nKurtosis\n\n\n\n\nACT\n16.4\n14.0\n5.6\n5.20\n0.65\n1.5\n\n\nVIC\n11.4\n8.6\n8.2\n6.72\n2.60\n11.4\n\n\nWA\n11.0\n10.8\n3.0\n3.12\n0.80\n3.0\n\n\nQLD\n9.8\n8.8\n5.1\n4.75\n1.09\n3.9\n\n\nTAS\n9.7\n9.3\n4.0\n0.98\n0.33\n2.5\n\n\nNT\n9.6\n9.6\n2.5\n1.75\n0.00\n1.0\n\n\nSA\n9.1\n8.9\n3.0\n3.41\n0.38\n2.9\n\n\nNSW\n8.1\n6.6\n4.1\n3.95\n1.50\n4.9\n\n\nNational\n9.9\n8.5\n5.6\n5.00\n2.67\n15.8\n\n\n\n\n\n\n\n\n\nWhy are the means and the medians different?\nHow are the standard deviations and the interquartile ranges similar or different?\nAre there some other numerical statistics we should show?\n\n\n\n\n\n\ntdf3 &lt;- df1 |&gt;\n  group_by(DivisionID) |&gt;\n  summarise(\n    DivisionNm = unique(DivisionNm),\n    State = unique(StateAb),\n    votes_GRN = TotalVotes[which(PartyAb == \"GRN\")],\n    votes_total = sum(TotalVotes)\n  ) |&gt;\n  mutate(perc_GRN = votes_GRN / votes_total * 100)\n\n\n\n\ntdf3 |&gt;\n  group_by(State) |&gt;\n  summarise(\n    mean = mean(perc_GRN),\n    median = median(perc_GRN),\n    sd = sd(perc_GRN),\n    iqr = IQR(perc_GRN),\n    skewness = moments::skewness(perc_GRN),\n    kurtosis = moments::kurtosis(perc_GRN)\n  ) |&gt;\n  arrange(desc(mean)) |&gt;\n  rbind(data.frame(\n    State = \"National\",\n    mean = mean(tdf3$perc_GRN),\n    median = median(tdf3$perc_GRN),\n    sd = sd(tdf3$perc_GRN),\n    iqr = IQR(tdf3$perc_GRN),\n    skewness = moments::skewness(tdf3$perc_GRN),\n    kurtosis = moments::kurtosis(tdf3$perc_GRN)\n  )) |&gt;\n  knitr::kable(col.names = c(\"State\", \"Mean\", \"Median\", \"SD\", \"IQR\", \"Skewness\", \"Kurtosis\"), digits = 3) |&gt;\n  kableExtra::kable_classic() |&gt;\n  kableExtra::add_header_above(c(\" \", \"% of first preference for the Greens\" = 4, \" \" = 2)) |&gt;\n  kableExtra::row_spec(9, extra_css = \"border-top: 2px solid black;\")"
  },
  {
    "objectID": "week5/slides.html#robust-measure-of-central-tendency",
    "href": "week5/slides.html#robust-measure-of-central-tendency",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Robust measure of central tendency",
    "text": "Robust measure of central tendency\n\n\n\nMean is a non-robust measure of location.\nMedian is the 50% quantile of the observations\nTrimmed mean is the sample mean after discarding observations at the tails.\nWinsorized mean is the sample mean after replacing observations at the tails with the minimum or maximum of the observations that remain.\n\nBoth trimmed and Winsorized mean trimmed 20% of the tails.\n\n\n     \n\n\n\n\n\n\nPlot\nMean\nMedian\nTrimmed Mean\nWinsorized Mean\n\n\n\n\n1\n0.109\n0.114\n0.120\n0.103\n\n\n2\n0.054\n-0.045\n-0.016\n-0.029\n\n\n3\n1.177\n0.729\n0.820\n0.888\n\n\n4\n0.533\n0.541\n0.543\n0.542\n\n\n5\n0.468\n0.329\n0.355\n0.390\n\n\n6\n5.626\n6.656\n5.918\n5.688"
  },
  {
    "objectID": "week5/slides.html#robust-measure-of-dispersion",
    "href": "week5/slides.html#robust-measure-of-dispersion",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Robust measure of dispersion",
    "text": "Robust measure of dispersion\n\n\n\nStandard deviation or its square, variance, is a popular choice of measure of dispersion but is not robust to outliers\nStandard deviation for sample \\(x_1, ..., x_n\\) is\n\n\\[\\sqrt{\\sum_{i=1}^n \\frac{(x_i - \\bar{x})^2}{n - 1}}\\]\n\nInterquartile range difference between 1st and 3rd quartile, more robust measure of spread\nMedian absolute deviance (MAD) is even more robust\n\n\\[\\text{median}(|x_i - \\text{median}(x_i)|)\\]\n\n\n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure of dispersion\n\n\n\n\nPlot\nSD\nIQR\nMAD\nSkewness\nKurtosis\n\n\n\n\n1\n0.90\n1.19\n0.87\n-0.072\n3.0\n\n\n2\n0.99\n1.41\n1.08\n0.358\n2.2\n\n\n3\n1.33\n1.18\n0.79\n1.944\n7.2\n\n\n4\n0.29\n0.45\n0.34\n-0.126\n1.8\n\n\n5\n0.47\n0.50\n0.34\n1.691\n6.4\n\n\n6\n2.78\n5.36\n2.98\n-0.351\n1.7"
  },
  {
    "objectID": "week5/slides.html#inference-for-robust-statistics",
    "href": "week5/slides.html#inference-for-robust-statistics",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Inference for robust statistics",
    "text": "Inference for robust statistics\n\n\nWe have seen the re-sampling methods simulation and permutation used for generating null plots in a lineup. Re-sampling methods can be used with numeric statistics also.\nSimulation from distribution, can be used to to check for outliers.\n\n\n\n\n\n\n\n\n\nWe can also compute how many simulated values are more than the observed which gives a simulation \\(p\\)-value: 0.61.\n\n\nFor sample means, conventional tests provide a means for assessing what might be observed if different samples were taken.\nBootstrapping the current sample, can be used for robust statistics. If we have a sample of values:\n\n\n[1] 2 2 3 6 7 7 8 8\n\n\nto bootstrap sample with replacement:\n\nsort(sample(x, replace=TRUE))\n\n[1] 2 2 3 3 7 7 7 7\n\nsort(sample(x, replace=TRUE))\n\n[1] 2 3 6 6 6 6 8 8\n\n\n\nHere‚Äôs an example of bootstrapping to get a confidence interval for a median.\n\n\n[1] \"Median: 6.34\"\n\n\n[1] \"95% CI: ( 4.99 , 9.16 )\""
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-78",
    "href": "week5/slides.html#australian-federal-election-78",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (7/8)",
    "text": "2019 Australian Federal Election (7/8)\n\nüìädataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere are these electorates?\nThe width of the boxplot is proportional to the number of electoral districts in the corresponding state (which is roughly proportional to the population).\n\n\n\n\n\ntdf3 &lt;- df1 |&gt;\n  group_by(DivisionID) |&gt;\n  summarise(\n    DivisionNm = unique(DivisionNm),\n    State = unique(StateAb),\n    votes_GRN = TotalVotes[which(PartyAb == \"GRN\")],\n    votes_total = sum(TotalVotes)\n  ) |&gt;\n  mutate(perc_GRN = votes_GRN / votes_total * 100)\n\n\n\n\ntdf3 |&gt;\n  mutate(State = fct_reorder(State, perc_GRN)) |&gt;\n  ggplot(aes(perc_GRN, State)) +\n  geom_boxplot(varwidth = TRUE) +\n  labs(\n    x = \"First preference votes %\",\n    y = \"Count\",\n    title = \"Greens party\"\n  )"
  },
  {
    "objectID": "week5/slides.html#outliers",
    "href": "week5/slides.html#outliers",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Outliers",
    "text": "Outliers\n\nOutliers are observations that are significantly different from the majority.\n\n\n\n\n\nOutliers can occur by chance in almost all distributions, but could be indicative of:\n\na measurement error,\na different population, or\nan issue with the sampling process."
  },
  {
    "objectID": "week5/slides.html#closer-look-at-the-boxplot",
    "href": "week5/slides.html#closer-look-at-the-boxplot",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Closer look at the boxplot",
    "text": "Closer look at the boxplot\n\n\nObservations that are outside the range of lower to upper fence (1.5 times the box length) are often referred to as outliers.\nPlotting boxplots for data from a skewed distribution will almost always show these ‚Äúoutliers‚Äù but these are not necessarily outliers.\nSome definitions of outliers assume a symmetrical population distribution (e.g.¬†in boxplots or observations a certain standard deviations away from the mean) and these definitions are ill-suited for asymmetrical distributions.\nDeclaring observations outliers typically requires additional data context.\n\n\nWhat cannot be seen from boxplots?"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-88",
    "href": "week5/slides.html#australian-federal-election-88",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (8/8)",
    "text": "2019 Australian Federal Election (8/8)\n\nüìädataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow what do you notice from this graph that you didn‚Äôt notice before?\n\n\nOnly two electoral districts in NT.\nAnd only 3 and 5 electoral districts in ACT and TAS, respectively!\nBoxplots requires 5 points!\nWe should have summarised the number of electoral districts for each state with numerical statistics as a first step.\nAlso the outlier (yes, safe to call this an outlier!) and the cluster in the Victoria electorates.\n\n\n\n\n\n\n\ntdf3 &lt;- df1 |&gt;\n  group_by(DivisionID) |&gt;\n  summarise(\n    DivisionNm = unique(DivisionNm),\n    State = unique(StateAb),\n    votes_GRN = TotalVotes[which(PartyAb == \"GRN\")],\n    votes_total = sum(TotalVotes)\n  ) |&gt;\n  mutate(perc_GRN = votes_GRN / votes_total * 100)\n\n\n\n\ntdf3 |&gt;\n  mutate(State = fct_reorder(State, perc_GRN)) |&gt;\n  ggplot(aes(perc_GRN, State)) +\n  ggbeeswarm::geom_quasirandom(groupOnX = FALSE, varwidth = TRUE) +\n  labs(\n    x = \"First preference votes %\",\n    y = \"State\",\n    title = \"Greens party\"\n  )"
  },
  {
    "objectID": "week5/slides.html#both-numerical-and-graphical-summaries-can-reveal-andor-hide-aspects-of-the-data",
    "href": "week5/slides.html#both-numerical-and-graphical-summaries-can-reveal-andor-hide-aspects-of-the-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Both numerical and graphical summaries can reveal and/or hide aspects of the data",
    "text": "Both numerical and graphical summaries can reveal and/or hide aspects of the data"
  },
  {
    "objectID": "week5/slides.html#transformations",
    "href": "week5/slides.html#transformations",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Transformations",
    "text": "Transformations"
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-16",
    "href": "week5/slides.html#melbourne-housing-prices-16",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (1/6)",
    "text": "Melbourne Housing Prices (1/6)\n\n\n\n\n\n\n\nSuburb\nRooms\nType\nPrice ($)\nDate\n\n\n\n\nAbbotsford\n3\nHome\n1,490,000\n2017-04-01\n\n\nAbbotsford\n3\nHome\n1,220,000\n2017-04-01\n\n\nAbbotsford\n3\nHome\n1,420,000\n2017-04-01\n\n\nAberfeldie\n3\nHome\n1,515,000\n2017-04-01\n\n\nAirport West\n2\nHome\n670,000\n2017-04-01\n\n\nAirport West\n2\nTownhouse\n530,000\n2017-04-01\n\n\nAirport West\n2\nUnit\n540,000\n2017-04-01\n\n\nAirport West\n3\nHome\n715,000\n2017-04-01\n\n\nAlbanvale\n6\nHome\nNA\n2017-04-01\n\n\nAlbert Park\n3\nHome\n1,925,000\n2017-04-01\n\n\nAlbion\n3\nUnit\n515,000\n2017-04-01\n\n\nAlbion\n4\nHome\n717,000\n2017-04-01\n\n\nAlphington\n2\nHome\n1,675,000\n2017-04-01\n\n\nAlphington\n4\nHome\n2,008,000\n2017-04-01\n\n\nAltona\n2\nHome\n860,000\n2017-04-01\n\n\nAltona Meadows\n4\nHome\nNA\n2017-04-01\n\n\nAltona North\n3\nHome\n720,000\n2017-04-01\n\n\nArmadale\n2\nUnit\n836,000\n2017-04-01\n\n\nArmadale\n2\nHome\n2,110,000\n2017-04-01\n\n\nArmadale\n3\nHome\n1,386,000\n2017-04-01\n\n\n\n\n\n\n\n\n\nThis data was scraped each week from domain.com.au from 2016-01-28 to 2018-10-13\nIn total there are 63,023 observations\nAll variables shown (there are more variables not shown here), except price, have complete records\nThe are 48,433 property prices across Melbourne (roughly 23% missing)\n\nData source: Tony Pio (2018) Melbourne Housing Market\nHow would you explore this data first?\n\nYes, with an overview plot."
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-26",
    "href": "week5/slides.html#melbourne-housing-prices-26",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (2/6)",
    "text": "Melbourne Housing Prices (2/6)\n\nüìädataRlineupR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs missingness more likely for expensive houses?\n\n\n\n\n\n\n\n\n\n\nCheck with a lineup\nTo impute missings other variables will need to be used.\n\nNote: Houses with more than 8 rooms removed. Why?\n\n\n\n\n\ndf2 &lt;- read_csv(here::here(\"data/MELBOURNE_HOUSE_PRICES_LESS.csv\"),\n  col_types = cols(\n    .default = col_character(),\n    Rooms = col_double(),\n    Price = col_double(),\n    Date = col_date(format = \"%d/%m/%Y\"),\n    Propertycount = col_double(),\n    Distance = col_double()\n  )\n)\n\n\nskimr::skim(df2)\n\n‚îÄ‚îÄ Data Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                           Values\nName                       df2   \nNumber of rows             63023 \nNumber of columns          13    \n_______________________          \nColumn type frequency:           \n  character                8     \n  Date                     1     \n  numeric                  4     \n________________________         \nGroup variables            None  \n\n‚îÄ‚îÄ Variable type: character ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  skim_variable n_missing complete_rate min max empty\n1 Suburb                0             1   3  18     0\n2 Address               0             1   7  27     0\n3 Type                  0             1   1   1     0\n4 Method                0             1   1   2     0\n5 SellerG               0             1   1  27     0\n6 Postcode              0             1   4   4     0\n7 Regionname            0             1  16  26     0\n8 CouncilArea           0             1  17  30     0\n  n_unique whitespace\n1      380          0\n2    57754          0\n3        3          0\n4        9          0\n5      476          0\n6      225          0\n7        8          0\n8       34          0\n\n‚îÄ‚îÄ Variable type: Date ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  skim_variable n_missing complete_rate min       \n1 Date                  0             1 2016-01-28\n  max        median     n_unique\n1 2018-10-13 2017-09-03      112\n\n‚îÄ‚îÄ Variable type: numeric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  skim_variable n_missing complete_rate      mean         sd\n1 Rooms                 0         1          3.11      0.958\n2 Price             14590         0.768 997898.   593499.   \n3 Propertycount         0         1       7618.     4424.   \n4 Distance              0         1         12.7       7.59 \n     p0    p25      p50       p75       p100 hist \n1     1      3      3         4         31   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n2 85000 620000 830000   1220000   11200000   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n3    39   4380   6795     10412      21650   ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñÅ\n4     0      7     11.4      16.7       64.1 ‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\ndf2 |&gt;\n  select(Suburb, Rooms, Type, Price, Date) |&gt;\n  arrange(Suburb, Date) |&gt;\n  visdat::vis_miss()\n\n\ndf2 |&gt;\n  mutate(miss = ifelse(is.na(Price), \n    \"Missing\", \"Recorded\")) |&gt;\n  count(Rooms, miss) |&gt;\n  filter(Rooms &lt; 8) |&gt;\n  group_by(miss) |&gt;\n  mutate(perc = n / sum(n) * 100) |&gt;\n  ggplot(aes(as.factor(Rooms), perc, fill = miss)) +\n    geom_col(position = \"dodge\") +\n    scale_fill_viridis_d(begin=0.3, end=0.7) +\n    labs(x = \"Rooms\", y = \"Percentage\", fill = \"Price\") +\n    theme(aspect.ratio = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs there a suspicious plot?\n\n\n\nBreak the association between Rooms and Missing/Not on Price, because the null hypothesis is that there is no difference in missing status for price based on the size of the house. Why?\n\nlibrary(nullabor)\ndf2_d &lt;- df2 |&gt;\n  mutate(miss = ifelse(is.na(Price), \"Missing\", \"Recorded\")) |&gt;\n  select(Rooms, miss) |&gt;\n  filter(Rooms &lt; 8)\ndf2_l &lt;- lineup(null_permute(\"miss\"), df2_d, n=10, pos=7) \ndf2_l_agg &lt;- df2_l |&gt;\n  group_by(.sample) |&gt;\n  count(Rooms, miss) |&gt;\n  ungroup() |&gt;\n  group_by(miss) |&gt;\n  mutate(perc = n / sum(n) * 100) |&gt;\n  mutate(Rooms = as.factor(Rooms))\nggplot(df2_l_agg, aes(x=Rooms, y=perc, fill = miss)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_viridis_d(begin=0.3, end=0.7) +\n  facet_wrap(~.sample, ncol=5) +\n  theme(legend.position = \"none\", \n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.grid.major.x = element_blank())"
  },
  {
    "objectID": "week5/slides.html#your-turn",
    "href": "week5/slides.html#your-turn",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üß© Your turn",
    "text": "üß© Your turn\n\n\nWhat might be alternative plots? Especially to reveal the relationship more clearly."
  },
  {
    "objectID": "week5/slides.html#check-the-support-of-your-data",
    "href": "week5/slides.html#check-the-support-of-your-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Check the support of your data",
    "text": "Check the support of your data\n\nIf you have too few measurements in any region (extreme), summaries for these regions will be unreliable.\n\n\nFor quantitative variables, it may be necessary to remove extremes.\nIf the variable is categorical it might be best to combine levels.\nIt is important to script so decisions can be reversed or rare events are not ignored.\n\n We removed houses with 8 or more rooms. What other way might we have handled these houses?"
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-36",
    "href": "week5/slides.html#melbourne-housing-prices-36",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (3/6)",
    "text": "Melbourne Housing Prices (3/6)\n\nüìädataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat can we say from this plot?\n\n\nThe housing prices are right-skewed\nThere appears to be a lot of outlying housing prices (how can we tell?)\n\n\nNote: We determined that it is likely that more higher price houses have not disclosed the sale price. The distribution of price will need to be checked again after imputation.\n\n\n\n\n\n\n\ndf2 &lt;- read_csv(here::here(\"data/MELBOURNE_HOUSE_PRICES_LESS.csv\"),\n  col_types = cols(\n    .default = col_character(),\n    Rooms = col_double(),\n    Price = col_double(),\n    Date = col_date(format = \"%d/%m/%Y\"),\n    Propertycount = col_double(),\n    Distance = col_double()\n  )\n)\n\n\n\n\ndf2 |&gt;\n  ggplot(aes(Price / 1e6)) +\n  geom_histogram(color = \"white\") +\n  labs(\n    x = \"Price (mil)\",\n    y = \"Count\"\n  )"
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-46",
    "href": "week5/slides.html#melbourne-housing-prices-46",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (4/6)",
    "text": "Melbourne Housing Prices (4/6)\n\nüìädataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe x-axis has been \\(\\log_{10}\\)-transformed in this plot\nThe plot appears more symmetrical now\nWhat is a useful measure of central tendency here?\n\n\n\n\n\n\ndf2 &lt;- read_csv(here::here(\"data/MELBOURNE_HOUSE_PRICES_LESS.csv\"),\n  col_types = cols(\n    .default = col_character(),\n    Rooms = col_double(),\n    Price = col_double(),\n    Date = col_date(format = \"%d/%m/%Y\"),\n    Propertycount = col_double(),\n    Distance = col_double()\n  )\n)\n\n\n\n\ndf2 |&gt;\n  ggplot(aes(Price / 1e6)) +\n  geom_histogram(color = \"white\") +\n  labs(\n    x = \"Price (mil)\",\n    y = \"Count\"\n  ) +\n  scale_x_log10()"
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-56",
    "href": "week5/slides.html#melbourne-housing-prices-56",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (5/6)",
    "text": "Melbourne Housing Prices (5/6)\n\nCentral tendencyR\n\n\nWith no transformation:\n\n\n\n\n\nMean\nMedian\nTrimmed Mean\nWinsorised Mean\n\n\n\n\n$997,898\n$830,000\n$871,375\n$903,823\n\n\n\n\n\n\n\n\nWith log transformation (and back-transformed to original scale):\n\n\n\n\n\nMean\nMedian\nTrimmed Mean\nWinsorised Mean\n\n\n\n\n$874,166\n$830,000\n$847,973\n$859,325\n\n\n\n\n\n\n\n\n\n\ndf2 |&gt;\n  filter(!is.na(Price)) |&gt;\n  summarise(\n    Mean = scales::dollar(mean(Price)),\n    Median = scales::dollar(median(Price)),\n    `Trimmed Mean` = scales::dollar(mean(Price, trim = 0.2)),\n    `Winsorised Mean` = scales::dollar(psych::winsor.mean(Price))\n  ) |&gt;\n  knitr::kable(align = \"r\") |&gt;\n  kableExtra::kable_classic() |&gt;\n  kableExtra::kable_styling(full_width=FALSE)\n\n\ndf2 |&gt;\n  filter(!is.na(Price)) |&gt;\n  mutate(lPrice = log10(Price)) |&gt;\n  summarise(\n    Mean = scales::dollar(10^mean(lPrice)),\n    Median = scales::dollar(10^median(lPrice)),\n    `Trimmed Mean` = scales::dollar(10^mean(lPrice, trim = 0.2)),\n    `Winsorised Mean` = scales::dollar(10^psych::winsor.mean(lPrice))\n  ) |&gt;\n  knitr::kable(align = \"r\") |&gt;\n  kableExtra::kable_classic() |&gt;\n  kableExtra::kable_styling(full_width=FALSE)"
  },
  {
    "objectID": "week5/slides.html#general-rules-for-transform-quantitative-variables",
    "href": "week5/slides.html#general-rules-for-transform-quantitative-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "General rules for transform quantitative variables",
    "text": "General rules for transform quantitative variables\n\n\nNon-shape changing, scaling:\n\nstandardise to mean 0, sd 1\nstandardise to min 0, max 1\nz-score\n\nShape changing, transformations: Remember the ladder of power transformations. (eg transforming left-skewed to more uniform using \\(^2\\))\n\n\n\n\n\n\n\n\n\n\nDistribution changing: quantile\n\n\n\n\n\n\n\n\n\nSome features cannot be fixed: gaps, multimodality, heaping. You need to find some explaining variable.\nSome features can be artificially fixed: discreteness. If regularly discretized, add random uniform noise to spread equally between gaps."
  },
  {
    "objectID": "week5/slides.html#multi-modality",
    "href": "week5/slides.html#multi-modality",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Multi-modality",
    "text": "Multi-modality"
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-66",
    "href": "week5/slides.html#melbourne-housing-prices-66",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (6/6)",
    "text": "Melbourne Housing Prices (6/6)\n\nüìädataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution from side-by-side univariate plots shows that higher number of rooms generally are pricier.\nThis strata could be responsible for multimodality in price distribution, even though it is not visible in the histogram.\nAccounting for rooms is important.\n\n\n\n\n\n\ndf2 &lt;- read_csv(here::here(\"data/MELBOURNE_HOUSE_PRICES_LESS.csv\"),\n  col_types = cols(\n    .default = col_character(),\n    Rooms = col_double(),\n    Price = col_double(),\n    Date = col_date(format = \"%d/%m/%Y\"),\n    Propertycount = col_double(),\n    Distance = col_double()\n  )\n)\n\n\n\n\ndf2 |&gt;\n  filter(Rooms &lt; 8) |&gt;\n  ggplot(aes(x=as.factor(Rooms), y=Price / 1e6, )) +\n  ggbeeswarm::geom_quasirandom(varwidth=TRUE, alpha=0.3) +\n  scale_y_log10() +\n  labs(y = \"Price (mil)\", x = \"# of Rooms\")"
  },
  {
    "objectID": "week5/slides.html#bins-and-bandwidths-more-details",
    "href": "week5/slides.html#bins-and-bandwidths-more-details",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Bins and Bandwidths: More details",
    "text": "Bins and Bandwidths: More details"
  },
  {
    "objectID": "week5/slides.html#hidalgo-stamps-thickness",
    "href": "week5/slides.html#hidalgo-stamps-thickness",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hidalgo stamps thickness",
    "text": "Hidalgo stamps thickness\n\nüìädataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFamous historical example\n\nA stamp collector, Walton von Winkle, bought several collections of Mexican stamps from 1872-1874 and measured the thickness of all of them.\nThe different bandwidth for the density plot suggests different possibilities for number of modes.\n\nWhich do you think most accurately reflects what‚Äôs in the data?\n\n\n\n\n\nload(here::here(\"data/Hidalgo1872.rda\"))\nskimr::skim(Hidalgo1872)\n\n‚îÄ‚îÄ Data Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                           Values     \nName                       Hidalgo1872\nNumber of rows             485        \nNumber of columns          3          \n_______________________               \nColumn type frequency:                \n  numeric                  3          \n________________________              \nGroup variables            None       \n\n‚îÄ‚îÄ Variable type: numeric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  skim_variable n_missing complete_rate   mean      sd    p0\n1 thickness             0         1     0.0860 0.0150  0.06 \n2 thicknessA          195         0.598 0.0922 0.0162  0.068\n3 thicknessB          289         0.404 0.0768 0.00508 0.06 \n     p25   p50   p75  p100 hist \n1 0.075  0.08  0.098 0.131 ‚ñÖ‚ñá‚ñÉ‚ñÇ‚ñÅ\n2 0.0772 0.092 0.105 0.131 ‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñÇ\n3 0.072  0.078 0.08  0.097 ‚ñÅ‚ñÉ‚ñá‚ñÅ‚ñÅ\n\n\n\n\n\nhid &lt;- ggplot(Hidalgo1872, aes(x=thickness, y=25)) +\n  geom_quasirandom(width=15, alpha=0.5, size=1) +\n  labs(x = \"Thickness (0.001 mm)\", y = \"Density\") + \n  ylim(c(0, 70)) +\n  theme(aspect.ratio = 0.6) \nhid_p1 &lt;- hid +\n  geom_density(aes(x=thickness), alpha=0.5,\n    color = \"#E16A86\", bw = 0.01, linewidth=2,\n    inherit.aes = FALSE)\nhid_p2 &lt;- hid +\n  geom_density(aes(x=thickness), alpha=0.5,\n    color = \"#E16A86\", bw = 0.0075, linewidth=2,\n    inherit.aes = FALSE) \nhid_p3 &lt;- hid +\n  geom_density(aes(x=thickness), alpha=0.5,\n    color = \"#E16A86\", bw = 0.004, linewidth=2,\n    inherit.aes = FALSE) \nhid_p4 &lt;- hid +\n  geom_density(aes(x=thickness), alpha=0.5,\n    color = \"#E16A86\", bw = 0.001, linewidth=2,\n    inherit.aes = FALSE) \nhid_p1 + hid_p2 + hid_p3 + hid_p4 + plot_layout(ncol=2)"
  },
  {
    "objectID": "week5/slides.html#olive-oil-content",
    "href": "week5/slides.html#olive-oil-content",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Olive oil content",
    "text": "Olive oil content\n\n\n\n\n\n\n\n\n\n\n\nWhat do you see?\n\nMixture of discreteness and normal shape of continuous values. Why might this happen?\n\n\n\nCheck if there is a difference in the strata (here 1 thru 9), implying measurement policy differences."
  },
  {
    "objectID": "week5/slides.html#re-focus",
    "href": "week5/slides.html#re-focus",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Re-focus",
    "text": "Re-focus"
  },
  {
    "objectID": "week5/slides.html#movie-length",
    "href": "week5/slides.html#movie-length",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Movie length",
    "text": "Movie length\n\nüìädataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpon further exploration, you can find the two movies that are well over 16 hours long are ‚ÄúCure for Insomnia‚Äù, ‚ÄúFour Stars‚Äù, and ‚ÄúLongest Most Meaningless Movie in the World‚Äù\nWe can restrict our attention to films under 3 hours:\n\n\n\n\n\n\n\n\n\n\n\nNotice that there is a peak at particular times. Why do you think so?\n\n\n\n\n\n\n\ndata(movies, package = \"ggplot2movies\")\nskimr::skim(movies)\n\n‚îÄ‚îÄ Data Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                           Values\nName                       movies\nNumber of rows             58788 \nNumber of columns          24    \n_______________________          \nColumn type frequency:           \n  character                2     \n  numeric                  22    \n________________________         \nGroup variables            None  \n\n‚îÄ‚îÄ Variable type: character ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  skim_variable n_missing complete_rate min max empty\n1 title                 0             1   1 121     0\n2 mpaa                  0             1   0   5 53864\n  n_unique whitespace\n1    56007          0\n2        5          0\n\n‚îÄ‚îÄ Variable type: numeric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   skim_variable n_missing complete_rate          mean\n 1 year                  0        1          1976.    \n 2 length                0        1            82.3   \n 3 budget            53573        0.0887 13412513.    \n 4 rating                0        1             5.93  \n 5 votes                 0        1           632.    \n 6 r1                    0        1             7.01  \n 7 r2                    0        1             4.02  \n 8 r3                    0        1             4.72  \n 9 r4                    0        1             6.37  \n10 r5                    0        1             9.80  \n11 r6                    0        1            13.0   \n12 r7                    0        1            15.5   \n13 r8                    0        1            13.9   \n14 r9                    0        1             8.95  \n15 r10                   0        1            16.9   \n16 Action                0        1             0.0797\n17 Animation             0        1             0.0628\n18 Comedy                0        1             0.294 \n19 Drama                 0        1             0.371 \n20 Documentary           0        1             0.0591\n21 Romance               0        1             0.0807\n22 Short                 0        1             0.161 \n             sd   p0      p25       p50        p75\n 1       23.7   1893   1958      1983       1997  \n 2       44.3      1     74        90        100  \n 3 23350085.       0 250000   3000000   15000000  \n 4        1.55     1      5         6.1        7  \n 5     3830.       5     11        30        112  \n 6       10.9      0      0         4.5        4.5\n 7        5.96     0      0         4.5        4.5\n 8        6.45     0      0         4.5        4.5\n 9        7.59     0      0         4.5        4.5\n10        9.73     0      4.5       4.5       14.5\n11       11.0      0      4.5      14.5       14.5\n12       11.6      0      4.5      14.5       24.5\n13       11.3      0      4.5      14.5       24.5\n14        9.44     0      4.5       4.5       14.5\n15       15.7      0      4.5      14.5       24.5\n16        0.271    0      0         0          0  \n17        0.243    0      0         0          0  \n18        0.455    0      0         0          1  \n19        0.483    0      0         0          1  \n20        0.236    0      0         0          0  \n21        0.272    0      0         0          0  \n22        0.367    0      0         0          0  \n          p100 hist \n 1      2005   ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñá\n 2      5220   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n 3 200000000   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n 4        10   ‚ñÅ‚ñÉ‚ñá‚ñÜ‚ñÅ\n 5    157608   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n 6       100   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n 7        84.5 ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n 8        84.5 ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n 9       100   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n10       100   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n11        84.5 ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n12       100   ‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\n13       100   ‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\n14       100   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n15       100   ‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\n16         1   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n17         1   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n18         1   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÉ\n19         1   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÖ\n20         1   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n21         1   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n22         1   ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n\n\n\n\n\nggplot(movies, aes(length)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Length of movie (minutes)\", y = \"Frequency\") +\n  theme(aspect.ratio = 0.6)\n\nggplot(movies, aes(length)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Length of movie (minutes)\", y = \"Frequency\") +\n  scale_x_log10() +\n  theme(aspect.ratio = 0.6)\n\n\nmovies |&gt;\n  filter(length &lt; 180) |&gt;\n  ggplot(aes(length)) +\n  geom_histogram(binwidth = 1, fill = \"#795549\", color = \"black\") +\n  labs(x = \"Length of movie (minutes)\", y = \"Frequency\")"
  },
  {
    "objectID": "week5/slides.html#categorical-variables",
    "href": "week5/slides.html#categorical-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Categorical variables",
    "text": "Categorical variables"
  },
  {
    "objectID": "week5/slides.html#there-are-two-types-of-categorical-variables",
    "href": "week5/slides.html#there-are-two-types-of-categorical-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "There are two types of categorical variables",
    "text": "There are two types of categorical variables\n\nNominal where there is no intrinsic ordering to the categories E.g. blue, grey, black, white.\n\nOrdinal where there is a clear order to the categories. E.g. Strongly disagree, disagree, neutral, agree, strongly agree."
  },
  {
    "objectID": "week5/slides.html#categorical-variables-in-r",
    "href": "week5/slides.html#categorical-variables-in-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Categorical variables in R",
    "text": "Categorical variables in R\n\n\n\nIn R, categorical variables may be encoded as factors.\n\n\ndata &lt;- c(2, 2, 1, 1, 3, 3, 3, 1)\nfactor(data)\n\n[1] 2 2 1 1 3 3 3 1\nLevels: 1 2 3\n\n\n\nYou can easily change the labels of the variables:\n\n\nfactor(data, labels = c(\"I\", \"II\", \"III\"))\n\n[1] II  II  I   I   III III III I  \nLevels: I II III\n\n\n\n\nOrder of the factors are determined by the input:\n\n\n# numerical input are ordered in increasing order \nfactor(c(1, 3, 10))\n\n[1] 1  3  10\nLevels: 1 3 10\n\n# character input are ordered by first char, alphabetically \nfactor(c(\"1\", \"3\", \"10\"))\n\n[1] 1  3  10\nLevels: 1 10 3\n\n# you can specify order of levels explicitly \nfactor(c(\"1\", \"3\", \"10\"),\n  levels = c(\"1\", \"3\", \"10\")\n)\n\n[1] 1  3  10\nLevels: 1 3 10"
  },
  {
    "objectID": "week5/slides.html#numerical-summaries-counts-proportions-percentages-and-odds",
    "href": "week5/slides.html#numerical-summaries-counts-proportions-percentages-and-odds",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Numerical summaries: counts, proportions, percentages and odds",
    "text": "Numerical summaries: counts, proportions, percentages and odds\n\n\nTuberculosis counts in Australia\n\n\n# A tibble: 22 √ó 7\n   country   iso3   year count      p   pct  odds\n   &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Australia AUS    2000   982 0.0522  5.22 1    \n 2 Australia AUS    2001   953 0.0507  5.07 0.970\n 3 Australia AUS    2002  1008 0.0536  5.36 1.03 \n 4 Australia AUS    2003   926 0.0493  4.93 0.943\n 5 Australia AUS    2004  1036 0.0551  5.51 1.05 \n 6 Australia AUS    2005  1030 0.0548  5.48 1.05 \n 7 Australia AUS    2006  1127 0.0600  6.00 1.15 \n 8 Australia AUS    2007  1081 0.0575  5.75 1.10 \n 9 Australia AUS    2008  1182 0.0629  6.29 1.20 \n10 Australia AUS    2009  1176 0.0626  6.26 1.20 \n11 Australia AUS    2010  1146 0.0610  6.10 1.17 \n12 Australia AUS    2011  1202 0.0640  6.40 1.22 \n13 Australia AUS    2012  1259 0.0670  6.70 1.28 \n14 Australia AUS    2013   512 0.0272  2.72 0.521\n15 Australia AUS    2014   474 0.0252  2.52 0.483\n16 Australia AUS    2015   438 0.0233  2.33 0.446\n17 Australia AUS    2016   481 0.0256  2.56 0.490\n18 Australia AUS    2017   524 0.0279  2.79 0.534\n19 Australia AUS    2018   502 0.0267  2.67 0.511\n20 Australia AUS    2019   554 0.0295  2.95 0.564\n21 Australia AUS    2020   609 0.0324  3.24 0.620\n22 Australia AUS    2021   593 0.0316  3.16 0.604\n\n\n\nFor qualitative data, compute\n\ncount/frequency,\nproportion/percentage\nand sometimes, an odds ratio. Here we have used ratio relative to the count in year 2000.\n\n Note: For exploration, no rounding of digits was done, but to report you would need to make the numbers pretty."
  },
  {
    "objectID": "week5/slides.html#australian-federal-election",
    "href": "week5/slides.html#australian-federal-election",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election",
    "text": "2019 Australian Federal Election\n\nüìädataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSorting levels sensibly is (almost) always better when plotting\n\n\n\n\ndf1 &lt;- read_csv(here::here(\"data/HouseFirstPrefsByCandidateByVoteTypeDownload-24310.csv\"),\n  skip = 1,\n  col_types = cols(\n    .default = col_character(),\n    OrdinaryVotes = col_double(),\n    AbsentVotes = col_double(),\n    ProvisionalVotes = col_double(),\n    PrePollVotes = col_double(),\n    PostalVotes = col_double(),\n    TotalVotes = col_double(),\n    Swing = col_double()\n  )\n)\n\n\ntdf3 &lt;- df1 |&gt;\n  group_by(DivisionID) |&gt;\n  summarise(\n    DivisionNm = unique(DivisionNm),\n    State = unique(StateAb),\n    votes_GRN = TotalVotes[which(PartyAb == \"GRN\")],\n    votes_total = sum(TotalVotes)\n  ) |&gt;\n  mutate(perc_GRN = votes_GRN / votes_total * 100)\n\n\n\n\ntdf3 |&gt;\n  ggplot(aes(perc_GRN, State)) +\n  ggbeeswarm::geom_quasirandom(groupOnX = FALSE, varwidth = TRUE) +\n  labs(\n    x = \"First preference votes %\",\n    y = \"State\",\n    title = \"Greens party\"\n  )\n\n\ntdf3 |&gt;\n  mutate(State = fct_reorder(State, perc_GRN)) |&gt;\n  ggplot(aes(perc_GRN, State)) +\n  ggbeeswarm::geom_quasirandom(groupOnX = FALSE, varwidth = TRUE) +\n  labs(\n    x = \"First preference votes %\",\n    y = \"State\",\n    title = \"Greens party\"\n  )"
  },
  {
    "objectID": "week5/slides.html#order-nominal-variables-meaningfully",
    "href": "week5/slides.html#order-nominal-variables-meaningfully",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Order nominal variables meaningfully",
    "text": "Order nominal variables meaningfully\n Coding tip: use below functions to easily change the order of factor levels\n \n\nstats::reorder(factor, value, mean)\nforcats::fct_reorder(factor, value, median)\nforcats::fct_reorder2(factor, value1, value2, func)"
  },
  {
    "objectID": "week5/slides.html#visual-inference-1",
    "href": "week5/slides.html#visual-inference-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference",
    "text": "Visual inference\n\n\nTypical plot description:\n\nggplot(data, aes(x=var1)) +\n  geom_col()\n\nggplot(data, aes(x=var1)) +\n  geom_bar()\n\n Is the distribution consistent with a sample from a binomial distribution with a given p?\n\nPotential simulation method from binomial\n\n# Only one option\nnull_dist(\"var1\", \"binom\", \n  list(size=n, p=phat))"
  },
  {
    "objectID": "week5/slides.html#lineup-of-tuberculosis-count-between-sexes",
    "href": "week5/slides.html#lineup-of-tuberculosis-count-between-sexes",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Lineup of tuberculosis count between sexes",
    "text": "Lineup of tuberculosis count between sexes\n\nüìäUse conventional testR\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor this problem there is nothing more to learn from a lineup that what can be learned from a conventional hypothesis test of \\(H_0: p=0.5\\).\n\nbinom.test(tb_oz_2012$count, n, p = 0.5, alternative = \"two.sided\")\n\n\n    Exact binomial test\n\ndata:  tb_oz_2012$count\nnumber of successes = 314, number of trials = 997,\np-value &lt;2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.29 0.34\nsample estimates:\nprobability of success \n                  0.31 \n\n\n\n\n\nlibrary(nullabor)\nset.seed(252)\ntb_oz_2012 &lt;- tb |&gt;\n  filter(iso3 == \"AUS\",\n         year == 2012) |&gt;\n  select(iso3, year, new_sp_m04:new_ep_m65) |&gt;\n  pivot_longer(cols=new_sp_m04:new_ep_m65, \n               names_to = \"var\", \n               values_to = \"count\") |&gt;\n  separate(var, into=c(\"new\", \"type\", \"sexage\")) |&gt;\n  select(-new) |&gt;\n  filter(!(sexage %in% c(\"sexunk014\", \"sexunk04\", \n                         \"sexunk15plus\", \"sexunk514\",\n                         \"f15plus\", \"m15plus\"))) |&gt;\n  group_by(sexage) |&gt;\n  summarise(count = sum(count, na.rm=TRUE)) |&gt;\n  mutate(sex=str_sub(sexage, 1, 1),\n         age=str_sub(sexage, 2, str_length(sexage))) |&gt;\n  group_by(sex) |&gt;\n  summarise(count=sum(count)) |&gt;\n  ungroup() |&gt;\n  mutate(sex01 = ifelse(sex==\"m\", 0, 1)) |&gt;\n  select(-sex)\n\nggplot(lineup(null_dist(\"count\", \"binom\", \n                        list(size=sum(tb_oz_2012$count),\n                               p=0.5)), \n              tb_oz_2012, n=10),\n       aes(x=sex01, y=count)) +\n  geom_col() +\n  facet_wrap(~.sample, ncol=5) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.grid.major = element_blank())"
  },
  {
    "objectID": "week5/slides.html#key-points",
    "href": "week5/slides.html#key-points",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Key points",
    "text": "Key points\n\nBe prepared to do multiple plots\nChanging bins or binwidth/bandwidth in histogram, violin or density plots can paint a different picture\nConsider different representations of categorical variables\n\nreordering meaningfully,\nlumping low frequencies together,\nplot or table, pie or barplot,\ninclude a missing category"
  },
  {
    "objectID": "week5/slides.html#imputing-missings-for-univariate-distributions",
    "href": "week5/slides.html#imputing-missings-for-univariate-distributions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Imputing missings for univariate distributions",
    "text": "Imputing missings for univariate distributions\n\n\nQuantitative variable: Simulate from a fitted distribution.\n\ndf2 &lt;- df2 |&gt;\n  mutate(lPrice = log10(Price),\n         price_miss = ifelse(is.na(Price), \"yes\", \"no\"))\n\ndf2_smry &lt;- df2 |&gt;\n  summarise(m = mean(lPrice, na.rm=TRUE),\n            s = sd(lPrice, na.rm=TRUE))\nset.seed(1003)  \ndf2 &lt;- df2 |&gt;\n  rowwise() |&gt;\n  mutate(lPrice = ifelse(price_miss == \"yes\", \n    rnorm(1, df2_smry$m, df2_smry$s), lPrice)) |&gt;\n  mutate(Price = ifelse(price_miss == \"yes\", 10^lPrice, Price))\n\n\n\n\n\n\n\n\n\n\n\nCategorical variable: Simulate from multinomial.\n\n\n# A tibble: 7 √ó 2\n  age   count\n  &lt;chr&gt; &lt;dbl&gt;\n1 1524     27\n2 2534     48\n3 3544     15\n4 4554     11\n5 5564      9\n6 65       15\n7 u        12\n\n\n [1] 5 3 1 1 2 1 1 2 1 1 1 1\n\n\n# A tibble: 7 √ó 2\n  age   count\n  &lt;chr&gt; &lt;dbl&gt;\n1 1524     35\n2 2534     50\n3 3544     16\n4 4554     11\n5 5564     10\n6 65       15\n7 u        12\n\n\nimputeMulti library can automate for multiple variables."
  },
  {
    "objectID": "week5/slides.html#resources",
    "href": "week5/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\n\nUnwin (2015) Graphical Data Analysis with R\nHarrison, David, and Daniel L. Rubinfeld (1978) Hedonic Housing Prices and the Demand for Clean Air, Journal of Environmental Economics and Management 5 81-102. Original data.\nGilley, O.W. and R. Kelley Pace (1996) On the Harrison and Rubinfeld Data. Journal of Environmental Economics and Management 31 403-405. Provided corrections and examined censoring.\nMaindonald, John H. and Braun, W. John (2020). DAAG: Data Analysis and Graphics Data and Functions. R package version 1.24\nBritish Board of Trade (1990), Report on the Loss of the ‚ÄòTitanic‚Äô (S.S.). British Board of Trade Inquiry Report (reprint). Gloucester, UK: Allan Sutton Publishing\nHand, D. J., Daly, F., McConway, K., Lunn, D. and Ostrowski, E. eds (1993) A Handbook of Small Data Sets. Chapman & Hall, Data set 285 (p.¬†229)  Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0\nFleiss JL (1993): The statistical basis of meta-analysis. Statistical Methods in Medical Research 2 121‚Äì145 Balduzzi S, R√ºcker G, Schwarzer G (2019), How to perform a meta-analysis with R: a practical tutorial, Evidence-Based Mental Health.\nJosse et al (2022) R-miss-tastic, https://rmisstastic.netlify.app\n\n\n\n\n\nETC5521 Lecture 5 | ddde.numbat.space"
  },
  {
    "objectID": "week6/index.html",
    "href": "week6/index.html",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "",
    "text": "Wilke (2019) Ch 12 Visualising associations"
  },
  {
    "objectID": "week6/index.html#main-reference",
    "href": "week6/index.html#main-reference",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "",
    "text": "Wilke (2019) Ch 12 Visualising associations"
  },
  {
    "objectID": "week6/index.html#what-you-will-learn-this-week",
    "href": "week6/index.html#what-you-will-learn-this-week",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nThe humble but powerful scatterplot\nAdditions and variations\nTransformations to linearity\n(Robust) numerical measures of association\nSimpson‚Äôs paradox\nMaking null samples to test for association"
  },
  {
    "objectID": "week6/index.html#lecture-slides",
    "href": "week6/index.html#lecture-slides",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week6/index.html#tutorial-instructions",
    "href": "week6/index.html#tutorial-instructions",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week6/index.html#assignments",
    "href": "week6/index.html#assignments",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 2 is due on Tuesday 27 August."
  },
  {
    "objectID": "week6/tutorialsol.html",
    "href": "week6/tutorialsol.html",
    "title": "ETC5521 Tutorial 6",
    "section": "",
    "text": "These are exercises in making plots of one variable and what can be learned about the distributions and the data patterns and problems."
  },
  {
    "objectID": "week6/tutorialsol.html#objectives",
    "href": "week6/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 6",
    "section": "",
    "text": "These are exercises in making plots of one variable and what can be learned about the distributions and the data patterns and problems."
  },
  {
    "objectID": "week6/tutorialsol.html#preparation",
    "href": "week6/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 6",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\ninstall.packages(c(\"ggplot2movies\", \"bayesm\", \"flexmix\",  \"ggbeeswarm\", \"mixtools\", \"lvplot\", \"patchwork\", \"nullabor\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week6/tutorialsol.html#exercises",
    "href": "week6/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 6",
    "section": "üì• Exercises",
    "text": "üì• Exercises\n\nExercise 1: Understanding the velocity of galaxies\nLoad the galaxies data in the MASS package and answer the following questions based on this dataset.\n\ndata(galaxies, package = \"MASS\")\n\nYou can access documentation of the data (if available) using the help function specifying the package name in the argument.\n\nhelp(galaxies, package = \"MASS\")\n\n\nWhat does the data contain? And what is the data source?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\ndata(galaxies, package = \"MASS\")\nglimpse(galaxies)\n\n num [1:82] 9172 9350 9483 9558 9775 ...\n\n\nThe data contains velocities in km/sec of 82 galaxies from 6 well-separated conic sections of an unfilled survey of the Corona Borealis region. The original data is from Postman et al.¬†(1986) and this data is from Roeder with 83rd observation removed from the original data as well as typo for the 78th observation.\n\nPostman, M., Huchra, J. P. and Geller, M. J. (1986) Probes of large-scale structures in the Corona Borealis region. Astronomical Journal 92, 1238‚Äì1247\nRoeder, K. (1990) Density estimation with confidence sets exemplified by superclusters and voids in galaxies. Journal of the American Statistical Association 85, 617‚Äì624.\n\n\n\n\n\n\nBased on the description in the R Help for the data, what would be an appropriate null distribution of this data?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe description in the R help for the data says Multimodality in such surveys is evidence for voids and superclusters in the far universe.\nDeciding on an appropriate null hypothesis is always tricky. If we wanted to test the statement that the data is multimodal, we could compare against a unimodal distribution, either a normal or an exponential depending on what shape we might expect.\nHowever, the published work has already made a claim that the data is multimodal, so it would be interesting to determine if we can generate samples from a multimodal distribution that are indistinguishable from the data.\n\\(H_0:\\) The distribution is multimodal. \\(H_a:\\) The distribution is something other than multimodal.\n\n\n\n\n\nHow many observations are there?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThere are 82 observations.\n\n\n\n\n\nIf the data is multimodal, which of the following displays do you think would be the best? Which would not be at all useful?\n\n\nhistogram\nboxplot\ndensity plot\nviolin plot\njittered dot plot\nletter value plot\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nIf you said a density plot, jittered dot plot, or a histogram, you‚Äôre on the right track, because each can give a fine resolution for showing modes. (The violin plot is not any different from a density plot, when only looking at one variable.)\n\n\n\n\n\nMake these plots for the data. Experiment with different binwidths for the histogram and different bandwiths for the density plot. Were you right in your thinking about which would be the best?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\ng &lt;- ggplot(tibble(galaxies), aes(galaxies)) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\ng1 &lt;- g + geom_histogram(binwidth = 1000, color = \"white\") \ng2 &lt;- g + geom_boxplot() \ng3 &lt;- g + geom_density() \ng4 &lt;- g + geom_violin(aes(x=galaxies, y=1), draw_quantiles = c(0.25, 0.5, 0.75))\ng5 &lt;- g + geom_quasirandom(aes(x=1, y=galaxies)) + coord_flip() \ng6 &lt;- g + geom_lv(aes(x=1, y=galaxies)) + coord_flip() \n\ng1 + g2 + g3 + g4 + g5 + g6 + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFit your best mixture model to the data, and simulate 19 nulls to make a lineup. Did you do a good job in matching the distribution, ie does the data plot stand out or not? (Extra bonus: What is the model that you have created? Can you make a plot to show how it looks relative to the observed data?)\n\nThis code might be helpful to get you started. This code generates a jittered dotplot, but you can use your preferred type from part e.\n\n# Fit a mixture model\nlibrary(mixtools)\ngalaxies_fit &lt;- normalmixEM(galaxies, k=3)\n\nset.seed(1138)\ngalaxies_sim1 &lt;- rnormmix(n=length(galaxies), \n              lambda=galaxies_fit$lambda, \n              mu=galaxies_fit$mu,\n              sigma=galaxies_fit$sigma)\n\n\n# Plot your data\nggplot(tibble(galaxies_sim1), aes(x=galaxies_sim1)) +\n  geom_quasirandom(aes(x=1, y=galaxies_sim1)) + \n  coord_flip() +\n  theme(\n    aspect.ratio = 0.7,\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n# Generate null plots and make a lineup\ngalaxies_null &lt;- tibble(.sample=1, galaxies=galaxies_sim1)\nfor (i in 2:19) {\n  gsim &lt;- rnormmix(n=length(galaxies), \n              lambda=galaxies_fit$lambda, \n              mu=galaxies_fit$mu,\n              sigma=galaxies_fit$sigma)\n  galaxies_null &lt;- bind_rows(galaxies_null,\n                             tibble(.sample=i, galaxies=gsim))\n}\ngalaxies_null &lt;- bind_rows(galaxies_null,\n                             tibble(.sample=20,\n                                    galaxies=galaxies))\n# Randomise .sample  to hide data plot\ngalaxies_null$.sample &lt;- rep(sample(1:20, 20), rep(82, 20))\nggplot(tibble(galaxies_null), aes(x=galaxies)) +\n  geom_quasirandom(aes(x=1, y=galaxies)) + \n  facet_wrap(~.sample, ncol=5) +\n  coord_flip() +\n  theme(\n    aspect.ratio = 0.7,\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nnumber of iterations= 38 \n\n\n\n\n\n\n\n\n\n\n\n\n# To make a rough plot of your model\nplot(galaxies_fit, whichplots=2)\n\n\n\n\n\n\n\n\nThe lambda value provides the proportion of mixing, from three normal samples. The mu and sigma give the mean and standard deviations for each of the distributions.\n\n\n\n\n\n\nExercise 2: What are the common lengths of movies?\nLoad the movies dataset in the ggplot2movies package and answer the following questions based on it.\n\nHow many observations are in the data?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThere are 58788 observations.\n\n\n\n\n\nDraw a histogram with an appropriate binwidth that shows the peaks at 7 minutes and 90 minutes. Draw another set of histograms to show whether these peaks existed both before and after 1980.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\ndata(movies)\nmovies |&gt;\n  mutate(\n    after1980 = ifelse(year &gt; 1980,\n      \"After 1980\",\n      \"1980 or before\"\n    ),\n    copy = FALSE\n  ) |&gt;\n  bind_rows(mutate(movies, copy = TRUE, after1980 = \"All\")) |&gt;\n  ggplot(aes(length)) +\n  geom_histogram(binwidth = 1, fill = \"yellow\", color = \"black\") +\n  scale_x_continuous(\n    breaks = c(0, 7, 30, 60, 90, 120, 150, 180),\n    limits = c(0, 180)\n  ) +\n  facet_grid(after1980 ~ .)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe variable Short indicates whether the film was classified as a short film (1) or not (0). Draw plots to investigate what rules was used to define a film as ‚Äúshort‚Äù and whether the films have been consistently classified.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nmovies |&gt;\n  group_by(Short) |&gt;\n  summarise(x = list(summary(length))) |&gt;\n  unnest_wider(x)\n\n# A tibble: 2 √ó 7\n  Short Min.        `1st Qu.`   Median      Mean        `3rd Qu.`   Max.       \n  &lt;int&gt; &lt;table[1d]&gt; &lt;table[1d]&gt; &lt;table[1d]&gt; &lt;table[1d]&gt; &lt;table[1d]&gt; &lt;table[1d]&gt;\n1     0 1           85          93          95          103         5220       \n2     1 1            7          10          14           19          240       \n\n\nThe maximum length for a film classified as short is 240 minutes.\n\nmovies |&gt;\n  mutate(short = factor(Short, labels = c(\"Long\", \"Short\"))) |&gt;\n  ggplot(aes(y=length, x=short)) +\n  geom_quasirandom() +\n  scale_y_log10(\n    limits = c(1, 240),\n    breaks = c(1, 7, 10, 15, 20, 30, 45, 50, 70, 90, 110, 240)\n  ) +\n  labs(y = \"\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nFrom the graph, majority of films classified as short are under 50 minutes while those classified as long tend to be longer than 50 minutes. There are clear cases of mislabelling, e.g.¬†a one-minute long film classified as ‚Äúnot short‚Äù.\nOn further detective work, the original source of the data says ‚ÄúAny theatrical film or made-for-video title with a running time of less than 45 minutes, i.e., 44 minutes or less, or any TV series or TV movie with a running time of less than 22 minutes, i.e.¬†21 minutes or less. (A‚Äùhalf-hour‚Äù television program should not be listed as a Short.)‚Äù Given this is an objective measure based on the length of the film, we can see that that any films that are 45 minutes or longer should be classified as long and less than that as short.\n\n\n\n\n\nHow would you use the lineup protocol to determine if the periodic peaks could happen by chance? What would be the null hypothesis? Make your lineup. Does the data plot stand out? Compute the \\(p\\)-value, if 5 out of 12 people picked the data plot as the most different one in the lineup. Comment on the results. (Note: It might be most useful to assess this only for movies between 50-150 minutes long.)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nmovies |&gt; \n  filter(between(length, 50, 150)) %&gt;%\n  select(length) %&gt;%\n  lineup(null_dist(\"length\", \"norm\"), true=., n=9) %&gt;%\n  ggplot(aes(x=length)) +\n    geom_histogram(binwidth = 1) +\n    scale_x_continuous(\n      breaks = c(0, 7, 30, 60, 90, 120, 150, 180),\n      limits = c(0, 180)\n    ) +\n  facet_wrap(~.sample, ncol=3, scales=\"free\") +\n  theme(axis.text = element_blank(),\n    axis.title = element_blank(),\n    panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\nSimulate samples from a normal distribution using the sample mean and standard deviation as the parameters.\n\\(H_0\\): Periodic peaks are not present by chance.\nI would expect the \\(p\\)-value to be 0 as the data plot is very clearly different from the nulls. This suggests that the multimodality is not possible to observe in samples from a normal distribution, and that the pattern is present because the movie lengths are commonly cut to specific numbers of minutes.\n\n\n\n\n\n\nExercise 3: What is the market for different brands of whisky?\nThe Scotch data set in bayesm package was collated from a survey on scotch drinkers, recording the brand they consumed. Take a quick look at the data, and rearrange it to look like:\n\n\n# A tibble: 21 √ó 2\n   brand                      count\n   &lt;chr&gt;                      &lt;int&gt;\n 1 Ballantine                    99\n 2 Black & White                 81\n 3 Chivas Regal                 806\n 4 Clan MacGregor               103\n 5 Cutty Sark                   339\n 6 Dewar's White Label          517\n 7 Glenfiddich                  334\n 8 Glenlivet                    354\n 9 Grant's                       74\n10 J&B                          458\n11 Johnnie Walker Black Label   502\n12 Johnnie Walker Red Label     424\n13 Knockando                     47\n14 Macallan                      95\n15 Other Brands                 414\n16 Passport                      82\n17 Pinch (Haig)                 117\n18 Scoresby Rare                 79\n19 Ushers                        67\n20 White Horse                   62\n21 Singleton                     31\n\n\n\nProduce a barplot of the number of respondents per brand. What ordering of the brands do you think is the best? What is interesting about the distribution of counts?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nscotch_consumption |&gt;\n  mutate(\n    brand = fct_reorder(brand, count),\n    #brand = fct_relevel(brand, \"Other brands\")\n  ) |&gt;\n  ggplot(aes(x=count, y=brand)) +\n  geom_col() +\n  ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are 20 named brands and one category that is labelled as Other.brands. Produce a barplot that you think best reduces the number of categories by selecting a criteria to lump certain brands to the Other category.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nscotch_consumption |&gt;\n  mutate(\n    brand = ifelse(count &gt; 200, brand, \"Other brands\"),\n    brand = fct_reorder(brand, count),\n    brand = fct_relevel(brand, \"Other brands\")\n  ) |&gt;\n  ggplot(aes(count, brand)) +\n  geom_col()\n\n\n\n\n\n\n\n\nI‚Äôve chosen the cut-off to be 200 as there was a gap in frequency between brands that sold more than 200 and less than 200. This reduces the comparison to 8 named brands, which is more manageable for comparison.\n\n\n\n\n\nThink about what a not interesting pattern might be for this data, and formulate an appropriate null hypothesis.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe least interesting pattern is probably if all the bars are similar heights, meaning that all brands are consumed equally.\nThis leads to a null hypothesis of \\(H_o: p_k = 1/K\\) where all brands have the same proportion of consumers.\n\n\n\n\n\nIf you were to test whether this sample were consistent with a sample from a multinomial distribution, where all whiskeys were equally popular, how would to generate null samples? Make the lineup for testing this.\n\nThe following code might help:\n\n# Subset the data, and anonymmise brand name\nscotch_consumption_sub &lt;- scotch_consumption |&gt;\n    mutate(\n    brand = ifelse(count &gt; 200, brand, \"Other Brands\")\n  ) |&gt;\n  filter(brand != \"Other Brands\") |&gt;\n  mutate(brand = as.character(factor(brand, labels=c(1:8)))) \n\nset.seed(220)\nsim &lt;- rmultinom(n=9,\n   size=sum(scotch_consumption_sub$count),\n   prob=rep(1/8, 8))\nsim &lt;- t(sim)\ncolnames(sim) &lt;- as.character(c(1:8))\nsim &lt;- sim |&gt;\n  as_tibble() |&gt;\n  mutate(.sample=1:9)\nsim &lt;- sim |&gt;\n  pivot_longer(cols=`1`:`8`, names_to = \"brand\", values_to = \"count\")\nscotch_lineup &lt;- bind_rows(sim,\n  bind_cols(.sample=10, scotch_consumption_sub))\n\n# Randomise .sample  to hide data plot\nscotch_lineup$.sample &lt;- rep(sample(1:10, 10), rep(8, 10))\n  \n# Make the lineup\nggplot(scotch_lineup, aes(x=brand, y=count)) +\n  geom_col() +\n  facet_wrap(~.sample, scales=\"free\", ncol=5) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe need to simulate samples from a multinomial distribution for the null sets.\nIf the data was a sample from a uniform distribution, it would mean that all of these brands are typically consumed in equal quantity.\n\\(H_0:\\) The counts for the brands are consistent with a sample from a uniform distribution.\n\n\n\n\n\n\n\n\n\nIdeally the bars are sorted from highest to lowest in each plot. This is tricky to do with facetting. The code below will do the sorting and re-draw the lineup.\n\n# Order categories in all samples from highest to lowest: TRICKY\np &lt;- list(\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\", \"p9\", \"p10\")\nfor (i in unique(scotch_lineup$.sample)) {\n  d &lt;- scotch_lineup |&gt; \n    filter(.sample == i)\n  d &lt;- d |&gt;\n    mutate(brand = fct_reorder(brand, count))\n  p[[i]] &lt;- ggplot(d, aes(y=brand, x=count)) +\n    geom_col() +\n    ggtitle(i) +\n    theme(axis.text = element_blank(),\n        axis.title = element_blank())\n}\n\np[[1]] + p[[2]] + p[[3]] + p[[4]] + p[[5]] +\n  p[[6]] + p[[7]] + p[[8]] + p[[9]] + p[[10]] + \n  plot_layout(ncol=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose you show your lineup to five of people who have not seen the data, and three of them report the data plot as the most different plot. Compute the \\(p\\)-value. What would these results tell you about the typical consumption of the different whiskey brands?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\npvisual(5, 3, 10)\n\n     x simulated binom\n[1,] 5         0     0\n\n\nLikely people will report that the reason for choosing the plot is that one bar is much bigger than the other bars.\nIt tells us that the data is not a sample from multinomial with equal probabilities, at least in the sense that one brand is consumed more frequently than the others.\n\n\n\n\n\nThis analysis ignored structure in the data, that survey participants could report consuming more than one brand. Have a discussion about what complications this might introduce for the analysis that we have just done. What might be an alternative way to compute the ‚Äúcounts‚Äù that takes this multiple responses into account? What else might we want to learn about survey participant responses?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nBy ignoring the number of responses per participant we have made the assumption that all responses are independent of each other.\nOne alternative way to compute the counts would be convert each participants‚Äô responses into a fraction of their responses, for example, participant 1,\n\nScotch[1,]\n\n  Chivas.Regal Dewar.s.White.Label Johnnie.Walker.Black.Label J...B\n1            1                   0                          0     0\n  Johnnie.Walker.Red.Label Other.Brands Glenlivet Cutty.Sark Glenfiddich\n1                        1            1         1          0           1\n  Pinch..Haig. Clan.MacGregor Ballantine Macallan Passport Black...White\n1            0              0          0        0        0             0\n  Scoresby.Rare Grants Ushers White.Horse Knockando the.Singleton\n1             0      0      0           0         0             0\n\n\nreports consuming Chivas.Regal, Johnnie.Walker.Red.Label, Other.Brands, Glenlivet, and Glenfiddich. Each 1 would then be converted to 1/5.\nThe reason for re-analysing this way is to equally weight participants‚Äô responses, so a participant who responded a lot would contribute the same relative amount to the overall count as a participant who responded very little.\nOther questions that might be of interest are:\n\nDo some respondents drink a variety of brands and others only few?\nDo some brands get consumed together more often than others?\n\nYou might come up with some other ideas!\n\n\n\n\n\n\nExercise 4: What is the best transformation to make?\nFor each of the variables in the data, which-transform.csv, decide on an appropriate transformation to make the distribution more symmetric for five of the variables and remove discreteness on one variable.\n\n\n\n\n\n\n\n\n\nYour tutor has suggestions."
  },
  {
    "objectID": "week6/tutorialsol.html#finishing-up",
    "href": "week6/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 6",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week6/tutorial.html",
    "href": "week6/tutorial.html",
    "title": "ETC5521 Tutorial 6",
    "section": "",
    "text": "These are exercises in making plots of one variable and what can be learned about the distributions and the data patterns and problems."
  },
  {
    "objectID": "week6/tutorial.html#objectives",
    "href": "week6/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 6",
    "section": "",
    "text": "These are exercises in making plots of one variable and what can be learned about the distributions and the data patterns and problems."
  },
  {
    "objectID": "week6/tutorial.html#preparation",
    "href": "week6/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 6",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\ninstall.packages(c(\"ggplot2movies\", \"bayesm\", \"flexmix\",  \"ggbeeswarm\", \"mixtools\", \"lvplot\", \"patchwork\", \"nullabor\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week6/tutorial.html#exercises",
    "href": "week6/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 6",
    "section": "üì• Exercises",
    "text": "üì• Exercises\n\nExercise 1: Understanding the velocity of galaxies\nLoad the galaxies data in the MASS package and answer the following questions based on this dataset.\n\ndata(galaxies, package = \"MASS\")\n\nYou can access documentation of the data (if available) using the help function specifying the package name in the argument.\n\nhelp(galaxies, package = \"MASS\")\n\n\nWhat does the data contain? And what is the data source?\n\n\nBased on the description in the R Help for the data, what would be an appropriate null distribution of this data?\n\n\nHow many observations are there?\n\n\nIf the data is multimodal, which of the following displays do you think would be the best? Which would not be at all useful?\n\n\nhistogram\nboxplot\ndensity plot\nviolin plot\njittered dot plot\nletter value plot\n\n\nMake these plots for the data. Experiment with different binwidths for the histogram and different bandwiths for the density plot. Were you right in your thinking about which would be the best?\n\n\nFit your best mixture model to the data, and simulate 19 nulls to make a lineup. Did you do a good job in matching the distribution, ie does the data plot stand out or not? (Extra bonus: What is the model that you have created? Can you make a plot to show how it looks relative to the observed data?)\n\nThis code might be helpful to get you started. This code generates a jittered dotplot, but you can use your preferred type from part e.\n\n# Fit a mixture model\nlibrary(mixtools)\ngalaxies_fit &lt;- normalmixEM(galaxies, k=3)\n\nset.seed(1138)\ngalaxies_sim1 &lt;- rnormmix(n=length(galaxies), \n              lambda=galaxies_fit$lambda, \n              mu=galaxies_fit$mu,\n              sigma=galaxies_fit$sigma)\n\n\n# Plot your data\nggplot(tibble(galaxies_sim1), aes(x=galaxies_sim1)) +\n  geom_quasirandom(aes(x=1, y=galaxies_sim1)) + \n  coord_flip() +\n  theme(\n    aspect.ratio = 0.7,\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n# Generate null plots and make a lineup\ngalaxies_null &lt;- tibble(.sample=1, galaxies=galaxies_sim1)\nfor (i in 2:19) {\n  gsim &lt;- rnormmix(n=length(galaxies), \n              lambda=galaxies_fit$lambda, \n              mu=galaxies_fit$mu,\n              sigma=galaxies_fit$sigma)\n  galaxies_null &lt;- bind_rows(galaxies_null,\n                             tibble(.sample=i, galaxies=gsim))\n}\ngalaxies_null &lt;- bind_rows(galaxies_null,\n                             tibble(.sample=20,\n                                    galaxies=galaxies))\n# Randomise .sample  to hide data plot\ngalaxies_null$.sample &lt;- rep(sample(1:20, 20), rep(82, 20))\nggplot(tibble(galaxies_null), aes(x=galaxies)) +\n  geom_quasirandom(aes(x=1, y=galaxies)) + \n  facet_wrap(~.sample, ncol=5) +\n  coord_flip() +\n  theme(\n    aspect.ratio = 0.7,\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n\nExercise 2: What are the common lengths of movies?\nLoad the movies dataset in the ggplot2movies package and answer the following questions based on it.\n\nHow many observations are in the data?\n\n\nDraw a histogram with an appropriate binwidth that shows the peaks at 7 minutes and 90 minutes. Draw another set of histograms to show whether these peaks existed both before and after 1980.\n\n\nThe variable Short indicates whether the film was classified as a short film (1) or not (0). Draw plots to investigate what rules was used to define a film as ‚Äúshort‚Äù and whether the films have been consistently classified.\n\n\nHow would you use the lineup protocol to determine if the periodic peaks could happen by chance? What would be the null hypothesis? Make your lineup. Does the data plot stand out? Compute the \\(p\\)-value, if 5 out of 12 people picked the data plot as the most different one in the lineup. Comment on the results. (Note: It might be most useful to assess this only for movies between 50-150 minutes long.)\n\n\n\nExercise 3: What is the market for different brands of whisky?\nThe Scotch data set in bayesm package was collated from a survey on scotch drinkers, recording the brand they consumed. Take a quick look at the data, and rearrange it to look like:\n\n\n# A tibble: 21 √ó 2\n   brand                      count\n   &lt;chr&gt;                      &lt;int&gt;\n 1 Ballantine                    99\n 2 Black & White                 81\n 3 Chivas Regal                 806\n 4 Clan MacGregor               103\n 5 Cutty Sark                   339\n 6 Dewar's White Label          517\n 7 Glenfiddich                  334\n 8 Glenlivet                    354\n 9 Grant's                       74\n10 J&B                          458\n11 Johnnie Walker Black Label   502\n12 Johnnie Walker Red Label     424\n13 Knockando                     47\n14 Macallan                      95\n15 Other Brands                 414\n16 Passport                      82\n17 Pinch (Haig)                 117\n18 Scoresby Rare                 79\n19 Ushers                        67\n20 White Horse                   62\n21 Singleton                     31\n\n\n\nProduce a barplot of the number of respondents per brand. What ordering of the brands do you think is the best? What is interesting about the distribution of counts?\n\n\nThere are 20 named brands and one category that is labelled as Other.brands. Produce a barplot that you think best reduces the number of categories by selecting a criteria to lump certain brands to the Other category.\n\n\nThink about what a not interesting pattern might be for this data, and formulate an appropriate null hypothesis.\n\n\nIf you were to test whether this sample were consistent with a sample from a multinomial distribution, where all whiskeys were equally popular, how would to generate null samples? Make the lineup for testing this.\n\nThe following code might help:\n\n# Subset the data, and anonymmise brand name\nscotch_consumption_sub &lt;- scotch_consumption |&gt;\n    mutate(\n    brand = ifelse(count &gt; 200, brand, \"Other Brands\")\n  ) |&gt;\n  filter(brand != \"Other Brands\") |&gt;\n  mutate(brand = as.character(factor(brand, labels=c(1:8)))) \n\nset.seed(220)\nsim &lt;- rmultinom(n=9,\n   size=sum(scotch_consumption_sub$count),\n   prob=rep(1/8, 8))\nsim &lt;- t(sim)\ncolnames(sim) &lt;- as.character(c(1:8))\nsim &lt;- sim |&gt;\n  as_tibble() |&gt;\n  mutate(.sample=1:9)\nsim &lt;- sim |&gt;\n  pivot_longer(cols=`1`:`8`, names_to = \"brand\", values_to = \"count\")\nscotch_lineup &lt;- bind_rows(sim,\n  bind_cols(.sample=10, scotch_consumption_sub))\n\n# Randomise .sample  to hide data plot\nscotch_lineup$.sample &lt;- rep(sample(1:10, 10), rep(8, 10))\n  \n# Make the lineup\nggplot(scotch_lineup, aes(x=brand, y=count)) +\n  geom_col() +\n  facet_wrap(~.sample, scales=\"free\", ncol=5) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\nSuppose you show your lineup to five of people who have not seen the data, and three of them report the data plot as the most different plot. Compute the \\(p\\)-value. What would these results tell you about the typical consumption of the different whiskey brands?\n\n\nThis analysis ignored structure in the data, that survey participants could report consuming more than one brand. Have a discussion about what complications this might introduce for the analysis that we have just done. What might be an alternative way to compute the ‚Äúcounts‚Äù that takes this multiple responses into account? What else might we want to learn about survey participant responses?\n\n\n\nExercise 4: What is the best transformation to make?\nFor each of the variables in the data, which-transform.csv, decide on an appropriate transformation to make the distribution more symmetric for five of the variables and remove discreteness on one variable.\n\n\n\n\n\n\n\n\n\nYour tutor has suggestions."
  },
  {
    "objectID": "week6/tutorial.html#finishing-up",
    "href": "week6/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 6",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week8/index.html",
    "href": "week8/index.html",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "",
    "text": "Cook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1"
  },
  {
    "objectID": "week8/index.html#main-reference",
    "href": "week8/index.html#main-reference",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "",
    "text": "Cook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1"
  },
  {
    "objectID": "week8/index.html#what-you-will-learn-this-week",
    "href": "week8/index.html#what-you-will-learn-this-week",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week8/index.html#lecture-slides",
    "href": "week8/index.html#lecture-slides",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week8/index.html#tutorial-instructions",
    "href": "week8/index.html#tutorial-instructions",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week8/index.html#assignments",
    "href": "week8/index.html#assignments",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week8/index.html#assignments-1",
    "href": "week8/index.html#assignments-1",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 3 is due on Monday 16 September."
  }
]