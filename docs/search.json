[
  {
    "objectID": "week2/slides.html#birth-of-eda",
    "href": "week2/slides.html#birth-of-eda",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Birth of EDA",
    "text": "Birth of EDA\n\nThe field of exploratory data analysis came of age when this book appeared in 1977.\n\nTukey held that too much emphasis in statistics was placed on statistical hypothesis testing (confirmatory data analysis); more emphasis needed to be placed on using data to suggest hypotheses to test."
  },
  {
    "objectID": "week2/slides.html#john-w.-tukey",
    "href": "week2/slides.html#john-w.-tukey",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "John W. Tukey",
    "text": "John W. Tukey\n\n\n\n\n\n\n Image source: wikimedia.org\n\n\nBorn in 1915, in New Bedford, Massachusetts.\nMum was a private tutor who home-schooled John. Dad was a Latin teacher.\nBA and MSc in Chemistry, and PhD in Mathematics\nAwarded the National Medal of Science in 1973, by President Nixon\nBy some reports, his home-schooling was unorthodox and contributed to his thinking and working differently."
  },
  {
    "objectID": "week2/slides.html#taking-a-glimpse-back-in-time",
    "href": "week2/slides.html#taking-a-glimpse-back-in-time",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Taking a glimpse back in time",
    "text": "Taking a glimpse back in time\nis possible with the American Statistical Association video lending library.\n We’re going to watch John Tukey talking about exploring high-dimensional data with an amazing new computer in 1973, four years before the EDA book.\n\nLook out for these things:\nTukey’s expertise is described as for trial and error learning and the computing equipment.\n\n\nFirst 4.25 minutes"
  },
  {
    "objectID": "week2/slides.html#setting-the-frame-of-mind",
    "href": "week2/slides.html#setting-the-frame-of-mind",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Setting the frame of mind",
    "text": "Setting the frame of mind\nExcerpt from the introduction\n\nThis book is based on an important principle.\n It is important to understand what you CAN DO before you learn to measure how WELL you seem to have DONE it.\n Learning first what you can do will help you to work more easily and effectively.\n This book is about exploratory data analysis, about looking at data to see what it seems to say. It concentrates on simple arithmetic and easy-to-draw pictures. It regards whatever appearances we have recognized as partial descriptions, and tries to look beneath them for new insights. Its concern is with appearance, not with confirmation.\n Examples, NOT case histories\n The book does not exist to make the case that exploratory data analysis is useful. Rather it exists to expose its readers and users to a considerable variety of techniques for looking more effectively at one’s data. The examples are not intended to be complete case histories. Rather they should isolated techniques in action on real data. The emphasis is on general techniques, rather than specific problems. \nA basic problem about any body of data is to make it more easily and effectively handleable by minds – our minds, her mind, his mind. To this general end:\n\nanything that make a simpler description possible makes the description more easily handleable.\nanything that looks below the previously described surface makes the description more effective.\n\n\nSo we shall always be glad (a) to simplify description and (b) to describe one layer deeper. In particular,\n\nto be able to say that we looked one layer deeper, and found nothing, is a definite step forward – though not as far as to be able to say that we looked deeper and found thus-and-such.\nto be able to say that “if we change our point of view in the following way … things are simpler” is always a gain–though not quite so much as to be able to say “if we don’t bother to change out point of view (some other) things are equally simple.”\n\n …\n Consistent with this view, we believe, is a clear demand that pictures based on exploration of data should force their messages upon us. Pictures that emphasize what we already know–“security blankets” to reassure us–are frequently not worth the space they take. Pictures that have to be gone over with a reading glass to see the main point are wasteful of time and inadequate of effect. The greatest value of a picture is when it forces us to notice what we never expected to see.\n\n\nConfirmation\n\n\nThe principles and procedures of what we call confirmatory data analysis are both widely used and one of the great intellectual products of our century. In their simplest form, these principles and procedures look at a sample–and at what that sample has told us about the population from which it came–and assess the precision with which our inference from sample to population is made. We can no longer get along without confirmatory data analysis. But we need not start with it.\n\nThe best way to understand what CAN be done is not longer–if it ever was–to ask what things could, in the current state of our skill techniques, be confirmed (positively or negatively). Even more understanding is lost if we consider each thing we can do to data only in terms of some set of very restrictive assumptions under which that thing is best possible–assumptions we know we CANNOT check in practice.\n\nExploration AND confirmation\n\nOnce upon a time, statisticians only explored. Then they learned to confirm exactly–to confirm a few things exactly, each under very specific circumstances. As they emphasized exact confirmation, their techniques inevitably became less flexible. The connection of the most used techniques with past insights was weakened. Anything to which confirmatory procedure was not explicitly attached was decried as “mere descriptive statistics”, no matter how much we learned from it.\n\nToday, the flexibility of (approximate) confirmation by the jacknife makes it relatively easy to ask, for almost any clearly specified exploration, “How far is it confirmed?”\n\nToday, exploratory and confirmatory can–and should–proceed side by side. This book, of course, considers only exploratory techniques, leaving confirmatory techniques to other accounts.\n\n\n About the problems \n\n The teacher needs to be careful about assigning problems. Not too many, please. They are likely to take longer than you think. The number supplied is to accommodate diversity of interest, not to keep everybody busy.\n Besides the length of our problems, both teacher and student need to realise that many problems do not have a single “right answer”. There can be many ways to approach a body of data. Not all are equally good. For some bodies of data this may be clear, but for others we may not be able to tell from a single body of data which approach is preferred. Even several bodies of data about very similar situations may not be enough to show which approach should be preferred. Accordingly, it will often be quite reasonable for different analysts to reach somewhat different analyses.\n Yet more–to unlock the analysis of a body of day, to find the good way to approach it, may require a key, whose finding is a creative act. Not everyone can be expected to create the key to any one situation. And to continue to paraphrase Barnum, no one can be expected to create a key to each situation he or she meets.\n To learn about data analysis, it is right that each of us try many things that do not work–that we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed–at least until we were told about it–an opportunity to learn more. Each teacher needs to recognize this in grading and commenting on problems.\n\n\n Precision\n\nThe teacher who heeds these words and admits that there need be no one correct approach may, I regret to contemplate, still want whatever is done to be digit perfect. (Under such a requirement, the write should still be able to pass the course, but it is not clear whether she would get an “A”.) One does, from time to time, have to produce digit-perfect, carefully checked results, but forgiving techniques that are not too distributed by unusual data are also, usually, little disturbed by SMALL arithmetic errors. The techniques we discuss here have been chosen to be forgiving. It is hoped, then, that small arithmetic errors will take little off the problem’s grades, leaving severe penalties for larger errors, either of arithmetic or concept."
  },
  {
    "objectID": "week2/slides.html#outline",
    "href": "week2/slides.html#outline",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Outline",
    "text": "Outline\n\n\n\nScratching down numbers\nSchematic summary\nEasy re-expression\nEffective comparison\nPlots of relationship\nStraightening out plots (using three points)\nSmoothing sequences\nParallel and wandering schematic plots\nDelineations of batches of points\nUsing two-way analyses\n\n\n\n\n\nMaking two-way analyses\nAdvanced fits\nThree way fits\nLooking in two or more ways at batched of points\nCounted fractions\nBetter smoothing\nCounts in bin after bin\nProduct-ratio plots\nShapes of distributions\nMathematical distributions"
  },
  {
    "objectID": "week2/slides.html#looking-at-numbers-with-tukey",
    "href": "week2/slides.html#looking-at-numbers-with-tukey",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Looking at numbers with Tukey",
    "text": "Looking at numbers with Tukey"
  },
  {
    "objectID": "week2/slides.html#scratching-down-numbers",
    "href": "week2/slides.html#scratching-down-numbers",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scratching down numbers",
    "text": "Scratching down numbers\n\n\nPrices of Chevrolet in the local used car newspaper ads of 1968.\n\noptions(width=20)\nchevrolets &lt;- tibble(\n  prices = c(250, 150, 795, 895, 695, \n               1699, 1499, 1099, 1693, 1166,\n               688, 1333, 895, 1775, 895,\n               1895, 795))\n#chevrolets$prices\n\n\nStem-and-leaf plot: still seen in introductory statistics texts"
  },
  {
    "objectID": "week2/slides.html#section-1",
    "href": "week2/slides.html#section-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "First stem-and-leaf, first digit on stem, second digit on leaf\n\n\nOrder any leaves which need it, eg stem 6\n\n\n\nA benefit is that the numbers can be read off the plot, but the focus is still on the pattern. Also quantiles like the median, can be computed easily."
  },
  {
    "objectID": "week2/slides.html#section-2",
    "href": "week2/slides.html#section-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Shrink the stem\n\n\nShrink the stem more"
  },
  {
    "objectID": "week2/slides.html#and-in-r",
    "href": "week2/slides.html#and-in-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "And, in R …",
    "text": "And, in R …\n\nchevrolets$prices\n\n [1]  250  150  795\n [4]  895  695 1699\n [7] 1499 1099 1693\n[10] 1166  688 1333\n[13]  895 1775  895\n[16] 1895  795\n\nstem(chevrolets$prices)\n\n\n  The decimal point is 3 digit(s) to the right of the |\n\n  0 | 23\n  0 | 7788999\n  1 | 123\n  1 | 57789"
  },
  {
    "objectID": "week2/slides.html#remember-the-tips-data",
    "href": "week2/slides.html#remember-the-tips-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "🔖 Remember the tips data",
    "text": "🔖 Remember the tips data\n\n\n [1] 1.01 1.66 3.50 3.31 3.61 4.71 2.00 3.12 1.96 3.23 1.71 5.00 1.57 3.00 3.02\n[16] 3.92 1.67 3.71 3.50 3.35 4.08 2.75 2.23 7.58 3.18 2.34 2.00 2.00 4.30 3.00\n[31] 1.45 2.50 3.00 2.45 3.27 3.60 2.00 3.07 2.31 5.00 2.24 2.54 3.06 1.32 5.60\n[46] 3.00 5.00 6.00 2.05 3.00\n\n\n\nstem(tips$tip, scale=0.5, width=120)\n\n\n  The decimal point is at the |\n\n   1 | 000001233334445555555555556666667777788889\n   2 | 000000000000000000000000000000000000000001122222223333555555555555556666677788899\n   3 | 00000000000000000000000011111112222222333344445555555555555666778889\n   4 | 0000000000001112233335777\n   5 | 00000000001122226799\n   6 | 05577\n   7 | 6\n   8 | \n   9 | 0\n  10 | 0"
  },
  {
    "objectID": "week2/slides.html#refining-the-size",
    "href": "week2/slides.html#refining-the-size",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Refining the size",
    "text": "Refining the size\n\n\nFive digits per stem\n\n\n\nWhat is the number in parentheses? And why might this be useful?\n\n\n\nTwo digits per stem\n\n\n\n\n\nstem(tips$tip, scale=2)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n   10 | 0000107\n   12 | 55526\n   14 | 44578000000000678\n   16 | 1346781356\n   18 | 032678\n   20 | 00000000000000000000000000000000011233598\n   22 | 0033440114\n   24 | 5700000000002456\n   26 | 01412455\n   28 | 382\n   30 | 00000000000000000000000267891245688\n   32 | 133557159\n   34 | 0188800000000015\n   36 | 0181566\n   38 | 2\n   40 | 0000000000006889\n   42 | 09004\n   44 | 0\n   46 | 713\n   48 | \n   50 | 000000000074567\n   52 | 0\n   54 | \n   56 | 05\n   58 | 52\n   60 | 0\n   62 | \n   64 | 00\n   66 | 03\n   68 | \n   70 | \n   72 | \n   74 | 8\n   76 | \n   78 | \n   80 | \n   82 | \n   84 | \n   86 | \n   88 | \n   90 | 0\n   92 | \n   94 | \n   96 | \n   98 | \n  100 | 0\n\n\n\n\nWhy no number in parentheses?\n\n\n\nmedian(tips$tip)\n\n[1] 2.9"
  },
  {
    "objectID": "week2/slides.html#summary",
    "href": "week2/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\nStem-and-leaf plots are similar information to the histogram.\nGenerally it is possible to also read off the numbers, and to then easily calculate median or Q1 or Q3.\nIt’s great for small data sets, when you only have pencil and paper.\nAlternatives are a histogram, (jittered) dotplot, density plot, box plot, violin plot, letter value plot."
  },
  {
    "objectID": "week2/slides.html#a-different-style-of-number-scratching",
    "href": "week2/slides.html#a-different-style-of-number-scratching",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "a different style of number scratching",
    "text": "a different style of number scratching\nfor categorical variables\n\n\nWe know about\n\nbut its too easy to\n\nmake a mistake\n\nIs this easier?\n\n\nor harder"
  },
  {
    "objectID": "week2/slides.html#count-this-data-using-the-squares-approach.",
    "href": "week2/slides.html#count-this-data-using-the-squares-approach.",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Count this data using the squares approach.",
    "text": "Count this data using the squares approach.\n\n\n\n\n [1] \"F\" \"M\" \"M\" \"M\" \"F\" \"M\"\n [7] \"M\" \"M\" \"M\" \"M\" \"M\" \"F\"\n[13] \"M\" \"M\" \"F\" \"M\" \"F\" \"M\"\n[19] \"F\" \"M\" \"M\" \"F\" \"F\" \"M\"\n[25] \"M\" \"M\" \"M\" \"M\" \"M\" \"F\"\n[31] \"M\" \"M\" \"F\" \"F\" \"M\" \"M\"\n[37] \"M\" \"F\" \"M\" \"M\" \"M\" \"M\"\n[43] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n[49] \"M\" \"M\" \"M\" \"F\" \"F\" \"M\"\n[55] \"M\" \"M\" \"M\" \"F\" \"M\" \"M\"\n[61] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n[67] \"F\" \"F\" \"M\" \"M\" \"M\" \"F\""
  },
  {
    "objectID": "week2/slides.html#what-does-it-mean-to-feel-what-the-data-are-like",
    "href": "week2/slides.html#what-does-it-mean-to-feel-what-the-data-are-like",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What does it mean to “feel what the data are like?”",
    "text": "What does it mean to “feel what the data are like?”"
  },
  {
    "objectID": "week2/slides.html#section-3",
    "href": "week2/slides.html#section-3",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "This is a stem and leaf of the height of the highest peak in each of the 50 US states.\n\nThe states roughly fall into three groups.\n\nIt’s not really surprising, but we can imagine this grouping. Alaska is in a group of its own, with a much higher high peak. Then the Rocky Mountain states, California, Washington and Hawaii also have high peaks, and the rest of the states lump together."
  },
  {
    "objectID": "week2/slides.html#section-4",
    "href": "week2/slides.html#section-4",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "More summaries of numerical values"
  },
  {
    "objectID": "week2/slides.html#section-5",
    "href": "week2/slides.html#section-5",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Isn’t this imposing a belief?"
  },
  {
    "objectID": "week2/slides.html#hinges-and-5-number-summaries",
    "href": "week2/slides.html#hinges-and-5-number-summaries",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hinges and 5-number summaries",
    "text": "Hinges and 5-number summaries\n\n\n\n\n [1] -3.2 -1.7 -0.4  0.1\n [5]  0.3  1.2  1.5  1.8\n [9]  2.4  3.0  4.3  6.4\n[13]  9.8\n\n\nYou know the median is the middle number. What’s a hinge?\nThere are 13 data values here, provided already sorted. We are going to write them into a Tukey named down-up-down-up pattern, evenly.\nMedian will be 7th, hinge will be 4th from each end."
  },
  {
    "objectID": "week2/slides.html#hinges-and-5-number-summary",
    "href": "week2/slides.html#hinges-and-5-number-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hinges and 5-number summary",
    "text": "Hinges and 5-number summary\n\n\n\n\n\nHinges are almost always the same as Q1 and Q3"
  },
  {
    "objectID": "week2/slides.html#box-and-whisker-display",
    "href": "week2/slides.html#box-and-whisker-display",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "box-and-whisker display",
    "text": "box-and-whisker display\n\n\n\n\nStarting with a 5-number summary"
  },
  {
    "objectID": "week2/slides.html#box-and-whisker-display-1",
    "href": "week2/slides.html#box-and-whisker-display-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "box-and-whisker display",
    "text": "box-and-whisker display\n\n\nStarting with a 5-number summary"
  },
  {
    "objectID": "week2/slides.html#identified-end-values",
    "href": "week2/slides.html#identified-end-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Identified end values",
    "text": "Identified end values\n\n\n\nWhy are some individual points singled out?\n\n\nRules for this one may be clearer?"
  },
  {
    "objectID": "week2/slides.html#section-6",
    "href": "week2/slides.html#section-6",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "There is no excuse for failing to plot and look\nAnother Tukey wisdom drop"
  },
  {
    "objectID": "week2/slides.html#section-7",
    "href": "week2/slides.html#section-7",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "We now regard re-expression as a tool, something to let us do a better job of grasping. The grasping is done with the eye and the better job is through a more symmetric appearance.\nAnother Tukey wisdom drop"
  },
  {
    "objectID": "week2/slides.html#fences-and-outside-values",
    "href": "week2/slides.html#fences-and-outside-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Fences and outside values",
    "text": "Fences and outside values\n\n\nH-spread: difference between the hinges (we would call this Inter-Quartile Range)\nstep: 1.5 times H-spread\ninner fences: 1 step outside the hinges\nouter fences: 2 steps outside the hinges\nthe value at each end closest to, but still inside the inner fence are “adjacent”\nvalues between an inner fence and its neighbouring outer fence are “outside”\nvalues beyond outer fences are “far out”\nthese rules produce a SCHEMATIC PLOT"
  },
  {
    "objectID": "week2/slides.html#new-statistics-trimeans",
    "href": "week2/slides.html#new-statistics-trimeans",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "New statistics: trimeans",
    "text": "New statistics: trimeans\nThe number that comes closest to\n\\[\\frac{\\text{lower hinge} + 2\\times \\text{median} + \\text{upper hinge}}{4}\\] is the trimean.\n \nThink about trimmed means, where we might drop the highest and lowest 5% of observations."
  },
  {
    "objectID": "week2/slides.html#letter-value-plots",
    "href": "week2/slides.html#letter-value-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Letter value plots",
    "text": "Letter value plots\n\n\nWhy break the data into quarters? Why not eighths, sixteenths? k-number summaries?\nWhat does a 7-number summary look like?\n\nHow would you make an 11-number summary?\n\n\nlibrary(lvplot)\np &lt;- ggplot(mpg, \n            aes(class, hwy))\np + geom_lv(aes(fill=..LV..)) + \n  scale_fill_brewer() + \n  coord_flip() + \n  xlab(\"\")"
  },
  {
    "objectID": "week2/slides.html#box-plots-are-ubiquitous-in-use-today.",
    "href": "week2/slides.html#box-plots-are-ubiquitous-in-use-today.",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Box plots are ubiquitous in use today.",
    "text": "Box plots are ubiquitous in use today.\n - 🐈🐩 Mostly used to compare distributions, multiple subsets of the data.\n\nPuts the emphasis on the middle 50% of observations, although variations can put emphasis on other aspects."
  },
  {
    "objectID": "week2/slides.html#easy-re-expression",
    "href": "week2/slides.html#easy-re-expression",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Easy re-expression",
    "text": "Easy re-expression"
  },
  {
    "objectID": "week2/slides.html#logs-square-roots-reciprocals",
    "href": "week2/slides.html#logs-square-roots-reciprocals",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Logs, square roots, reciprocals",
    "text": "Logs, square roots, reciprocals\n\n\nWhat you need to know about logs?\n\nhow to find good enough logs fast and easily\nthat equal differences in logs correspond to equal ratios of raw values.\n\n(This means that wherever you find people using products or ratios– even in such things as price indexes–using logs–thus converting producers to sums and ratios to differences–is likely to help.)\n\n\nThe most common transformations are logs, sqrt root, reciprocals, reciprocals of square roots\n\n-1, -1/2, +1/2, +1\n\nWhat happened to ZERO?\n\n\nIt turns out that the role of a zero power, is for the purposes of re-expression, neatly solved by the logarithm."
  },
  {
    "objectID": "week2/slides.html#re-express-to-symmetrize-the-distribution",
    "href": "week2/slides.html#re-express-to-symmetrize-the-distribution",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Re-express to symmetrize the distribution",
    "text": "Re-express to symmetrize the distribution"
  },
  {
    "objectID": "week2/slides.html#power-ladder",
    "href": "week2/slides.html#power-ladder",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Power ladder",
    "text": "Power ladder\n \n⬅️ fix RIGHT-skewed values  \n-2, -1, -1/2, 0 (log), 1/3, 1/2, 1, 2, 3, 4\n\nfix LEFT-skewed values ➡️"
  },
  {
    "objectID": "week2/slides.html#section-8",
    "href": "week2/slides.html#section-8",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The book is a digest of 🌟 tricks and treats 🌟 of massaging numbers and drafting displays.\nMany of the tools have made it into today’s analyses in various ways. Many have not.\nNotice the word developments too: froots, fences. Tukey brought you the word “software”\nThe temperament of the book is an inspiration for the mind-set for this unit. There is such delight in working with numbers!\n“We love data!”"
  },
  {
    "objectID": "week2/slides.html#linearising-bivariate-relationships",
    "href": "week2/slides.html#linearising-bivariate-relationships",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linearising bivariate relationships",
    "text": "Linearising bivariate relationships\n  \n\nSurprising observation: The small fluctuations in later years.\nWhat might be possible reasons?"
  },
  {
    "objectID": "week2/slides.html#linearising-bivariate-relationships-1",
    "href": "week2/slides.html#linearising-bivariate-relationships-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linearising bivariate relationships",
    "text": "Linearising bivariate relationships\n  \n\nSee some fluctuations in the early years, too. Note that the log transformation couldn’t linearise."
  },
  {
    "objectID": "week2/slides.html#section-9",
    "href": "week2/slides.html#section-9",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The book is a digest of 🌟 tricks and treats 🌟 of massaging numbers and drafting displays.\nMany of the tools have made it into today’s analyses in various ways. Many have not.\nNotice the word developments too: froots, fences. Tukey brought you the word “software”\nThe temperament of the book is an inspiration for the mind-set for this unit. There is such delight in working with numbers!\n“We love data!”"
  },
  {
    "objectID": "week2/slides.html#rules-and-advice",
    "href": "week2/slides.html#rules-and-advice",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Rules and advice",
    "text": "Rules and advice\n\n\n\nGraphics are friendly.\nArithmetic often exists to make graphs possible.\nGraphs force us to notice the unexpected; nothing could be more important.\nDifferent graphs show us quite different aspects of the same data.\nThere is no more reason to expect one graph to “tell all” than to expect one number to do the same.\n“Plotting \\(y\\) against \\(x\\)” involves significant choices–how we express one or both variables can be crucial.\n\n\n\n\n\nThe first step in penetrating plotting is to straighten out the dependence or point scatter as much as reasonable.\nPlotting \\(y^2\\), \\(\\sqrt{y}\\), \\(log(y)\\), \\(-1/y\\) or the like instead of \\(y\\) is one plausible step to take in search of straightness.\nPlotting \\(x^2\\), \\(\\sqrt{x}\\), \\(log(x)\\), \\(-1/x\\) or the like instead of \\(x\\) is another.\nOnce the plot is straightened, we can usually gain much by flattening it, usually by plotting residuals.\nWhen plotting scatters, we may need to be careful about how we express \\(x\\) and \\(y\\) in order to avoid concealment by crowding."
  },
  {
    "objectID": "week2/slides.html#section-10",
    "href": "week2/slides.html#section-10",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The book is a digest of 🌟 tricks and treats 🌟 of massaging numbers and drafting displays.\nMany of the tools have made it into today’s analyses in various ways. Many have not.\nNotice the word developments too: froots, fences. Tukey brought you the word “software”\nThe temperament of the book is an inspiration for the mind-set for this unit. There is such delight in working with numbers!\n“We love data!”"
  },
  {
    "objectID": "week2/slides.html#take-aways",
    "href": "week2/slides.html#take-aways",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Take-aways",
    "text": "Take-aways\n\nTukey’s approach was a reaction to many years of formalising data analysis using statistical hypothesis testing.\nMethodology development in statistical testing was a reaction to the ad-hoc nature of data analysis.\nComplex machine learning models like neural networks are in reaction to the inability of statistical models to capture highly non-linear relationships, and depend heavily on the data provided.\nExploring data today is in reaction to the need to explain complex models, to support organisations against legal challenges to decisions made from the model\nIt is much easier to accomplish computers.\n“Exploratory data analysis” as commonly used today term is unfortunately synonymous with “descriptive statistics”, but it is truly much more. Understanding its history from Tukey’s advocation helps you see it is the tooling to discover what you don’t know."
  },
  {
    "objectID": "week2/slides.html#resources",
    "href": "week2/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nwikipedia\nJohn W. Tukey (1977) Exploratory data analysis\nData coding using tidyverse suite of R packages\nSketching canvases made using fabricerin"
  },
  {
    "objectID": "week2/slides.html#letter-value-plots-todays-solution",
    "href": "week2/slides.html#letter-value-plots-todays-solution",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Letter value plots: today’s solution",
    "text": "Letter value plots: today’s solution\n\n\nWhy break the data into quarters? Why not eighths, sixteenths? k-number summaries?\nWhat does a 7-number summary look like?\n\nHow would you make an 11-number summary?\n\n\nlibrary(lvplot)\np &lt;- ggplot(mpg, \n            aes(class, hwy))\np + geom_lv(aes(fill=..LV..)) + \n  scale_fill_brewer() + \n  coord_flip() + \n  xlab(\"\")"
  }
]