[
  {
    "objectID": "week9/worksheet.html",
    "href": "week9/worksheet.html",
    "title": "ETC5521 Worksheet Week 9",
    "section": "",
    "text": "Exercise 1: Men’s heights\nThe heights data provided in the brolgar package contains average male heights in 144 countries from 1500-1989.\n\nWhat’s the time index for this data? What is the key?\n\n\nFilter the data to keep only measurements since 1700, when there are records for many countries. Make a spaghetti plot for the values from Australia. Does it look like Australian males are getting taller?\n\n\nCheck the number of observations for each country. How many countries have less than five years of measurements? Filter these countries out of the data, because we can’t study temporal trend without sufficient measurements.\n\n\nMake a spaghetti plot of all the data, with a smoother overlaid. Does it look like men are generally getting taller?\n\n\nUse facet_strata to break the data into subsets using the year, and plot is several facets. What sort of patterns are there in terms of the earliest year that a country appears in the data?\n\n\nCompute the three number summary (min, median, max) for each country. Make density plots of these statistics, overlaid in a single plot, and a parallel coordinate plot of these three statistics. What is the average minimum (median, maximum) height across countries? Are there some countries who have roughly the same minimum, median and maximum height?\n\n\nWhich country has the tallest men? Which country has highest median male height? Which country has the shortest men? Would you say that the distribution of heights within a country is similar for all countries?"
  },
  {
    "objectID": "week9/worksheetsol.html",
    "href": "week9/worksheetsol.html",
    "title": "ETC5521 Worksheet Week 9",
    "section": "",
    "text": "Exercise 1: Men’s heights\nThe heights data provided in the brolgar package contains average male heights in 144 countries from 1500-1989.\n\nWhat’s the time index for this data? What is the key?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe time index is year, and key is country.\n\n\n\n\n\nFilter the data to keep only measurements since 1700, when there are records for many countries. Make a spaghetti plot for the values from Australia. Does it look like Australian males are getting taller?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nIts looking like Australian males are getting taller BUT …. There are few measurements in the 1900s, and none since 1975. The data for Australia looks unreliable.\n\n\nCode\nheights &lt;- brolgar::heights |&gt; filter(year &gt; 1700)\nheights_oz &lt;- heights |&gt; \n  filter(country == \"Australia\") \nggplot(heights_oz,\n       aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_point() + \n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck the number of observations for each country. How many countries have less than five years of measurements? Filter these countries out of the data, because we can’t study temporal trend without sufficient measurements.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\nheights &lt;- heights |&gt; \n  add_n_obs() |&gt; \n  filter(n_obs &gt;= 5)\n\n\n\n\n\n\n\nMake a spaghetti plot of all the data, with a smoother overlaid. Does it look like men are generally getting taller?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nGenerally, the trend is up, so yes it does look like men are getting taller across the globe.\n\n\nCode\nggplot(heights,\n       aes(x = year,\n           y = height_cm)) + \n  geom_line(aes(group = country), alpha = 0.3) + \n  geom_smooth(se=FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse facet_strata to break the data into subsets using the year, and plot is several facets. What sort of patterns are there in terms of the earliest year that a country appears in the data?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe countries are pretty evenly distributed across the facets, which means that there are roughly similar numbers of countries regularly joining their data into the collection.\n\n\nCode\nheights &lt;- as_tsibble(heights,\n                      index = year,\n                      key = country,\n                      regular = FALSE)\nset.seed(530)\nggplot(heights, aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() + \n  facet_strata(along = -year)\n\n\n\n\n\n\n\n\n\nCode\nset.seed(530)\nheights |&gt;\n  sample_n_keys(size = 10) |&gt;\n  ggplot(aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompute the three number summary (min, median, max) for each country. Make density plots of these statistics, overlaid in a single plot, and a parallel coordinate plot of these three statistics. What is the average minimum (median, maximum) height across countries? Are there some countries who have roughly the same minimum, median and maximum height?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe average minimum height is about 164cm, median is about 168cm and tallest is about 172cm. The maximum height appears to be bimodal, with a small peak around 178cm.\nMost countries have the expected pattern of increasing heights from minimum, median to maximum. There are a few which have very similar values of these, though, which is a bit surprising. It means that there has been no change in these metrics over time.\n\n\nCode\nheights_three &lt;- heights |&gt;\n  features(height_cm, c(\n    min = min,\n    median = median,\n    max = max\n  ))\nheights_three_l &lt;- heights_three |&gt; \n  pivot_longer(cols = min:max,\n               names_to = \"feature\",\n               values_to = \"value\")\n\np1 &lt;- heights_three_l |&gt; \n  ggplot(aes(x = value,\n             fill = feature)) + \n  geom_density(alpha = 0.5) +\n  labs(x = \"Value\",\n       y = \"Density\",\n       fill = \"Feature\") + \n  scale_fill_discrete_qualitative(palette = \"Dark 3\") +\n  xlab(\"Height\") +\n  ylab(\"\") +\n  theme(legend.position = \"none\",\n        aspect.ratio = 1)\n\np2 &lt;- heights_three_l |&gt; \n ggplot(aes(x = factor(feature, \n                       levels = c(\"min\", \"median\", \"max\")),\n            y = value,\n             group = country)) + \n  geom_line(alpha = 0.4) +\n  xlab(\"\") +\n  ylab(\"Height\") +\n  theme(aspect.ratio = 1)\n\nheights_three &lt;- heights_three |&gt; \n  mutate(country = factor(country)) |&gt;\n  mutate(country = fct_reorder(country, median)) \np3 &lt;- heights_three |&gt;\n    ggplot() + \n    geom_point(aes(x = country,\n           y = median)) +\n    geom_errorbar(aes(x = country, \n                      ymin=min, ymax=max), \n                  alpha = 0.6, width=0) +\n    xlab(\"\") + ylab(\"heights\") +\n    coord_flip() +\n  theme(axis.text.y = element_text(size=6),\n        aspect.ratio = 2)\n \ndesign &lt;- \"\n1133\n1133\n2233\n2233\"\np1 + p2 + p3 + \n  plot_layout(design = design)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich country has the tallest men? Which country has highest median male height? Which country has the shortest men? Would you say that the distribution of heights within a country is similar for all countries?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\nh_slope &lt;- heights |&gt;\n  add_n_obs() |&gt;\n  filter(n_obs &gt; 4) |&gt;\n  add_key_slope(height_cm ~ year) \n\nh_slope |&gt;\n  filter(.slope_year &gt; 0.1) |&gt;\n    ggplot(aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() \n\n\n\n\n\n\n\n\n\nCode\nh_slope |&gt;\n  filter(.slope_year &lt; 0 ) |&gt;\n    ggplot(aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() \n\n\n\n\n\n\n\n\n\nCode\nlibrary(plotly)\nggplotly()\n\n\n\n\n\n\nDenmark has the tallest man (max). Estonia has the tallest median height. Papua New Guinea has the shortest men, on all metrics. The distribution of heights over the years is not the same for each country."
  },
  {
    "objectID": "week9/slides.html#outline",
    "href": "week9/slides.html#outline",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "What is temporal data?\nWhat is exploratory temporal data analysis?\nUsing temporal objects in R: tsibble\nData wrangling: aggregation, creating temporal components, missing values\nPlotting conventions: connect the dots; aspect ratio, landscape or portrait\nCalendar plots: arranging daily records into a calendar format\nVisual inference for temporal data\ntignostics: cognostics for temporal data\nInteractive graphics for temporal data\nExploring longitudinal data, with the brolgar package"
  },
  {
    "objectID": "week9/slides.html#philosophy",
    "href": "week9/slides.html#philosophy",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Philosophy",
    "text": "Philosophy\n\n\n\nTime series analysis is what you do after all the interesting stuff has been done!\n\nHeike Hofmann, 2005\n\n\n\nTime series analysis focuses on modeling the temporal dependence. Data needs to have trend, seasonality, anomalies removed first.\n\nExploratory temporal analysis involves exploring and discovering temporal trend, patterns related to seasons, and anomalies. And possibly also unusual temporal dependence."
  },
  {
    "objectID": "week9/slides.html#what-is-temporal-data",
    "href": "week9/slides.html#what-is-temporal-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is temporal data?",
    "text": "What is temporal data?\n\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal data has date/time/ordering index variable, call it time.\nA time variable has special structure:\n\nit can have cyclical patterns, eg seasonality (summer, winter), an over in cricket\nthe cyclical patterns can be nested, eg postcode within state, over within innings\n\nMeasurements are also NOT independent - yesterday may influence today.\nIt still likely has non-cyclical patterns, trends and associations with other variables, eg temperature increasing over time, over is bowled by Elise Perry or Sophie Molineaux"
  },
  {
    "objectID": "week9/slides.html#tsibble-r-temporal-object",
    "href": "week9/slides.html#tsibble-r-temporal-object",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "tsibble: R temporal object",
    "text": "tsibble: R temporal object\n The tsibble package provides a data infrastructure for tidy temporal data with wrangling tools. Adapting the tidy data principles, tsibble is a data- and model-oriented object. In tsibble:\n\nIndex is a variable with inherent ordering from past to present.\nKey is a set of variables that define observational units over time.\nEach observation should be uniquely identified by index and key.\nEach observational unit should be measured at a common interval, if regularly spaced."
  },
  {
    "objectID": "week9/slides.html#regular-vs-irregular",
    "href": "week9/slides.html#regular-vs-irregular",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Regular vs irregular",
    "text": "Regular vs irregular\n\n\nThe Melbourne pedestrian sensor data has a regular period. Counts are provided for every hour, at numerous locations.\n\noptions(width=55)\npedestrian \n\n# A tsibble: 66,037 x 5 [1h] &lt;Australia/Melbourne&gt;\n# Key:       Sensor [4]\n   Sensor    Date_Time           Date        Time Count\n   &lt;chr&gt;     &lt;dttm&gt;              &lt;date&gt;     &lt;int&gt; &lt;int&gt;\n 1 Birrarun… 2015-01-01 00:00:00 2015-01-01     0  1630\n 2 Birrarun… 2015-01-01 01:00:00 2015-01-01     1   826\n 3 Birrarun… 2015-01-01 02:00:00 2015-01-01     2   567\n 4 Birrarun… 2015-01-01 03:00:00 2015-01-01     3   264\n 5 Birrarun… 2015-01-01 04:00:00 2015-01-01     4   139\n 6 Birrarun… 2015-01-01 05:00:00 2015-01-01     5    77\n 7 Birrarun… 2015-01-01 06:00:00 2015-01-01     6    44\n 8 Birrarun… 2015-01-01 07:00:00 2015-01-01     7    56\n 9 Birrarun… 2015-01-01 08:00:00 2015-01-01     8   113\n10 Birrarun… 2015-01-01 09:00:00 2015-01-01     9   166\n# ℹ 66,027 more rows\n\n\n\n\nIn contrast, the US flights data, below, is irregular.\n\noptions(width=55)\nlibrary(nycflights13)\nflights_ts &lt;- flights |&gt;\n  mutate(dt = ymd_hm(paste(paste(year, month, day, sep=\"-\"), \n                           paste(hour, minute, sep=\":\")))) |&gt;\n  as_tsibble(index = dt, key = c(origin, dest, carrier, tailnum), regular = FALSE)\nflights_ts \n\n# A tsibble: 336,776 x 20 [!] &lt;UTC&gt;\n# Key:       origin, dest, carrier, tailnum [52,807]\n    year month   day dep_time sched_dep_time dep_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1    30     2224           2000       144\n 2  2013     2    17     2012           2010         2\n 3  2013     2    26     2356           2000       236\n 4  2013     3    13     1958           2005        -7\n 5  2013     5    16     2214           2000       134\n 6  2013     5    30     2045           2000        45\n 7  2013     9    11     2254           2159        55\n 8  2013     9    12       NA           2159        NA\n 9  2013     9     8     2156           2159        -3\n10  2013     1    26     1614           1620        -6\n# ℹ 336,766 more rows\n# ℹ 14 more variables: arr_time &lt;int&gt;,\n#   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;,\n#   carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,\n#   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,\n#   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,\n#   time_hour &lt;dttm&gt;, dt &lt;dttm&gt;\n\n\n\n\n\n\nIs pedestrian traffic really regular?"
  },
  {
    "objectID": "week9/slides.html#getting-started",
    "href": "week9/slides.html#getting-started",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Getting started",
    "text": "Getting started\n\nWrangling prior to analysing temporal data includes:\n\naggregate by temporal unit.\nconstruct temporal units to study seasonality, such as month, week, day of the week, quarter, …\nchecking and imputing missings.\n\n\n For the airlines data, you can aggregate by multiple quantities, eg number of arrivals, departures, average hourly arrival delay and departure delays."
  },
  {
    "objectID": "week9/slides.html#aggregating",
    "href": "week9/slides.html#aggregating",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Aggregating",
    "text": "Aggregating\n\n\nThe US flights data already has some temporal components created, so aggregating by these is easy. Here is departure delay.\n\n\nCode\nflights_mth &lt;- flights_ts |&gt; \n  as_tibble() |&gt;\n  group_by(month, origin) |&gt;\n  summarise(dep_delay = mean(dep_delay, na.rm=TRUE)) |&gt;\n  as_tsibble(key=origin, index=month)\nggplot(flights_mth, aes(x=month, y=dep_delay, colour=origin)) +\n  geom_point() +\n  geom_smooth(se=F) +\n  scale_x_continuous(\"\", breaks = seq(1, 12, 1), \n                     labels=c(\"J\",\"F\",\"M\",\"A\",\"M\",\"J\",\n                              \"J\",\"A\",\"S\",\"O\",\"N\",\"D\")) +\n  scale_y_continuous(\"av dep delay (mins)\", limits=c(0, 25)) +\n  theme(aspect.ratio = 0.5)\n\n\n\n\n\n\n\n\n\n\nAggregate by month, but examine arrival delays.\n\n\nCode\nflights_mth_arr &lt;- flights_ts |&gt; \n  as_tibble() |&gt;\n  group_by(month, origin) |&gt;\n  summarise(arr_delay = mean(arr_delay, na.rm=TRUE)) |&gt;\n  as_tsibble(key=origin, index=month)\nggplot(flights_mth_arr, aes(x=month, y=arr_delay, colour=origin)) +\n  geom_point() +\n  geom_smooth(se=F) +\n  scale_x_continuous(\"\", breaks = seq(1, 12, 1), \n                     labels=c(\"J\",\"F\",\"M\",\"A\",\"M\",\"J\",\n                              \"J\",\"A\",\"S\",\"O\",\"N\",\"D\")) +\n  scale_y_continuous(\"av arr delay (mins)\", limits=c(0, 25)) +\n  theme(aspect.ratio = 0.5)"
  },
  {
    "objectID": "week9/slides.html#constructing-temporal-units",
    "href": "week9/slides.html#constructing-temporal-units",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Constructing temporal units",
    "text": "Constructing temporal units\n\n\nWeek day vs weekend would be expected to have different patterns of delay, but this is not provided.\n\n\nCode\nflights_wk &lt;- flights_ts |&gt; \n  as_tibble() |&gt;\n  mutate(wday = wday(dt, label=TRUE, week_start = 1)) |&gt;\n  group_by(wday, origin) |&gt;\n  summarise(dep_delay = mean(dep_delay, na.rm=TRUE)) |&gt;\n  mutate(weekend = ifelse(wday %in% c(\"Sat\", \"Sun\"), \"yes\", \"no\")) |&gt;\n  as_tsibble(key=origin, index=wday)\nggplot(flights_wk, aes(x=wday, y=dep_delay, fill=weekend)) +\n  geom_col() +\n  facet_wrap(~origin, ncol=1, scales=\"free_y\") +\n  xlab(\"\") +\n  ylab(\"av dep delay (mins)\") +\n  theme(aspect.ratio = 0.5, legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nBe careful of times!\n\n\nCode\nflights_airtm &lt;- flights |&gt;\n  mutate(dep_min = dep_time %% 100,\n         dep_hr = dep_time %/% 100,\n         arr_min = arr_time %% 100,\n         arr_hr = arr_time %/% 100) |&gt;\n  mutate(dep_dt = ymd_hm(paste(paste(year, month, day, sep=\"-\"), \n                           paste(dep_hr, dep_min, sep=\":\")))) |&gt;\n  mutate(arr_dt = ymd_hm(paste(paste(year, month, day, sep=\"-\"), \n                           paste(arr_hr, arr_min, sep=\":\")))) |&gt;\n  mutate(air_time2 = as.numeric(difftime(arr_dt, dep_dt)))\n\nfp &lt;- flights_airtm |&gt; \n  sample_n(3000) |&gt;\n  ggplot(aes(x=air_time, y=air_time2, label = paste(origin, dest))) + \n    geom_abline(intercept=0, slope=1) +\n    geom_point()\nggplotly(fp, width=500, height=500)\n\n\n\n\n\n\nWhy is this not what we expect?"
  },
  {
    "objectID": "week9/slides.html#checking-and-filling-missings-14",
    "href": "week9/slides.html#checking-and-filling-missings-14",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Checking and filling missings (1/4)",
    "text": "Checking and filling missings (1/4)\n\n\n\nset.seed(328)\nharvest &lt;- tsibble(\n  year = c(2010, 2011, 2013, 2011, \n           2012, 2013),\n  fruit = rep(c(\"kiwi\", \"cherry\"), \n              each = 3),\n  kilo = sample(1:10, size = 6),\n  key = fruit, index = year\n)\nharvest\n\n# A tsibble: 6 x 3 [1Y]\n# Key:       fruit [2]\n   year fruit   kilo\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  2011 cherry     2\n2  2012 cherry     7\n3  2013 cherry     1\n4  2010 kiwi       6\n5  2011 kiwi       5\n6  2013 kiwi       8\n\n\n\n\nhas_gaps(harvest, .full = TRUE) \n\n# A tibble: 2 × 2\n  fruit  .gaps\n  &lt;chr&gt;  &lt;lgl&gt;\n1 cherry TRUE \n2 kiwi   TRUE \n\n\n Can you see the gaps in time?\nBoth levels of the key have missings."
  },
  {
    "objectID": "week9/slides.html#checking-and-filling-missings-24",
    "href": "week9/slides.html#checking-and-filling-missings-24",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Checking and filling missings (2/4)",
    "text": "Checking and filling missings (2/4)\n\n\n\n\n# A tsibble: 6 x 3 [1Y]\n# Key:       fruit [2]\n   year fruit   kilo\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  2011 cherry     2\n2  2012 cherry     7\n3  2013 cherry     1\n4  2010 kiwi       6\n5  2011 kiwi       5\n6  2013 kiwi       8\n\n\n\n\ncount_gaps(harvest,  .full=TRUE)\n\n# A tibble: 2 × 4\n  fruit  .from   .to    .n\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 cherry  2010  2010     1\n2 kiwi    2012  2012     1\n\n\n One missing in each level, although it is a different year.\n \nNotice how tsibble handles this summary so neatly."
  },
  {
    "objectID": "week9/slides.html#checking-and-filling-missings-34",
    "href": "week9/slides.html#checking-and-filling-missings-34",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Checking and filling missings (3/4)",
    "text": "Checking and filling missings (3/4)\n\n\n\n\n# A tsibble: 6 x 3 [1Y]\n# Key:       fruit [2]\n   year fruit   kilo\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  2011 cherry     2\n2  2012 cherry     7\n3  2013 cherry     1\n4  2010 kiwi       6\n5  2011 kiwi       5\n6  2013 kiwi       8\n\n\n\nMake the implicit missing values explicit.\n\nharvest &lt;- fill_gaps(harvest, \n                     .full=TRUE) \nharvest \n\n# A tsibble: 8 x 3 [1Y]\n# Key:       fruit [2]\n   year fruit   kilo\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  2010 cherry    NA\n2  2011 cherry     2\n3  2012 cherry     7\n4  2013 cherry     1\n5  2010 kiwi       6\n6  2011 kiwi       5\n7  2012 kiwi      NA\n8  2013 kiwi       8"
  },
  {
    "objectID": "week9/slides.html#checking-and-filling-missings-44",
    "href": "week9/slides.html#checking-and-filling-missings-44",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Checking and filling missings (4/4)",
    "text": "Checking and filling missings (4/4)\n\n\n\n\n# A tsibble: 6 x 3 [1Y]\n# Key:       fruit [2]\n   year fruit   kilo\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  2011 cherry     2\n2  2012 cherry     7\n3  2013 cherry     1\n4  2010 kiwi       6\n5  2011 kiwi       5\n6  2013 kiwi       8\n\n\n\nWe have already seen na_ma() function, that imputes using a moving average. Alternatively, na_interpolation() uses the previous and next values to impute.\n\nharvest_nomiss &lt;- harvest |&gt; \n  group_by(fruit) |&gt; \n  mutate(kilo = \n    na_interpolation(kilo)) |&gt; \n  ungroup()\nharvest_nomiss \n\n# A tsibble: 6 x 3 [1Y]\n# Key:       fruit [2]\n   year fruit   kilo\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  2011 cherry     2\n2  2012 cherry     7\n3  2013 cherry     1\n4  2010 kiwi       6\n5  2011 kiwi       5\n6  2013 kiwi       8"
  },
  {
    "objectID": "week9/slides.html#plotting-conventions",
    "href": "week9/slides.html#plotting-conventions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Plotting conventions",
    "text": "Plotting conventions"
  },
  {
    "objectID": "week9/slides.html#conventions",
    "href": "week9/slides.html#conventions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Conventions",
    "text": "Conventions\n\nlines: connecting sequential time points reminding the reader that the temporal dependence is important.\naspect ratio: wide or tall? Cleveland, McGill, McGill (1988) argue the average line slope in a line chart should be 45 degrees, which is called banking to 45 degrees. But this is refuted in Talbot, Gerth, Hanrahan (2012) that the conclusion was based on a flawed study. Nevertheless, aspect ratio is an inescapable skill for designing effective plots. For time series, typically a wide aspect ratio is good.\nconventions:\n\ntime on the horizontal axis,\nordering of elements like week day, month. Most software organises by alphabetical order, so this needs to be controlled."
  },
  {
    "objectID": "week9/slides.html#aspect-ratio",
    "href": "week9/slides.html#aspect-ratio",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Aspect ratio",
    "text": "Aspect ratio\n\n📊learnR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs the trend linear or non-linear?\n\nYes, slightly non-linear. We could fit a linear regression model, and examine the residuals to better assess non-linear trend.\n\nIs there a cyclical pattern?\n\nYes, there is a yearly trend.   \n\n\nThis type of data is easy to model, and forecast.\n\n\n\nload(here::here(\"data/CO2_ptb.rda\"))\nCO2.ptb &lt;- CO2.ptb |&gt; \n  filter(year &gt; 1980) |&gt;\n  filter(co2_ppm &gt; 100) # handle missing values\np &lt;- ggplot(CO2.ptb, aes(x=date, y=co2_ppm)) + \n  geom_line(size=1) + xlab(\"\") + ylab(\"CO2 (ppm)\")\np1 &lt;- p + theme(aspect.ratio = 1) + ggtitle(\"1 to 1 (may be useless)\")\np3 &lt;- p + theme(aspect.ratio = 2) + ggtitle(\"tall & skinny:  trend\")\np2 &lt;- ggplot(CO2.ptb, aes(x=date, y=co2_ppm)) + \n  annotate(\"text\", x=2000, y=375, label=\"CO2 at \\n Point Barrow,\\n Alaska\", size=8) + theme_solid()\np4 &lt;- p + \n  scale_x_continuous(\"\", breaks = seq(1980, 2020, 5)) + \n  theme(aspect.ratio = 0.2) + ggtitle(\"short & wide: seasonality\")\ngrid.arrange(p1, p2, p3, p4, layout_matrix= matrix(c(1,2,3,4,4,4), nrow=2, byrow=T))"
  },
  {
    "objectID": "week9/slides.html#calendar-plot",
    "href": "week9/slides.html#calendar-plot",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Calendar plot",
    "text": "Calendar plot"
  },
  {
    "objectID": "week9/slides.html#case-study-nyc-flights-12",
    "href": "week9/slides.html#case-study-nyc-flights-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: NYC flights (1/2)",
    "text": "Case study: NYC flights (1/2)\n\n📊About calendarsWhat do we see?R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDraws the daily data in the layout of a regular calendar\nA wonderful way to get a lot of data into a page\nEasy to examine daily patterns, weekly, monthly patterns\n\n\n\n\nThe daily pattern at JFK is very regular.\nIt is similar for every day of the week, and for every month\nThere is a peak in early flights, a drop around lunchtime and then the number of flights pick up again.\n\nSomething is fishy here. What is it?\n\n\n\nflights_hourly &lt;- flights |&gt;\n  group_by(time_hour, origin) |&gt; \n  summarise(count = n(), \n    dep_delay = mean(dep_delay, \n                     na.rm = TRUE)) |&gt; \n  ungroup() |&gt;\n  as_tsibble(index = time_hour, \n             key = origin) |&gt;\n    mutate(dep_delay = \n    na_interpolation(dep_delay)) \ncalendar_df &lt;- flights_hourly |&gt; \n  filter(origin == \"JFK\") |&gt;\n  mutate(hour = hour(time_hour), \n         date = as.Date(time_hour)) |&gt;\n  filter(year(date) &lt; 2014) |&gt;\n  frame_calendar(x=hour, y=count, date=date, nrow=2) \np1 &lt;- calendar_df |&gt;\n  ggplot(aes(x = .hour, y = .count, group = date)) +\n  geom_line() + theme(axis.line.x = element_blank(),\n                      axis.line.y = element_blank()) +\n  theme(aspect.ratio=0.5)\nprettify(p1, size = 3, label.padding = unit(0.15, \"lines\"))"
  },
  {
    "objectID": "week9/slides.html#case-study-nyc-flights-22",
    "href": "week9/slides.html#case-study-nyc-flights-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: NYC flights (2/2)",
    "text": "Case study: NYC flights (2/2)\n\n📊What do we see?R\n\n\n\n\n\n\n\n\n\n\n\n\n\nDelays are much more interesting to examine\n\nMost days have few delays\nJun and July seem to have more delays\nA few days, sporadically in the year, have big delays\n\nCan you find a reason for one of the days with a big delay?\n\nFrom ChatGPT: As of my last update in September 2021, a significant late-season snowstorm did affect parts of the United States in April 2013, but it was more focused on the Midwest rather than the Northeast where JFK Airport (John F. Kennedy International Airport) is located. The storm impacted states like Minnesota, Wisconsin, and South Dakota, among others, and brought heavy snowfall and icy conditions.\nHowever, weather conditions can have a cascading effect on flight schedules nationwide, so it’s possible that there were some delays at JFK related to this or other weather phenomena.\n\n\n\n\ncalendar_df &lt;- flights_hourly |&gt; \n  filter(origin == \"JFK\") |&gt;\n  mutate(hour = hour(time_hour), \n         date = as.Date(time_hour)) |&gt;\n  filter(year(date) &lt; 2014) |&gt;\n  frame_calendar(x=hour, y=dep_delay, date=date, nrow=2) \np1 &lt;- calendar_df |&gt;\n  ggplot(aes(x = .hour, y = .dep_delay, group = date)) +\n  geom_line() + theme(axis.line.x = element_blank(),\n                      axis.line.y = element_blank()) +\n  theme(aspect.ratio=0.5)\nprettify(p1, size = 3, label.padding = unit(0.15, \"lines\"))"
  },
  {
    "objectID": "week9/slides.html#visual-inference",
    "href": "week9/slides.html#visual-inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference",
    "text": "Visual inference"
  },
  {
    "objectID": "week9/slides.html#temporal-patterns-simulation",
    "href": "week9/slides.html#temporal-patterns-simulation",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Temporal patterns: simulation",
    "text": "Temporal patterns: simulation\n\n\n\n\nCode\np_bourke &lt;- pedestrian |&gt;\n  as_tibble() |&gt;\n  filter(Sensor == \"Bourke Street Mall (North)\",\n         Date &gt;= ymd(\"2015-05-03\"), Date &lt;= ymd(\"2015-05-16\")) |&gt;\n  mutate(date_num = \n    as.numeric(difftime(Date_Time,ymd_hms(\"2015-05-03 00:00:00\"),\n       units=\"hours\"))+11) |&gt; # UTC to AEST\n  mutate(day = wday(Date, label=TRUE, week_start=1)) |&gt;\n  select(date_num, Time, day, Count) |&gt;\n  rename(time = date_num, hour=Time, count = Count)\n# Fit a linear model with categorical hour variable\np_bourke_lm &lt;- glm(count~day+factor(hour), family=\"poisson\", \n  data=p_bourke)\n# Function to simulate from a Poisson\nsimulate_poisson &lt;- function(model, newdata) {\n  lambda_pred &lt;- predict(model, newdata, type = \"response\")\n  rpois(length(lambda_pred), lambda = lambda_pred)\n}\n\nset.seed(436)\npos &lt;- sample(1:12)\np_bourke_lineup &lt;- bind_cols(.sample = rep(pos[1], \n  nrow(p_bourke)), p_bourke[,-2])\nfor (i in 1:11) {\n  new &lt;- simulate_poisson(p_bourke_lm, p_bourke)\n  x &lt;- tibble(time=p_bourke$time, count=new)\n  x &lt;- bind_cols(.sample = rep(pos[i+1], \n         nrow(p_bourke)), x)\n  p_bourke_lineup &lt;- bind_rows(p_bourke_lineup, x)\n}\n\nggplot(p_bourke_lineup,\n  aes(x=time, y=count)) + \n  geom_line() +\n  facet_wrap(~.sample, ncol=4) +\n  theme(aspect.ratio=0.5, \n        axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nDecide on a model\nSimulate from the model to generate nulls"
  },
  {
    "objectID": "week9/slides.html#association-permutation",
    "href": "week9/slides.html#association-permutation",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Association: permutation",
    "text": "Association: permutation\n\n\n\n\nCode\nset.seed(514)\nggplot(lineup(null_permute(\"origin\"), true=flights_mth, n=12), \n       aes(x=month, y=dep_delay, colour=origin)) +\n  geom_point() +\n  geom_smooth(se=F) +\n  facet_wrap(~.sample, ncol=4) +\n  theme(aspect.ratio = 0.5, \n        legend.position = \"none\",\n        axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\n\n\nBreak association between variables. Here origin is permuted which breaks association with dep_delay, while keeping month fixed. \nWhich plot has the biggest difference between the three groups?"
  },
  {
    "objectID": "week9/slides.html#tignostics",
    "href": "week9/slides.html#tignostics",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Tignostics",
    "text": "Tignostics"
  },
  {
    "objectID": "week9/slides.html#feasts-time-series-features",
    "href": "week9/slides.html#feasts-time-series-features",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "feasts: time series features",
    "text": "feasts: time series features\n\n\n The feasts package provides functions to calculate tignostics for time series.\nRemember scagnostics? \nCompute tignostics for each series, for example,\n\ntrend\nseasonality\nlinearity\nspikiness\npeak\ntrough\n\n\n\n\nCode\ntourism_feat &lt;- tourism |&gt;\n  features(Trips, feat_stl)\ntourism_feat |&gt;\n  ggplot(aes(x = trend_strength, y = seasonal_strength_year)) +\n  geom_point()"
  },
  {
    "objectID": "week9/slides.html#interactivity",
    "href": "week9/slides.html#interactivity",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Interactivity",
    "text": "Interactivity"
  },
  {
    "objectID": "week9/slides.html#interactive-exploration-with-tsibbletalk",
    "href": "week9/slides.html#interactive-exploration-with-tsibbletalk",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Interactive exploration with tsibbletalk",
    "text": "Interactive exploration with tsibbletalk\n\n\nCode\ntourism_shared &lt;- tourism |&gt;\n  as_shared_tsibble(spec = (State / Region) * Purpose)\n\ntourism_feat &lt;- tourism_shared |&gt;\n  features(Trips, feat_stl)\n\np1 &lt;- tourism_shared |&gt;\n  ggplot(aes(x = Quarter, y = Trips)) +\n  geom_line(aes(group = Region), alpha = 0.5) +\n  facet_wrap(~ Purpose, scales = \"free_y\") \np2 &lt;- tourism_feat |&gt;\n  ggplot(aes(x = trend_strength, y = seasonal_strength_year)) +\n  geom_point(aes(group = Region))\n  \nsubplot(\n    ggplotly(p1, tooltip = \"Region\", width = 1400, height = 700),\n    ggplotly(p2, tooltip = \"Region\", width = 1200, height = 600),\n    nrows = 1, widths=c(0.5, 0.5), heights=1) |&gt;\n  highlight(dynamic = FALSE)"
  },
  {
    "objectID": "week9/slides.html#wrapping-series",
    "href": "week9/slides.html#wrapping-series",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Wrapping series",
    "text": "Wrapping series\n\n\nPedestrian counts at Bourke St Mall, has a daily seasonality.\nDEMO\n\npp &lt;- p_bourke |&gt;\n        as_tsibble(index = time) |&gt;\n        ggplot(aes(x=time, y=count)) + \n          geom_line() +\n          theme(aspect.ratio=0.5)\n \n  \nui &lt;- fluidPage(tsibbleWrapUI(\"tswrap\"))\nserver &lt;- function(input, output, session) {\n  tsibbleWrapServer(\"tswrap\", pp, period = \"1 day\")\n}\n\nshinyApp(ui, server)\n\n\nFamous data: Lynx\nAnnual numbers of lynx trappings for 1821–1934 in Canada. Almost 10 year cycle. Explore periodicity by wrapping series on itself.\nDEMO\n\nlynx_tsb &lt;- as_tsibble(lynx) |&gt;\n  rename(count = value)\npl &lt;- ggplot(lynx_tsb, \n  aes(x = index, y = count)) +\n  geom_line(size = .2) \n\nui &lt;- fluidPage(\n  tsibbleWrapUI(\"tswrap\"))\nserver &lt;- function(input, output, \n                   session) {\n  tsibbleWrapServer(\"tswrap\", pl, \n       period = \"1 year\")\n}\nshinyApp(ui, server)"
  },
  {
    "objectID": "week9/slides.html#longitudinal-data",
    "href": "week9/slides.html#longitudinal-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Longitudinal data",
    "text": "Longitudinal data"
  },
  {
    "objectID": "week9/slides.html#longitudinal-vs-time-series",
    "href": "week9/slides.html#longitudinal-vs-time-series",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Longitudinal vs time series",
    "text": "Longitudinal vs time series\n\nLongitudinal data tracks the same sample of individuals at different points in time. It often has different lengths and different time points for each individual.\n\n\n\n\n\n\n\n\n\n\nWhen the time points are the same for each individual, it is usually referred to as panel data. When different individuals are measured at each time point, it is called cross-sectional data."
  },
  {
    "objectID": "week9/slides.html#overall-trend",
    "href": "week9/slides.html#overall-trend",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Overall trend",
    "text": "Overall trend\n\n\n\nLog(wages) of 888 individuals, measured at various times in their employment US National Longitudinal Survey of Youth.\n\n\nCode\nwages |&gt;\n  ggplot() +\n    geom_line(aes(x = xp, y = ln_wages, group = id), alpha=0.1) +\n    geom_smooth(aes(x = xp, y = ln_wages), se=F) +\n    xlab(\"years of experience\") +\n    ylab(\"wages (log)\") +\n  theme(aspect.ratio = 0.6)\n\n\n\n\n\n\n\n\n\nWages tend to increase as time in the workforce gets longer, on average.\n\n\n\n\n\nCode\nwages |&gt;\n  ggplot() +\n    geom_line(aes(x = xp, y = ln_wages, group = id), alpha=0.1) +\n    geom_smooth(aes(x = xp, y = ln_wages, \n      group = high_grade, colour = high_grade), se=F) +\n    xlab(\"years of experience\") +\n    ylab(\"wages (log)\") +\n  scale_colour_viridis_c(\"education\") +\n  theme(aspect.ratio = 0.6)\n\n\n\n\n\n\n\n\n\nThe higher the education level achieved, the higher overall wage, on average."
  },
  {
    "objectID": "week9/slides.html#eating-spaghetti",
    "href": "week9/slides.html#eating-spaghetti",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Eating spaghetti",
    "text": "Eating spaghetti\n\n\nbrolgar uses tsibble as the data object, and provides:\n\nsampling individuals\nlongnostics for individuals\ndiagnostics for statistical models\n\n\n\nSample 1Sample 2Sample 3\n\n\n\n\nCode\nset.seed(753)\nwages |&gt;\n  sample_n_keys(size = 10) |&gt; \n  ggplot(aes(x = xp,\n             y = ln_wages,\n             group = id,\n             colour = as.factor(id))) + \n  geom_line() +\n  xlim(c(0,13)) + ylim(c(0, 4.5)) +\n  xlab(\"years of experience\") +\n  ylab(\"wages (log)\") +\n  theme(aspect.ratio = 0.6, legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(749)\nwages |&gt;\n  sample_n_keys(size = 10) |&gt; \n  ggplot(aes(x = xp,\n             y = ln_wages,\n             group = id,\n             colour = as.factor(id))) + \n  geom_line() +\n  xlim(c(0,13)) + ylim(c(0, 4.5)) +\n  xlab(\"years of experience\") +\n  ylab(\"wages (log)\") +\n  theme(aspect.ratio = 0.6, legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(757)\nwages |&gt;\n  sample_n_keys(size = 10) |&gt; \n  ggplot(aes(x = xp,\n             y = ln_wages,\n             group = id,\n             colour = as.factor(id))) + \n  geom_line() +\n  xlim(c(0,13)) + ylim(c(0, 4.5)) +\n  xlab(\"years of experience\") +\n  ylab(\"wages (log)\") +\n  theme(aspect.ratio = 0.6, legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\nFew individuals experience wages like the overall trend."
  },
  {
    "objectID": "week9/slides.html#individual-patterns",
    "href": "week9/slides.html#individual-patterns",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Individual patterns",
    "text": "Individual patterns\n\n\nRemember scagnostics? \nCompute longnostics for each subject, for example,\n\nSlope, intercept from simple linear model\nVariance, standard deviation\nJumps, differences\n\n\n\nIncreasingDecreasingConsistentVolatile\n\n\n\n\nCode\nwages_slope &lt;- wages |&gt;   \n  add_n_obs() |&gt;\n  filter(n_obs &gt; 4) |&gt;\n  add_key_slope(ln_wages ~ xp) |&gt; \n  as_tsibble(key = id, index = xp) \nwages_spread &lt;- wages |&gt;\n  features(ln_wages, feat_spread) |&gt;\n  right_join(wages_slope, by=\"id\")\n\nwages_slope |&gt; \n  filter(.slope_xp &gt; 0.3) |&gt; \n  ggplot(aes(x = xp, \n             y = ln_wages, \n             group = id,\n             colour = factor(id))) + \n  geom_line() +\n  xlim(c(0, 4.5)) +\n  ylim(c(0, 4.5)) +\n  xlab(\"years of experience\") +\n  ylab(\"wages (log)\") +\n  theme(aspect.ratio = 0.6, legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwages_slope |&gt; \n  filter(.slope_xp &lt; (-0.4)) |&gt; \n  ggplot(aes(x = xp, \n             y = ln_wages, \n             group = id,\n             colour = factor(id))) + \n  geom_line() +\n  xlim(c(0, 4.5)) +\n  ylim(c(0, 4.5)) +\n  xlab(\"years of experience\") +\n  ylab(\"wages (log)\") +\n  theme(aspect.ratio = 0.6, legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwages_spread |&gt; \n  filter(sd &lt; 0.1) |&gt; \n  ggplot(aes(x = xp, \n             y = ln_wages, \n             group = id,\n             colour = factor(id))) + \n  geom_line() +\n  xlim(c(0, 12)) +\n  ylim(c(0, 4.5)) +\n  xlab(\"years of experience\") +\n  ylab(\"wages (log)\") +\n  theme(aspect.ratio = 0.6, legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwages_spread |&gt; \n  filter(sd &gt; 0.8) |&gt; \n  ggplot(aes(x = xp, \n             y = ln_wages, \n             group = id,\n             colour = factor(id))) + \n  geom_line() +\n  xlim(c(0, 12)) +\n  ylim(c(0, 4.5)) +\n  xlab(\"years of experience\") +\n  ylab(\"wages (log)\") +\n  theme(aspect.ratio = 0.6, legend.position = \"none\")"
  },
  {
    "objectID": "week9/slides.html#individual-summaries",
    "href": "week9/slides.html#individual-summaries",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Individual summaries",
    "text": "Individual summaries\n\n\nA different style of five number summary: What does average look like? What do extremes look like?\nFind those individuals who are representative of the min, median, maximum, etc of a particular feature, e.g. trend, using keys_near(). This reports the individual who is closest to a particular statistic.\nwages_threenum() returns the three individuals: min, max and closest to the median value, for a particular feature.\nwages_fivenum() returns the five individuals: min, max and closest to the median, Q1 and Q3 values, for a particular feature.\n\n\n\nCode\nwages_fivenum &lt;- wages |&gt;   \n  add_n_obs() |&gt;\n  filter(n_obs &gt; 6) |&gt;\n  key_slope(ln_wages ~ xp) |&gt;\n  keys_near(key = id,\n            var = .slope_xp,\n            funs = l_five_num) |&gt; \n  left_join(wages, by = \"id\") |&gt;\n  as_tsibble(key = id, index = xp) \n  \nwages_fivenum |&gt;\n  ggplot(aes(x = xp,\n             y = ln_wages,\n             group = id)) + \n  geom_line() + \n  ylim(c(0, 4.5)) +\n  facet_wrap(~stat, ncol=3) +\n  xlab(\"years of experience\") +\n  ylab(\"wages (log)\") +\n  theme(aspect.ratio = 0.6, legend.position = \"none\")"
  },
  {
    "objectID": "week9/slides.html#assessing-model-fits",
    "href": "week9/slides.html#assessing-model-fits",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Assessing model fits",
    "text": "Assessing model fits\n\n\nFit a mixed effect model, education as fixed effect, subject random effect using slope.\nSummary of the fit\n\n\nCode\nwages_fit_int &lt;- \n  lmer(ln_wages ~ xp + high_grade + \n         (xp |id), data = wages) \nwages_aug &lt;- wages |&gt;\n  add_predictions(wages_fit_int, \n                  var = \"pred_int\") |&gt;\n  add_residuals(wages_fit_int, \n                var = \"res_int\")\n  \nm1 &lt;- ggplot(wages_aug,\n       aes(x = xp,\n           y = pred_int,\n           group = id)) + \n  geom_line(alpha = 0.2) +\n  xlab(\"years of experience\") +\n  ylab(\"wages (log)\") +\n  theme(aspect.ratio = 0.6)\n  \nm2 &lt;- ggplot(wages_aug,\n       aes(x = pred_int,\n           y = res_int,\n           group = id)) + \n  geom_point(alpha = 0.5) +\n  xlab(\"fitted values\") + ylab(\"residuals\")  \n\nm1 + m2 + plot_layout(ncol=2) \n\n\n\n\n\n\n\n\n\n\nDiagnosing the fit: each individual model\n\n\nCode\nwages_aug |&gt; add_n_obs() |&gt; filter(n_obs &gt; 4) |&gt;\n  sample_n_keys(size = 12) |&gt;\n  ggplot() + \n  geom_line(aes(x = xp, y = pred_int, group = id, \n             colour = factor(id))) + \n  geom_point(aes(x = xp, y = ln_wages, \n                 colour = factor(id))) + \n  facet_wrap(~id, ncol=3)  +\n  xlab(\"Years of experience\") + ylab(\"Log wages\") +\n  theme(aspect.ratio = 0.6, legend.position = \"none\")"
  },
  {
    "objectID": "week9/slides.html#resources",
    "href": "week9/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nWang, Cook, Hyndman (2019) A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data\nWang, Cook, Hyndman, O’Hara-Wild (2019) tsibble\nO’Hara-Wild, Hyndman, Wang (2020). fabletools: Core Tools for Packages in the ‘fable’ Framework\nO’Hara-Wild, Hyndman, Wang (2024). feasts: Feature Extraction and Statistics for Time Series\nTierney, Cook, Prvan (2020) Browse Over Longitudinal Data Graphically and Analytically in R"
  },
  {
    "objectID": "week8/worksheet.html",
    "href": "week8/worksheet.html",
    "title": "ETC5521 Worksheet Week 8",
    "section": "",
    "text": "Exercise 1: Parkinsons\nThis dataset is composed of a range of biomedical voice measurements from 31 people, 23 with Parkinson’s disease (PD). Each column in the table is a particular voice measure, and each row corresponds one of 195 voice recording from these individuals (“name” column). The main aim of the data is to discriminate healthy people from those with PD, according to “status” column which is set to 0 for healthy and 1 for PD.\nThe data is available at The UCI Machine Learning Repository in ASCII CSV format. The rows of the CSV file contain an instance corresponding to one voice recording. There are around six recordings per patient, the name of the patient is identified in the first column. There are 24 variables in the file, including the persons name in column 1.\nThe data are originally analysed in: Max A. Little, Patrick E. McSharry, Eric J. Hunter, Lorraine O. Ramig (2008), ‘Suitability of dysphonia measurements for telemonitoring of Parkinson’s disease’, IEEE Transactions on Biomedical Engineering (to appear).\n\n\nCode\nlibrary(cassowaryr)\n# Load the data\ndata(pk)\n\n\n\nHow many pairwise plots would you need to look at, to look at all of them?\n\n\nCompute several of the scagnostics (monotonic, outlying, clumpy2) for the first five variables of variables, except for name. (Note: We are using just five for computing speed, but the scagnostics could be calculated on all variables.)\n\n\n\nCode\n# Compute the scagnostics on the relevant variables\ns &lt;- calc_scags_wide(pk[,2:5],\n                scags=c(\"outlying\",\"monotonic\",\n                        \"clumpy2\"))\ns\n\n\n\nSort the scagnostics, separately by the values on (i) monotonic (ii) outlying (iii) clumpy2, and plot the pair of variables with the highest values on each.\n\n\nMake an interactive scatterplot matrix. Browse over it to choose other interesting pairs of variables and make the plots.\n\n\nThe scagnostics help us to find interesting associations between pairs of variables. However, the problem here is to detect differences between Parkinsons’ patients and normal patients. How would you go about that? Think about some ideas long the line of scagnostics but look for differences between the two groups.\n\n\nNow try to do this using the scagnostics."
  },
  {
    "objectID": "week8/worksheetsol.html",
    "href": "week8/worksheetsol.html",
    "title": "ETC5521 Worksheet Week 8",
    "section": "",
    "text": "Exercise 1: Parkinsons\nThis dataset is composed of a range of biomedical voice measurements from 31 people, 23 with Parkinson’s disease (PD). Each column in the table is a particular voice measure, and each row corresponds one of 195 voice recording from these individuals (“name” column). The main aim of the data is to discriminate healthy people from those with PD, according to “status” column which is set to 0 for healthy and 1 for PD.\nThe data is available at The UCI Machine Learning Repository in ASCII CSV format. The rows of the CSV file contain an instance corresponding to one voice recording. There are around six recordings per patient, the name of the patient is identified in the first column. There are 24 variables in the file, including the persons name in column 1.\nThe data are originally analysed in: Max A. Little, Patrick E. McSharry, Eric J. Hunter, Lorraine O. Ramig (2008), ‘Suitability of dysphonia measurements for telemonitoring of Parkinson’s disease’, IEEE Transactions on Biomedical Engineering (to appear).\n\n\nCode\nlibrary(cassowaryr)\n# Load the data\ndata(pk)\n\n\n\nHow many pairwise plots would you need to look at, to look at all of them?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThere are 23 numeric variables in the data set, which would require 253 pairwise plots to be made.\n\n\n\n\n\nCompute several of the scagnostics (monotonic, outlying, clumpy2) for the first five variables of variables, except for name. (Note: We are using just five for computing speed, but the scagnostics could be calculated on all variables.)\n\n\n\nCode\n# Compute the scagnostics on the relevant variables\ns &lt;- calc_scags_wide(pk[,2:5],\n                scags=c(\"outlying\",\"monotonic\",\n                        \"clumpy2\"))\ns\n\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\n# A tibble: 6 × 5\n  Var1           Var2         outlying clumpy2 monotonic\n  &lt;fct&gt;          &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 MDVP:Fhi(Hz)   MDVP:Fo(Hz)     0.412   0.756    0.796 \n2 MDVP:Flo(Hz)   MDVP:Fo(Hz)     0.172   0.541    0.324 \n3 MDVP:Flo(Hz)   MDVP:Fhi(Hz)    0.336   0.671    0.0956\n4 MDVP:Jitter(%) MDVP:Fo(Hz)     0.289   0        0.270 \n5 MDVP:Jitter(%) MDVP:Fhi(Hz)    0.541   0.764    0.0978\n6 MDVP:Jitter(%) MDVP:Flo(Hz)    0.430   0        0.407 \n\n\n\n\n\n\n\nSort the scagnostics, separately by the values on (i) monotonic (ii) outlying (iii) clumpy2, and plot the pair of variables with the highest values on each.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\n# Check the results for monotonic\ns %&gt;% \n  select(Var1, Var2, monotonic) %&gt;% \n  arrange(desc(monotonic)) \n\n\n# A tibble: 6 × 3\n  Var1           Var2         monotonic\n  &lt;fct&gt;          &lt;fct&gt;            &lt;dbl&gt;\n1 MDVP:Fhi(Hz)   MDVP:Fo(Hz)     0.796 \n2 MDVP:Jitter(%) MDVP:Flo(Hz)    0.407 \n3 MDVP:Flo(Hz)   MDVP:Fo(Hz)     0.324 \n4 MDVP:Jitter(%) MDVP:Fo(Hz)     0.270 \n5 MDVP:Jitter(%) MDVP:Fhi(Hz)    0.0978\n6 MDVP:Flo(Hz)   MDVP:Fhi(Hz)    0.0956\n\n\nCode\nggplot(data=pk, \n       aes(x=`MDVP:Fhi(Hz)`, y=`MDVP:Fo(Hz)`)) + \n  geom_point() + theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\n\nThe top pair of variables on monotonic has a strong positive association with the majority of points, and a few outliers. The top pair of variables on outlying, is also the top pair on clumpy2, and has outliers with some clumpiness in the mass of points with low values.\n\n\n\n\n\nMake an interactive scatterplot matrix. Browse over it to choose other interesting pairs of variables and make the plots.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\n# Create an interactive splom\ns &lt;- s %&gt;%\n  mutate(vars = paste(Var1, Var2))\nhighlight_key(s) %&gt;%\n  GGally::ggpairs(columns = 3:5, mapping = aes(label = vars)) %&gt;%\n  ggplotly(tooltip = \"all\", \n           width=500, \n           height=500) %&gt;%\n  highlight(\"plotly_selected\") \n\n\n\n\n\n\n\n\n\n\n\nThe scagnostics help us to find interesting associations between pairs of variables. However, the problem here is to detect differences between Parkinsons’ patients and normal patients. How would you go about that? Think about some ideas long the line of scagnostics but look for differences between the two groups.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\n# One way to examine difference between Parkinsons and healthy\npk_med &lt;- pk %&gt;% \n  select(-name) %&gt;% \n  group_by(status) %&gt;%\n  summarise_all(list(median, sd)) %&gt;%\n  pivot_longer(\n    cols=`MDVP:Fo(Hz)_fn1`:`PPE_fn2`,\n    names_to=\"var\", \n    values_to=\"value\") %&gt;%\n  separate(var, c(\"var\",\"stat\"), \"_\") %&gt;%\n  mutate(stat = fct_recode(stat,\n                           \"m\"=\"fn1\",\n                           \"s\"=\"fn2\")) %&gt;%\n  pivot_wider(names_from=stat,\n              values_from=value) %&gt;%\n  group_by(var) %&gt;%\n  summarise(\n    d = (m[status==0]-m[status==1])/sqrt(s[status==0]^2+s[status==1]^2))\npk_med %&gt;% arrange(desc(d)) %&gt;% head()\n\n\n# A tibble: 6 × 2\n  var               d\n  &lt;chr&gt;         &lt;dbl&gt;\n1 MDVP:Fo(Hz)   0.870\n2 HNR           0.647\n3 MDVP:Fhi(Hz)  0.518\n4 MDVP:Flo(Hz)  0.211\n5 NHR          -0.243\n6 MDVP:RAP     -0.356\n\n\nCode\nggplot(pk, aes(x=factor(status), y=`MDVP:Fo(Hz)`)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nGenerally we are looking for variables where the differences between the Parkinsons and normal patients are big. You need to measure big, relative to the variance of each group. Doing a two sample t-test for each variable is one approach. Here, I’ve computed the median for each group of patients and compared the difference in medians relative to the pooled standard deviation in each group.\n\n\n\n\n\nNow try to do this using the scagnostics.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\n# Check the pair of variables already examined\nggplot(data=pk, \n       aes(x=`MDVP:Fhi(Hz)`, \n           y=`MDVP:Fo(Hz)`, \n           colour=factor(status))) + \n  geom_point(alpha=0.5) + \n  scale_colour_discrete_divergingx(palette=\"Zissou 1\") +\n  theme(aspect.ratio = 1,\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compute scagnostics separately for each group and compare differences\npk_0 &lt;- pk |&gt;\n  filter(status == 0)\npk_1 &lt;- pk |&gt;\n  filter(status == 1)\ns_0 &lt;- calc_scags_wide(pk_0[,2:5],\n                scags=c(\"outlying\",\"monotonic\",\n                        \"clumpy2\"))\ns_1 &lt;- calc_scags_wide(pk_1[,2:5],\n                scags=c(\"outlying\",\"monotonic\",\n                        \"clumpy2\"))\ns_0 &lt;- s_0 |&gt;\n  mutate(status = 0)\ns_1 &lt;- s_1 |&gt;\n  mutate(status = 1)\ns &lt;- bind_rows(s_0, s_1)  \ns_wide &lt;- s |&gt;\n  pivot_longer(outlying:monotonic, names_to=\"scag\", values_to=\"value\") |&gt;\n  pivot_wider(names_from = status, values_from = \"value\") |&gt;\n  mutate(d = abs(`1`-`0`)) |&gt;\n  arrange(desc(d))\n\n\n\n\nCode\n# Plot top pair\nggplot(data=pk, \n       aes(x=`MDVP:Jitter(%)`, \n           y=`MDVP:Fo(Hz)`, \n           colour=factor(status))) + \n  geom_point(alpha=0.5) + \n  scale_colour_discrete_divergingx(palette=\"Zissou 1\") +\n  theme(aspect.ratio = 1,\n        legend.title = element_blank())"
  },
  {
    "objectID": "week8/slides.html#outline",
    "href": "week8/slides.html#outline",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Outline",
    "text": "Outline\n\nWhat is high-dimensional data? (If all variables are quantitative)\nExploring relationships between more than two variables\n\nTours - scatterplots of combinations of variables\nMatrix of plots\nParallel coordinates\n\nWhat can be hidden\nAutomating the search for pairwise relationships using scagnostics\nLinking elements of multiple plots\nExploring multiple categorical variables"
  },
  {
    "objectID": "week8/slides.html#flatland",
    "href": "week8/slides.html#flatland",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Flatland",
    "text": "Flatland\n Click here to watch video \nTrailer for “FLATLAND 2: SPHERELAND”. Original book, and movie information at wikipedia"
  },
  {
    "objectID": "week8/slides.html#high-dimensional-shapes-shadows-and-slices",
    "href": "week8/slides.html#high-dimensional-shapes-shadows-and-slices",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "High-dimensional shapes: shadows and slices",
    "text": "High-dimensional shapes: shadows and slices"
  },
  {
    "objectID": "week8/slides.html#low-dimensional-shapes-in-high-dimensions",
    "href": "week8/slides.html#low-dimensional-shapes-in-high-dimensions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Low-dimensional shapes in high-dimensions",
    "text": "Low-dimensional shapes in high-dimensions"
  },
  {
    "objectID": "week8/slides.html#what-is-high-dimensions",
    "href": "week8/slides.html#what-is-high-dimensions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is high-dimensions?",
    "text": "What is high-dimensions?\n\n\n\nWhen all variables are quantitative, an extra variable adds an extra orthogonal axis. It has a name, Euclidean space which dates back to the ancient Greeks."
  },
  {
    "objectID": "week8/slides.html#features-to-find",
    "href": "week8/slides.html#features-to-find",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Features to find",
    "text": "Features to find\n\n\n\n\n\n\n\nFeature\nExample\nDescription\n\n\n\n\nlinear form\n\nThe shape is linear\n\n\nnonlinear form\n\nThe shape is more of a curve\n\n\noutliers\n\nThere are one or more points that do not fit the pattern on the others\n\n\nclusters\n\nThe observations group into multiple clumps\n\n\ngaps\n\nThere is a gap, or gaps, but its not clumped\n\n\nbarrier\n\nThere is combination of the variables which appears impossible\n\n\nl-shape\n\nWhen one variable changes the other is approximately constant\n\n\ndiscreteness\n\nRelationship between two variables is different from the overall, and observations are in a striped pattern\n\n\n\n\n\n\nAny of the features from 2D are patterns to find in higher dimensions."
  },
  {
    "objectID": "week8/slides.html#a-movie-of-linear-combinations-tour",
    "href": "week8/slides.html#a-movie-of-linear-combinations-tour",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "A movie of linear combinations: tour",
    "text": "A movie of linear combinations: tour"
  },
  {
    "objectID": "week8/slides.html#grand-tour",
    "href": "week8/slides.html#grand-tour",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Grand tour",
    "text": "Grand tour\n\n\n\n\ndata-processing\n#library(palmerpenguins)\nf_std &lt;- function(x) (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)\np_std &lt;- penguins |&gt;\n  select(bill_len:body_mass, species) |&gt;\n  dplyr::rename(bl = bill_len,\n         bd = bill_dep,\n         fl = flipper_len,\n         bm = body_mass) |&gt;\n  na.omit() |&gt;\n  dplyr::mutate(bl = f_std(bl),\n         bd = f_std(bd),\n         fl = f_std(fl),\n         bm = f_std(bm))\n\n\n\n\nCode\nanimate_xy(p_std[,1:4], axes=\"off\")\nrender_gif(p_std[,1:4], grand_tour(), display_xy(axes=\"off\"),\n           gif_file = \"images/penguins_grand.gif\",\n           frames = 50,\n           start = basis_random(4,2))\n\n\n\nHow many clusters?\n\n\n\n\nCode\nanimate_xy(p_std[,1:4], axes=\"off\", col=p_std$species)\nrender_gif(p_std[,1:4], grand_tour(),\n           display_xy(col=p_std$species, axes=\"off\"),\n           gif_file = \"images/penguins_grand_sp.gif\",\n           frames = 50,\n           start = basis_random(4,2))\n\n\n\nThe clusters correspond the three species."
  },
  {
    "objectID": "week8/slides.html#what-does-linear-combination-of-variables-mean",
    "href": "week8/slides.html#what-does-linear-combination-of-variables-mean",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What does linear combination of variables mean?",
    "text": "What does linear combination of variables mean?\n Click to see demo \n If your data values are -0.88, 0.78, -1.42, -0.56 and the coefficients are 0.23, -0.63, 0.67, -0.31 then the projected data value is -1.47. It’s like a regression equation.\n To make a scatterplot, two linear combinations are used. With special care: (1) the sum of square of each equals 1, and (2) the sum of the product of the two linear combinations equals 0."
  },
  {
    "objectID": "week8/slides.html#guided-tour",
    "href": "week8/slides.html#guided-tour",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Guided tour",
    "text": "Guided tour\n\n\n\n\nCode\nset.seed(815)\nproj &lt;- animate_xy(p_std[,1:4], \n           guided_tour(lda_pp(p_std$species)),\n           start = basis_random(4,2),\n           axes=\"bottomleft\", col=p_std$species)\nbest_proj &lt;- proj$basis[314][[1]]\ncolnames(best_proj) &lt;- c(\"PP1\", \"PP2\")\nrownames(best_proj) &lt;- colnames(p_std[,1:4])\nset.seed(815)\nrender_gif(p_std[,1:4], \n           guided_tour(lda_pp(p_std$species)),\n           display_xy(col=p_std$species, axes=\"bottomleft\"),\n           gif_file = \"images/penguins_guided.gif\",\n           frames = 50,\n           start = basis_random(4,2),\n           loop = FALSE)\n\n\n\n\n Define what structure is interesting, numerically, calculated by a function. Use an optimiser to choose linear combinations that maximise this function. \nMore on creating functions defining interesting structure soon!"
  },
  {
    "objectID": "week8/slides.html#manualradial-tour",
    "href": "week8/slides.html#manualradial-tour",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Manual/radial tour",
    "text": "Manual/radial tour\n\n\n\n\nCode\nset.seed(829)\nanimate_xy(p_std[,1:4], \n           radial_tour(best_proj, mvar=3),\n           axes=\"bottomleft\", col=p_std$species)\n# Generate a path that shows multiple variables being rotated ou\nset.seed(829)\np_rad_fl &lt;- save_history(p_std[,1:4], \n                      radial_tour(best_proj, mvar=3),\n                      max_bases = 3)\np_rad_bd &lt;- save_history(p_std[,1:4], \n                      radial_tour(best_proj, mvar=2),\n                      max_bases = 3)\np_rad_bl &lt;- save_history(p_std[,1:4], \n                      radial_tour(best_proj, mvar=1),\n                      max_bases = 3)\np_rad &lt;- array(dim = c(4, 2, 7))\np_rad[,,1:3] &lt;- p_rad_fl\np_rad[,,4:5] &lt;- p_rad_bd[,,2:3]\np_rad[,,6:7] &lt;- p_rad_bl[,,2:3]\nclass(p_rad) &lt;- \"history_array\"\nanimate_xy(p_std[,1:4], \n           planned_tour(p_rad),\n           axes=\"bottomleft\", col=p_std$species)\n\nrender_gif(p_std[,1:4], \n           planned_tour(p_rad),\n           display_xy(col=p_std$species, axes=\"bottomleft\"),\n           gif_file = \"images/penguins_radial.gif\",\n           frames = 100)\n\n\n\n\n Remove a variable to see what the change to the pattern is. \nUse this to assess whether a variable is important for a pattern, and hence the relationship between multiple variables."
  },
  {
    "objectID": "week8/slides.html#scale-your-data",
    "href": "week8/slides.html#scale-your-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scale your data!",
    "text": "Scale your data!\n\n\n\nThe scale of the variables can affect how you see the relationships between multiple variables.\n\n\n Generally, each variable should be scaled to have mean 0, standard deviation 1. (Or min -1 and max 1.)\n If different scales on different variables is meaningful, and they are in the same units, you can scale with global mean and standard deviation (or minimum and maximum)."
  },
  {
    "objectID": "week8/slides.html#static-plots-of-multivariate-data",
    "href": "week8/slides.html#static-plots-of-multivariate-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Static plots of multivariate data",
    "text": "Static plots of multivariate data"
  },
  {
    "objectID": "week8/slides.html#simpler-scatterplot-matrix",
    "href": "week8/slides.html#simpler-scatterplot-matrix",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Simpler: scatterplot matrix",
    "text": "Simpler: scatterplot matrix\n\n\n\n\nCode\nggscatmat(p_std, columns=1:4, col=\"species\", alpha=0.5) +\n  scale_color_discrete_divergingx(palette=\"Zissou 1\") +\n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\n\n\n\n Plot\n\nall the pairs of variables.\nunivariate distributions.\nmaybe show correlations, too."
  },
  {
    "objectID": "week8/slides.html#parallel-coordinate-plot",
    "href": "week8/slides.html#parallel-coordinate-plot",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Parallel coordinate plot",
    "text": "Parallel coordinate plot\n\n\n\n\nCode\np_std |&gt;\n  pcp_select(1:5) |&gt;  # select everything\n  pcp_arrange() |&gt;\n  ggplot(aes_pcp(colour=species)) +\n    geom_pcp() + \n    scale_color_discrete_divergingx(palette = \"Zissou 1\") +\n    theme_pcp()\n\n\n\n\n\n\n\n\n\n\nLike side-by-side dot plots, where points are connected.\nLook for patterns in the lines, such as\n\ngrouped together indicating clustering.\nsome lines going in different directions, indicating outliers.\nparallel or crossing lines, indicating linear positive and negative association."
  },
  {
    "objectID": "week8/slides.html#what-you-might-miss-without-a-tour",
    "href": "week8/slides.html#what-you-might-miss-without-a-tour",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What you might miss without a tour",
    "text": "What you might miss without a tour"
  },
  {
    "objectID": "week8/slides.html#hidden-structure-13",
    "href": "week8/slides.html#hidden-structure-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hidden structure (1/3)",
    "text": "Hidden structure (1/3)\n\n\nMany relationships can only be seen when multiple variables are combined:\n\noutlier(s)\nclustering\nnon-linearity\n\n\n\nExample from my experience with early experiments in laser equipment construction."
  },
  {
    "objectID": "week8/slides.html#hidden-structure-23",
    "href": "week8/slides.html#hidden-structure-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hidden structure (2/3)",
    "text": "Hidden structure (2/3)\n\n\n\n\n\n\n\n\n\n\n\nCan you see an anomaly?\n\n\n\n\nCan you see it now?"
  },
  {
    "objectID": "week8/slides.html#hidden-structure-33",
    "href": "week8/slides.html#hidden-structure-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hidden structure (3/3)",
    "text": "Hidden structure (3/3)\n\n\n\n\n\n\n\n\n\n\n\nHow many clusters?\n\n\n\n\nHow many can you see now?"
  },
  {
    "objectID": "week8/slides.html#famous-example-randu",
    "href": "week8/slides.html#famous-example-randu",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Famous example: RANDU",
    "text": "Famous example: RANDU\n\n\nRANDU[1] is a linear congruential pseudorandom number generator (LCG) used primarily in the 1960s and 1970s. Using RANDU for sampling a unit cube will only sample 15 parallel planes. As a result of the wide use of RANDU in the early 1970s, many results from that time are seen as suspicious.\nRead more on wikipedia\n\n\n\nCode\nset.seed(1031)\ndata(randu)\nrandu_std &lt;- as.data.frame(apply(randu, 2, function(x) (x-mean(x))/sd(x)))\nanimate_xy(randu_std, axes=\"off\")\nrender_gif(randu_std, grand_tour(),\n           display_xy(axes=\"bottomleft\"),\n           gif_file = \"images/randu.gif\",\n           frames = 50,\n           start = basis_random(3,2))"
  },
  {
    "objectID": "week8/slides.html#automating-the-search-with-scagnostics",
    "href": "week8/slides.html#automating-the-search-with-scagnostics",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Automating the search with scagnostics",
    "text": "Automating the search with scagnostics"
  },
  {
    "objectID": "week8/slides.html#scagnostics",
    "href": "week8/slides.html#scagnostics",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scagnostics",
    "text": "Scagnostics\n\n\n\n\nCode\ns &lt;- df |&gt;\n  group_by(set) |&gt;\n  summarise(calc_scags(x, y, \n    scags = c(\"outlying\", \"stringy\", \"striated\",\n              \"clumpy\", \"sparse\", \n              \"monotonic\", \"dcor\"))) |&gt;\n  mutate(plot = \"\") |&gt;\n  select(plot, set, outlying, stringy, striated,\n         clumpy, sparse, monotonic, dcor)\n\n\n\n\n\n\n\nplot\nset\noutlying\nstringy\nstriated\nclumpy\nsparse\nmonotonic\ndcor\n\n\n\n\n\nline\n0.000\n1.00\n0.60\n0.37\n0.157\n0.997\n0.99\n\n\n\nnorm\n0.190\n0.79\n0.33\n0.60\n0.095\n0.013\n0.16\n\n\n\ncircle\n0.000\n1.00\n0.98\n0.97\n0.065\n0.009\n0.25\n\n\n\nstripes\n0.129\n0.70\n0.34\n0.98\n0.094\n0.665\n0.63\n\n\n\nclumps\n0.038\n0.61\n0.23\n0.99\n0.107\n0.375\n0.50\n\n\n\n\n\n\n\nclumpy\nconvex\ndcor\nmonotonic\noutlying\nskewed\nskinny\nsparse\nsplines\nstriated\nstringy\nstriped"
  },
  {
    "objectID": "week8/slides.html#how-are-scagnostics-calculated-13",
    "href": "week8/slides.html#how-are-scagnostics-calculated-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How are scagnostics calculated? (1/3)",
    "text": "How are scagnostics calculated? (1/3)\nThe building blocks are: convex hull, alpha hull, and minimal spanning tree\n\n\n\nSketches made by Harriet Mason"
  },
  {
    "objectID": "week8/slides.html#how-are-scagnostics-calculated-23",
    "href": "week8/slides.html#how-are-scagnostics-calculated-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How are scagnostics calculated? (2/3)",
    "text": "How are scagnostics calculated? (2/3)\n\n\nConvex: Measure of how convex the shape of the data is. Computed as the ratio between the area of the alpha hull (A) and convex hull (C).\n\n\nSkinny: A measure of how “thin” the shape of the data is. It is calculated as the ratio between the area and perimeter of the alpha hull (A) with some normalisation such that 0 correspond to a perfect circle and values close to 1 indicate a skinny polygon.\n\n\nSketches made by Harriet Mason"
  },
  {
    "objectID": "week8/slides.html#how-are-scagnostics-calculated-33",
    "href": "week8/slides.html#how-are-scagnostics-calculated-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How are scagnostics calculated? (3/3)",
    "text": "How are scagnostics calculated? (3/3)\n\n\nOutlying: A measure of proportion and severity of outliers in dataset. Calculated by comparing the edge lengths of the outlying points in the MST with the length of the entire MST.\n\n\nStringy: This measure identifies a “stringy” shape with no branches, such as a thin line of data. It is calculated by comparing the number of vertices of degree two \\((V^{(2)})\\) with the total number of vertices \\((V)\\), dropping those of degree one \\((V^{(1)})\\).\n\n\nSketches made by Harriet Mason"
  },
  {
    "objectID": "week8/slides.html#scagnostics-from-familiar-measures",
    "href": "week8/slides.html#scagnostics-from-familiar-measures",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scagnostics from familiar measures",
    "text": "Scagnostics from familiar measures\nThere are many more ways to numerically characterise association that can be used as scagnostics too:\n\nWe used those available in the cassowaryr R package\nSlope, intercept, error, \\(R^2\\) from a simple linear model\nAlso beyond scatterplots there are:\n\ntignostics for time series (feasts R package)\nlongnostics for longitudinal data (brolgar R package)"
  },
  {
    "objectID": "week8/slides.html#linking-elements-of-multiple-plots",
    "href": "week8/slides.html#linking-elements-of-multiple-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linking elements of multiple plots",
    "text": "Linking elements of multiple plots"
  },
  {
    "objectID": "week8/slides.html#brushing-in-a-scatterplot-matrix",
    "href": "week8/slides.html#brushing-in-a-scatterplot-matrix",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Brushing in a scatterplot matrix",
    "text": "Brushing in a scatterplot matrix\n\n\n Selecting points using a square “brush”, using plotly, allows you to see where observations lie in the other plots (pairs of variables).\n\n\n\nCode\nhighlight_key(p_std) |&gt;\n  ggpairs(columns = 1:4) |&gt;\n  ggplotly(width=600, height=600) |&gt;\n  highlight(\"plotly_selected\")"
  },
  {
    "objectID": "week8/slides.html#linking-between-a-tour-and-a-scatterplot",
    "href": "week8/slides.html#linking-between-a-tour-and-a-scatterplot",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linking between a tour and a scatterplot",
    "text": "Linking between a tour and a scatterplot\n\n\nDemo\nlibrary(detourr)\nlibrary(dplyr)\nlibrary(crosstalk)\nlibrary(plotly)\np_all &lt;- penguins |&gt;\n  rename(bl = bill_len,\n         bd = bill_dep,\n         fl = flipper_len,\n         bm = body_mass) |&gt;\n  filter(!is.na(bl)) |&gt;\n  mutate(bl = f_std(bl),\n         bd = f_std(bd),\n         fl = f_std(fl),\n         bm = f_std(bm)) |&gt;\n  mutate(island_i = as.integer(island),\n         sex_i = as.integer(sex)) |&gt;\n  mutate(island_i = jitter(island_i),\n         sex_i = jitter(sex_i)) |&gt;\n  select(bl:bm, species, island_i, sex_i, island, sex)\nshared_p_all &lt;- SharedData$new(p_all)\n\ndetour_plot &lt;- detour(shared_p_all, \n  tour_aes(projection = bl:bm, \n           colour = species)) |&gt;\n    tour_path(grand_tour(2), \n                    max_bases=50, fps = 60) |&gt;\n       show_scatter(alpha = 0.7, axes = FALSE,\n                    width = \"100%\", height = \"450px\",\n                    palette = pal_brewer(type = \"qual\",\n                                         palette = \"Dark2\"))\n\ndemog_plot &lt;- plot_ly(shared_p_all,\n                    x = ~island_i, \n                    y = ~sex_i,\n                    color = ~species,\n                    text = ~island,\n                    colors = \"Dark2\",\n                    height = 450) |&gt;\n    highlight(on = \"plotly_selected\", \n              off = \"plotly_doubleclick\") |&gt;\n    add_trace(type = \"scatter\", \n              mode = \"markers\",\n              hoverinfo = 'text')\n\nbscols(\n     detour_plot, demog_plot,\n     widths = c(5, 6)\n )\n\n\nLinking between plots allows some queries to be made interactively. The penguins data has variables, sex and island which provide more information.\nThis demo code is setup to learn more the penguins: a (jittered) scatterplot shows island and sex, and a tour shows the four size measurements.\n\nWhat can you learn in response to these questions?\n\nIs the size of the penguins different between the sexes?\nHow does the size differ between penguins recorded at different locations?\nWhich island did the outlier come from?"
  },
  {
    "objectID": "week8/slides.html#exploring-multiple-categorical-variables",
    "href": "week8/slides.html#exploring-multiple-categorical-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Exploring multiple categorical variables",
    "text": "Exploring multiple categorical variables"
  },
  {
    "objectID": "week8/slides.html#facetted-plots",
    "href": "week8/slides.html#facetted-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Facetted plots",
    "text": "Facetted plots\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional variables can be folded in to horizontal and vertical facets.\nOrdering could be important to change, and scales on the axes need care.\nSwitching to proportions may allow direct comparison."
  },
  {
    "objectID": "week8/slides.html#tabulation",
    "href": "week8/slides.html#tabulation",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Tabulation",
    "text": "Tabulation\n\n\nOverall counts\n\n\n\n\n\nage\nf\nm\n\n\n\n\n1524\n314\n395\n\n\n2534\n432\n469\n\n\n3544\n216\n345\n\n\n4554\n126\n332\n\n\n5564\n103\n244\n\n\n65\n269\n575\n\n\nTotal\n1460\n2360\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nf\nm\n\n\n\n\n1524\n0.44\n0.56\n\n\n2534\n0.48\n0.52\n\n\n3544\n0.39\n0.61\n\n\n4554\n0.28\n0.72\n\n\n5564\n0.30\n0.70\n\n\n65\n0.32\n0.68\n\n\nTotal\n0.38\n0.62\n\n\n\n\n\nWhat percentage was taken? What is the comparison? What other percentages could be useful?\nThe way percentages are calculated corresponds to conditional distributions, e.g. if age is 15-24 what is the distribution of tuberculosis between the sexes."
  },
  {
    "objectID": "week8/slides.html#famous-example-titanic",
    "href": "week8/slides.html#famous-example-titanic",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Famous example: titanic",
    "text": "Famous example: titanic\nThis data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner “Titanic”, summarized according to economic status (class), sex, age and survival."
  },
  {
    "objectID": "week8/slides.html#mosaic-plots-conditional-distributions",
    "href": "week8/slides.html#mosaic-plots-conditional-distributions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Mosaic plots: conditional distributions",
    "text": "Mosaic plots: conditional distributions\n\n\n\n\nCode\ndoubledecker(Survived ~ ., data = Titanic)\n\n\n\n\n\n\n\n\n\n\nOrder in which variables are entered can affect the conditioning."
  },
  {
    "objectID": "week8/slides.html#fluctuation-diagrams-joint-distributions",
    "href": "week8/slides.html#fluctuation-diagrams-joint-distributions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Fluctuation diagrams: joint distributions",
    "text": "Fluctuation diagrams: joint distributions\n\n\n\n\nCode\nggplot(as_tibble(Titanic), aes(x=interaction(Sex, Age),\n                               y=interaction(Class, Survived), \n                               fill=n)) +\n  geom_tile() +\n  xlab(\"Sex, Age\") +\n  ylab(\"Class, Survived\") +\n  scale_fill_continuous_sequential(palette = \"Terrain\", trans=\"sqrt\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(as_tibble(Titanic), aes(x=interaction(Sex, Age),\n                               y=interaction(Class, Survived), \n                               size=n)) +\n  geom_point(shape=15) +\n  xlab(\"Sex, Age\") +\n  ylab(\"Class, Survived\") +\n  scale_size(transform = \"sqrt\", range=c(1,15))\n\n\n\n\n\n\n\n\n\n\nOverall counts mapped to tiling colour or size. Big counts completely dominate."
  },
  {
    "objectID": "week8/slides.html#resources",
    "href": "week8/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nCook and Laa (2023) Interactively exploring high-dimensional data and models in R\nWickham et al (2011). tourr: An R Package for Exploring Multivariate Data with Projections\nSievert (2019) Interactive web-based data visualization with R, plotly, and shiny\nMason, Lee, Laa, and Cook (2022). cassowaryr: Compute Scagnostics on Pairs of Numeric Variables in a Data Set"
  },
  {
    "objectID": "week7/worksheet.html",
    "href": "week7/worksheet.html",
    "title": "ETC5521 Worksheet Week 7",
    "section": "",
    "text": "Exercise 1: Discussion\nThe Women’s Weekly published a story about famous Australian model, Elle McPherson’s breast cancer story. Diagnosed 7 years ago, she is in remission after choosing alternative therapies as treatment. The original diagnosis was accompanied by lumpectomy removing the cancerous tissue.\nWhat does data say relative to this statement?\n\nAlternative therapies assisted Elle’s being considered cleared of cancer today.\n\n\n\nExercise 2: Hate Crime\n\nPossibly uncomfortable?Continue\n\n\nIf this topic is upsetting for you, please feel free to take yourself out of the discussion, and exit the workshop.\n\n\nA certain person made the following statement about this data and used the graph below to illustrate his point.\n\nThe post-9/11 upsurge in hate crimes against Muslims was real and unforgivable, but the horrible truth is that it didn’t loom that large compared with what Blacks face year in and year out.\n\n\n\nCode\ndf &lt;- tribble(\n  ~year, ~offense, ~count,\n  2000, \"Anti-Black\", 3535,\n  2000, \"Sexual Orientation\", 1558,\n  2000, \"Anti-Islamic\", 36,\n  2001, \"Anti-Black\", 3700,\n  2001, \"Sexual Orientation\", 1664,\n  2001, \"Anti-Islamic\", 554,\n  2002, \"Anti-Black\", 3076,\n  2002, \"Sexual Orientation\", 1513,\n  2002, \"Anti-Islamic\", 174\n) |&gt;\n  mutate(offense = fct_reorder(offense, -count))\n\npop_df &lt;- tribble(\n  ~pop, ~size,\n  \"Anti-Black\", 36.4e6,\n  \"Sexual Orientation\", 28.2e6,\n  \"Anti-Islamic\", 3.4e6\n)\n\ncrime_df &lt;- left_join(df, pop_df, by = c(\"offense\" = \"pop\")) |&gt;\n  mutate(prop = count / size)\n\n\n\n\n\n\n\nVictims of hate crime in USA in years 2000-2002.\n\n\n\n\nDiscuss whether the plot supports his statement or not. Is his comparison of the number of crimes against Muslim and Blacks fair? What graph would you suggest to make to support/disprove his statement? The data and additional information is provided below.\nThis uses the data from the USA hate crime statistics found here. The number of victims by three particular hate crime is shown in the table below.\n\n\n\n\nThe number of victims by hate crime in the USA. Data sourced from https://ucr.fbi.gov/hate-crime.\n\n\nYear\nOffense\nVictims\n\n\n\n\n2000\nAnti-Black\n3535\n\n\n2000\nSexual Orientation\n1558\n\n\n2000\nAnti-Islamic\n36\n\n\n2001\nAnti-Black\n3700\n\n\n2001\nSexual Orientation\n1664\n\n\n2001\nAnti-Islamic\n554\n\n\n2002\nAnti-Black\n3076\n\n\n2002\nSexual Orientation\n1513\n\n\n2002\nAnti-Islamic\n174\n\n\n\n\n\nThe 2000 USA Census reports that there were a total of 36.4 million people who reported themselves as Black or African American. Weeks (2003) estimated there are 3.4 million Muslims in the USA. The LGBT population is harder to estimate but reports indicate 2-10% of the population so likely below 28.2 million people in the USA."
  },
  {
    "objectID": "week7/worksheetsol.html",
    "href": "week7/worksheetsol.html",
    "title": "ETC5521 Worksheet Week 7",
    "section": "",
    "text": "Exercise 1: Discussion\nThe Women’s Weekly published a story about famous Australian model, Elle McPherson’s breast cancer story. Diagnosed 7 years ago, she is in remission after choosing alternative therapies as treatment. The original diagnosis was accompanied by lumpectomy removing the cancerous tissue.\nWhat does data say relative to this statement?\n\nAlternative therapies assisted Elle’s being considered cleared of cancer today.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThis article has good explanations that clarify missing pieces from the Women’s Weekly article.\nElle was diagnosed with HER2 positive oestrogen receptive intraductal carcinoma. What was not reported was that her cancer was non-invasive.\nIf you read this information you will see the survival rate for localised (non-invasive) is 99%.\n\n\n\n\n\n\nExercise 2: Hate Crime\n\nPossibly uncomfortable?Continue\n\n\nIf this topic is upsetting for you, please feel free to take yourself out of the discussion, and exit the workshop.\n\n\nA certain person made the following statement about this data and used the graph below to illustrate his point.\n\nThe post-9/11 upsurge in hate crimes against Muslims was real and unforgivable, but the horrible truth is that it didn’t loom that large compared with what Blacks face year in and year out.\n\n\n\nCode\ndf &lt;- tribble(\n  ~year, ~offense, ~count,\n  2000, \"Anti-Black\", 3535,\n  2000, \"Sexual Orientation\", 1558,\n  2000, \"Anti-Islamic\", 36,\n  2001, \"Anti-Black\", 3700,\n  2001, \"Sexual Orientation\", 1664,\n  2001, \"Anti-Islamic\", 554,\n  2002, \"Anti-Black\", 3076,\n  2002, \"Sexual Orientation\", 1513,\n  2002, \"Anti-Islamic\", 174\n) |&gt;\n  mutate(offense = fct_reorder(offense, -count))\n\npop_df &lt;- tribble(\n  ~pop, ~size,\n  \"Anti-Black\", 36.4e6,\n  \"Sexual Orientation\", 28.2e6,\n  \"Anti-Islamic\", 3.4e6\n)\n\ncrime_df &lt;- left_join(df, pop_df, by = c(\"offense\" = \"pop\")) |&gt;\n  mutate(prop = count / size)\n\n\n\n\n\n\n\nVictims of hate crime in USA in years 2000-2002.\n\n\n\n\nDiscuss whether the plot supports his statement or not. Is his comparison of the number of crimes against Muslim and Blacks fair? What graph would you suggest to make to support/disprove his statement? The data and additional information is provided below.\nThis uses the data from the USA hate crime statistics found here. The number of victims by three particular hate crime is shown in the table below.\n\n\n\n\nThe number of victims by hate crime in the USA. Data sourced from https://ucr.fbi.gov/hate-crime.\n\n\nYear\nOffense\nVictims\n\n\n\n\n2000\nAnti-Black\n3535\n\n\n2000\nSexual Orientation\n1558\n\n\n2000\nAnti-Islamic\n36\n\n\n2001\nAnti-Black\n3700\n\n\n2001\nSexual Orientation\n1664\n\n\n2001\nAnti-Islamic\n554\n\n\n2002\nAnti-Black\n3076\n\n\n2002\nSexual Orientation\n1513\n\n\n2002\nAnti-Islamic\n174\n\n\n\n\n\nThe 2000 USA Census reports that there were a total of 36.4 million people who reported themselves as Black or African American. Weeks (2003) estimated there are 3.4 million Muslims in the USA. The LGBT population is harder to estimate but reports indicate 2-10% of the population so likely below 28.2 million people in the USA.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nThe use of a line plot rather than bar plot makes it easier to compare the trend across years.\nThe second sentence compares the number of victims of anti-Black hate crimes and of anti-Islamic hate crimes.\nThe problem with this comparison is that the population size is vastly different for the two comparisons.\nWhile the number of anti-Black victims are far larger than anti-Islamic victims as shown in Plot (A) below, the Muslim community is roughly 10% of the size of the Black community.\nAssuming the population size is roughly the same across 2000-2002, a rough estimate of the proportions of hate crime victims for each population is compared in Plot (B).\n\nThe significant surge in anti-Islamic crimes in 2001 is more apparent in Plot (B).\nPlot (C) shows the odds ratio with respect to year 2000. This shows that the anti-Islamic crime in 2001 was nearly 15 times higher than in 2000 lowering to about 4.8 in 2001. This however is higher than that of the incidences related to anti-Black and sexual orientation hate crimes which remain somewhat stable from 2000-2002 (odds ratio is close to 1 or slightly lower).\n\nOnce again, these plots show that the answer to the question (is the quote true), depends on how one interprets the quote. Is the person saying that the number of victims of anti-Black hate crimes is always higher than anti-Islamic hate crimes? Then the answer is likely yes. However, per capita, the rate of anti-Islamic hate crimes far exceeded anything else in 2001, which goes against the quote.\n\n\nCode\nggplot(crime_df, aes(as.factor(year), count, color = offense)) +\n  geom_point() +\n  geom_line(aes(group = offense)) +\n  scale_color_discrete_qualitative() +\n  labs(\n    x = \"Year\", y = \"The number of victims\",\n    color = \"Offense\", tag = \"(A)\"\n  )\n\n\n\n\n\n\n\n\n\nCode\nggplot(crime_df, aes(as.factor(year), prop * 10000, color = offense)) +\n  geom_point() +\n  geom_line(aes(group = offense)) +\n  scale_color_discrete_qualitative() +\n  labs(\n    x = \"Year\", y = \"Incidence estimate per 10,000 people\",\n    color = \"Offense\", tag = \"(B)\"\n  )\n\n\n\n\n\n\n\n\n\nCode\nyear2000dict &lt;- crime_df |&gt;\n  dplyr::filter(year == 2000) |&gt;\n  dplyr::select(offense, prop) |&gt;\n  deframe()\n\ncrime_df |&gt;\n  mutate(rel2000 = prop / year2000dict[offense]) |&gt;\n  dplyr::filter(year != 2000) |&gt;\n  ggplot(aes(as.factor(year), rel2000, color = offense)) +\n  geom_point() +\n  geom_line(aes(group = offense)) +\n  scale_color_discrete_qualitative() +\n  scale_y_continuous(breaks = c(1, 4, 5, 15, 16)) +\n  labs(\n    x = \"Year\", y = \"Odds ratio with respect to year 2000\",\n    color = \"Offense\", tag = \"(C)\"\n  )"
  },
  {
    "objectID": "week7/slides.html#at-the-heart-of-quantitative-reasoning-is-a-single-question-compared-to-what",
    "href": "week7/slides.html#at-the-heart-of-quantitative-reasoning-is-a-single-question-compared-to-what",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "At the heart of quantitative reasoning is a single question: Compared to what?",
    "text": "At the heart of quantitative reasoning is a single question: Compared to what?\n -Edward Tufte"
  },
  {
    "objectID": "week7/slides.html#making-comparisons",
    "href": "week7/slides.html#making-comparisons",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Making comparisons",
    "text": "Making comparisons\n\nGroups defined by strata labelled in categorical variables\nObservations in strata, same or different?\nIs there a baseline, or normal value?\nWhat are the dependencies in the way the data was collected?\nAre multiple samples recorded for the same individual, or recorded on different individuals?"
  },
  {
    "objectID": "week7/slides.html#how-would-you-answer-these-questions",
    "href": "week7/slides.html#how-would-you-answer-these-questions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How would you answer these questions?",
    "text": "How would you answer these questions?\n\nAre housing prices increasing more in Sydney or Melbourne?\n\n\n\nIs the quoted price of the unit/apartment I might buy reasonable, or is it too high?\n\n\n\n\nAre you more at risk of MPox in Australia or Germany?\n\n\n\n\nIs the Alfred or Epworth hospital better for giving birth?\n\n\n\n\nIt’s hot and dry today, is the risk of bushfires too high to go hiking?"
  },
  {
    "objectID": "week7/slides.html#comparing-strata",
    "href": "week7/slides.html#comparing-strata",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Comparing strata",
    "text": "Comparing strata"
  },
  {
    "objectID": "week7/slides.html#case-study-melbournes-daily-maximum-temperature-12",
    "href": "week7/slides.html#case-study-melbournes-daily-maximum-temperature-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Melbourne’s daily maximum temperature (1/2)",
    "text": "Case study: Melbourne’s daily maximum temperature (1/2)\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMelbourne’s daily maximum temperature from 1970 to 2020.\nWhat are the strata in temporal data?\n\n\nHow are the temperatures different across months?\nWhat about the temperature within a month?\n\n\n\n\n\n\ndf9 &lt;- read_csv(here::here(\"data\", \"melb_temp.csv\")) |&gt;\n  janitor::clean_names() |&gt;\n  rename(temp = maximum_temperature_degree_c) |&gt;\n  filter(!is.na(temp)) |&gt;\n  dplyr::select(year, month, day, temp)\nskimr::skim(df9)\n\n── Data Summary ────────────────────────\n                           Values\nName                       df9   \nNumber of rows             18310 \nNumber of columns          4     \n_______________________          \nColumn type frequency:           \n  character                2     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────\n  skim_variable n_missing complete_rate min max empty\n1 month                 0             1   2   2     0\n2 day                   0             1   2   2     0\n  n_unique whitespace\n1       12          0\n2       31          0\n\n── Variable type: numeric ──────────────────────────────────\n  skim_variable n_missing complete_rate   mean    sd     p0\n1 year                  0             1 1995.  14.5  1970  \n2 temp                  0             1   19.9  6.48    5.7\n     p25    p50    p75   p100 hist \n1 1983   1995   2008   2020   ▇▇▇▇▇\n2   14.8   18.6   23.6   46.8 ▃▇▃▁▁\n\n\n\n\n\nggplot(df9, aes(x=month, y=temp)) +\n  geom_violin(draw_quantiles=c(0.25, 0.5, 0.75), fill= \"#56B4E9\") +\n  labs(x = \"month\", y = \"max daily temp (°C)\") +\n  theme(aspect.ratio=0.5)"
  },
  {
    "objectID": "week7/slides.html#case-study-melbournes-daily-maximum-temperature-22",
    "href": "week7/slides.html#case-study-melbournes-daily-maximum-temperature-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Melbourne’s daily maximum temperature (2/2)",
    "text": "Case study: Melbourne’s daily maximum temperature (2/2)\n\n\n Why can we make the comparison across months?\n\nBecause it is the same location, and same years, for each month subset.\n\n\n Is some variation in temperature each month due to changing climate?\nHow would you check this?\n\n\n\n\n\nCode\ndf9 |&gt;\n  group_by(year, month) |&gt;\n  summarise(temp = mean(temp)) |&gt;\n  ggplot(aes(x=year, y=temp)) +\n  geom_point(alpha=0.5) +\n  geom_smooth(se=F) + \n  facet_wrap(~month, ncol=4, scales=\"free_y\") +\n  scale_x_continuous(\"year\", breaks=seq(1970, 2020, 20)) +\n  ylab(\"max daily temp (°C)\") +\n  theme(aspect.ratio=0.7)\n\n\n\n\n\n\n\n\n\nWhat is scales=\"free_y\" for?"
  },
  {
    "objectID": "week7/slides.html#case-study-olive-oils-14",
    "href": "week7/slides.html#case-study-olive-oils-14",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: olive oils (1/4)",
    "text": "Case study: olive oils (1/4)\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe olive oil data consists of the percentage composition of 8 fatty acids (palmitic, palmitoleic, stearic, oleic, linoleic, linolenic, arachidic, eicosenoic) found in the lipid fraction of 572 Italian olive oils.\nThere are 9 collection areas, 4 from southern Italy (North and South Apulia, Calabria, Sicily), two from Sardinia (Inland and Coastal) and 3 from northern Italy (Umbria, East and West Liguria).\n\n\n\n\n\ndata(olives, package = \"classifly\")\ndf2 &lt;- olives |&gt;\n  mutate(Region = factor(Region, labels = c(\"South\", \"Sardinia\", \"North\")))\n\nskimr::skim(df2)\n\n── Data Summary ────────────────────────\n                           Values\nName                       df2   \nNumber of rows             572   \nNumber of columns          10    \n_______________________          \nColumn type frequency:           \n  factor                   2     \n  numeric                  8     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 Region                0             1 FALSE          3\n2 Area                  0             1 FALSE          9\n  top_counts                         \n1 Sou: 323, Nor: 151, Sar: 98        \n2 Sou: 206, Inl: 65, Cal: 56, Umb: 51\n\n── Variable type: numeric ──────────────────────────────────\n  skim_variable n_missing complete_rate   mean    sd   p0\n1 palmitic              0             1 1232.  169.   610\n2 palmitoleic           0             1  126.   52.5   15\n3 stearic               0             1  229.   36.7  152\n4 oleic                 0             1 7312.  406.  6300\n5 linoleic              0             1  981.  243.   448\n6 linolenic             0             1   31.9  13.0    0\n7 arachidic             0             1   58.1  22.0    0\n8 eicosenoic            0             1   16.3  14.1    1\n     p25   p50    p75 p100 hist \n1 1095   1201  1360   1753 ▁▂▇▆▁\n2   87.8  110   169.   280 ▂▇▅▃▁\n3  205    223   249    375 ▂▇▃▁▁\n4 7000   7302. 7680   8410 ▁▇▇▇▁\n5  771.  1030  1181.  1470 ▃▅▃▇▃\n6   26     33    40.2   74 ▂▅▇▂▁\n7   50     61    70    105 ▂▁▇▇▂\n8    2     17    28     58 ▇▃▅▂▁\n\n\n\n\n\ng1 &lt;-\n  df2 |&gt;\n  mutate(Area = fct_reorder(Area, palmitic)) |&gt;\n  ggplot(aes(Area, palmitic, color = Region)) +\n  geom_boxplot() +\n  scale_color_discrete_divergingx(palette=\"Zissou 1\") +\n  guides(color = FALSE, x = guide_axis(n.dodge = 2)) +\n  theme(aspect.ratio=0.5)\n\ng2 &lt;- ggplot(df2, aes(Region, palmitic, color = Region)) +\n  geom_boxplot() +\n  scale_color_discrete_divergingx(palette=\"Zissou 1\") +\n  guides(color = FALSE) +\n  theme(axis.text = element_blank())\n\ng3 &lt;- ggplot(df2, aes(palmitic, color = Region)) +\n  geom_density() +\n  scale_color_discrete_divergingx(palette=\"Zissou 1\") +\n  guides(color = FALSE) +\n  theme(axis.text = element_blank())\n\ng4 &lt;- ggplot(df2, aes(palmitic, color = Region)) +\n  stat_ecdf() +\n  scale_color_discrete_divergingx(palette=\"Zissou 1\") +\n  guides(color = FALSE) +\n  theme(axis.text = element_blank())\n\ng5 &lt;- g2 + g3 + g4 + plot_layout(ncol=3)\n  \ng1 + g5 + plot_layout(ncol=1, heights=c(2,1),\n  guides = \"collect\")"
  },
  {
    "objectID": "week7/slides.html#case-study-olive-oils-24",
    "href": "week7/slides.html#case-study-olive-oils-24",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: olive oils (2/4)",
    "text": "Case study: olive oils (2/4)\n\n📊R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColour is generally good to differentiate strata but if there are too many categories then it becomes hard to compare.\n\n\n\n\nggplot(olives, aes(palmitoleic, palmitic, color = Area)) +\n  geom_point() +\n  scale_color_discrete_divergingx(palette=\"Zissou 1\")"
  },
  {
    "objectID": "week7/slides.html#case-study-olive-oils-34",
    "href": "week7/slides.html#case-study-olive-oils-34",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: olive oils (3/4)",
    "text": "Case study: olive oils (3/4)\n\n📊R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt can be hard to compare across plots, because we need to remember what the previous pattern was when focusing on the new cell.\n\n\n\n\nggplot(olives, aes(palmitoleic, palmitic, color = Area)) +\n  geom_point() +\n  facet_wrap(~Area) +\n  scale_color_discrete_divergingx(palette=\"Zissou 1\") +\n  guides(color = FALSE)"
  },
  {
    "objectID": "week7/slides.html#case-study-olive-oils-44",
    "href": "week7/slides.html#case-study-olive-oils-44",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: olive oils (4/4)",
    "text": "Case study: olive oils (4/4)\n\n📊R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison to all, by putting a shadow of all the data underneath the subset in each cell.\n\n\n\n\nggplot(olives, aes(palmitoleic, palmitic)) +\n  geom_point(data = dplyr::select(olives, -Area), color = \"gray\") +\n  geom_point(aes(color = Area), size=2) +\n  facet_wrap(~Area) +\n  scale_color_discrete_divergingx(palette=\"Zissou 1\") +\n  guides(color = FALSE)"
  },
  {
    "objectID": "week7/slides.html#strata-from-quantitative-variable",
    "href": "week7/slides.html#strata-from-quantitative-variable",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Strata from quantitative variable",
    "text": "Strata from quantitative variable\n\n\nThe coplot divides the numerical variable into chunks, and facets by these. The chunks traditionally we overlapping.\n Becker, Cleveland and Shyu, (1996); Cleveland (1993)\n\n\n\nCode\n# Sizing of figure is difficult, so save it\nlibrary(ggcleveland)\nolives_sard &lt;- df2 |&gt;\n  filter(Region == \"Sardinia\")\np &lt;- gg_coplot(olives_sard,\n  x=arachidic, y=oleic, \n  faceting = linoleic,\n  number_bins = 6, \n  overlap = 1/4) +\n  theme(aspect.ratio=0.5)"
  },
  {
    "objectID": "week7/slides.html#famous-example-trade",
    "href": "week7/slides.html#famous-example-trade",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Famous example: trade",
    "text": "Famous example: trade\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe export from England to the East Indies and the import to England from the East Indies in millions of pounds (A).\nImport and export figures are easier to compare by plotting the difference like in (B).\nRelative difference may be more of an interest: (C) plots the relative difference with respect to the average of export and import values.\nThe red area correspond to War of the Spanish Succession (1701-14), Seven Years’ War (1756-63) and the American Revolutionary War (1775-83).\n\n\n\n\n\ndata(EastIndiesTrade, package = \"GDAdata\")\nskimr::skim(EastIndiesTrade)\n\n── Data Summary ────────────────────────\n                           Values         \nName                       EastIndiesTrade\nNumber of rows             81             \nNumber of columns          3              \n_______________________                   \nColumn type frequency:                    \n  numeric                  3              \n________________________                  \nGroup variables            None           \n\n── Variable type: numeric ──────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd   p0\n1 Year                  0             1 1740   23.5 1700\n2 Exports               0             1  518. 421.   100\n3 Imports               0             1 1005. 320.   460\n   p25  p50  p75 p100 hist \n1 1720 1740 1760 1780 ▇▇▇▇▇\n2  145  370  840 1395 ▇▂▃▂▂\n3  835  975 1000 1550 ▃▃▇▁▅\n\n\n\n\n\ng1 &lt;- ggplot(EastIndiesTrade, aes(Year, Exports)) +\n  annotate(\"rect\",\n    xmin = 1701, xmax = 1714,\n    ymin = -Inf, ymax = Inf,\n    fill = \"red\", alpha = 0.3\n  ) +\n  annotate(\"rect\",\n    xmin = 1756, xmax = 1763,\n    ymin = -Inf, ymax = Inf,\n    fill = \"red\", alpha = 0.3\n  ) +\n  annotate(\"rect\",\n    xmin = 1775, xmax = 1780,\n    ymin = -Inf, ymax = Inf,\n    fill = \"red\", alpha = 0.3\n  ) +\n  geom_line(color = \"#339933\", size = 2) +\n  geom_line(aes(Year, Imports), color = \"red\", size = 2) +\n  geom_ribbon(aes(ymin = Exports, ymax = Imports), fill = \"gray\") +\n  labs(y = \"&lt;span style='color:#339933'&gt;Export&lt;/span&gt;/&lt;span style='color:red'&gt;Import&lt;/span&gt;\", tag = \"(A)\") +\n  theme(aspect.ratio=0.7, axis.title.y = ggtext::element_markdown())\n\ng2 &lt;- ggplot(EastIndiesTrade, aes(Year, Imports - Exports)) +\n  annotate(\"rect\",\n    xmin = 1701, xmax = 1714,\n    ymin = -Inf, ymax = Inf,\n    fill = \"red\", alpha = 0.3\n  ) +\n  annotate(\"rect\",\n    xmin = 1756, xmax = 1763,\n    ymin = -Inf, ymax = Inf,\n    fill = \"red\", alpha = 0.3\n  ) +\n  annotate(\"rect\",\n    xmin = 1775, xmax = 1780,\n    ymin = -Inf, ymax = Inf,\n    fill = \"red\", alpha = 0.3\n  ) +\n  geom_line(size = 2) +\n  labs(tag = \"(B)\") +\n  theme(aspect.ratio=0.7)\n\ng3 &lt;- ggplot(EastIndiesTrade, aes(Year, (Imports - Exports) / (Exports + Imports) * 2)) +\n  annotate(\"rect\",\n    xmin = 1701, xmax = 1714,\n    ymin = -Inf, ymax = Inf,\n    fill = \"red\", alpha = 0.3\n  ) +\n  annotate(\"rect\",\n    xmin = 1756, xmax = 1763,\n    ymin = -Inf, ymax = Inf,\n    fill = \"red\", alpha = 0.3\n  ) +\n  annotate(\"rect\",\n    xmin = 1775, xmax = 1780,\n    ymin = -Inf, ymax = Inf,\n    fill = \"red\", alpha = 0.3\n  ) +\n  geom_line(color = \"#001a66\", size = 2) +\n  labs(y = \"Relative difference\", tag = \"(C)\") +\n  theme(aspect.ratio=0.7)\n\ng1 + g1 + g2 + g3 + plot_layout(ncol=2)"
  },
  {
    "objectID": "week7/slides.html#paired-samples",
    "href": "week7/slides.html#paired-samples",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Paired samples",
    "text": "Paired samples"
  },
  {
    "objectID": "week7/slides.html#pairing-adjusts-for-individual-differences",
    "href": "week7/slides.html#pairing-adjusts-for-individual-differences",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Pairing adjusts for individual differences",
    "text": "Pairing adjusts for individual differences\nIf we were wanting to measure the effect of incorporating a data analytics competition on student learning which is the best design?\n\n\nMETHOD A\n\nDivide students into two groups. Make sure that each group has similar types of students, so both groups are as similar as possible.\nOne group gets an extra traditional assignment, and the other group participates in a data competition.\nEach student takes an exam on the content being taught.\nThe scores are compared using side-by-side boxplots and a two-sample permutation test\n\n\nMETHOD B\n\nEach student takes an exam on the content being taught. We’ll call this their BEFORE score.\nDivide students into two groups. Make sure that each group has similar types of students, so both groups are as similar as possible.\nOne group gets an extra traditional assignment, and the other group participates in a data competition.\nEach student takes an exam on the content being taught. We’ll call this their AFTER score.\nThe difference between the before and after scores are compared using side-by-side boxplots and a two-sample permutation test\n\n\n\nWhat other modifications to the design can you think of?"
  },
  {
    "objectID": "week7/slides.html#case-study-choropleth-vs-hexagon-tile-13",
    "href": "week7/slides.html#case-study-choropleth-vs-hexagon-tile-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: choropleth vs hexagon tile (1/3)",
    "text": "Case study: choropleth vs hexagon tile (1/3)\nThe goal is to demonstrate that the hexagon tile map is better than the choropleth for communicating disease incidence across Australia.\n\n\n\n\n\n\n\n\n\n\n\nThe choropleth fills geographic regions (LGAs, SA2s, …) with colour corresponding to the thyroid cancer relative difference from the overall mean. The hexagons, are also filled this way.\n\nKobakian et al"
  },
  {
    "objectID": "week7/slides.html#case-study-choropleth-vs-hexagon-tile-23",
    "href": "week7/slides.html#case-study-choropleth-vs-hexagon-tile-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: choropleth vs hexagon tile (2/3)",
    "text": "Case study: choropleth vs hexagon tile (2/3)\n\n\n\nEach participant can only see the (same) data once.\nNeed to test for different types of spatial patterns.\nNeed to repeat measure each type of pattern, and each participant.\n\nPairing is done on the data set. Four different data sets used for each pattern.\n\nTrial 1\nParticipant 1\n\nParticipant 2\n\n\n\nTrial 2\nParticipant 1\n\nParticipant 2"
  },
  {
    "objectID": "week7/slides.html#case-study-choropleth-vs-hexagon-tile-33",
    "href": "week7/slides.html#case-study-choropleth-vs-hexagon-tile-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: choropleth vs hexagon tile (3/3)",
    "text": "Case study: choropleth vs hexagon tile (3/3)\n\n\nIgnore the pairing\n\n\nCode\nhstudy &lt;- read_csv(\"https://raw.githubusercontent.com/srkobakian/experiment/master/data/DAT_HexmapPilotData_V1_20191115.csv\")\nhstudy |&gt;\n  filter(trend == \"three cities\") |&gt;\n  ggplot(aes(x=detect)) + geom_bar() + facet_wrap(~type, ncol=2)\n\n\n\n\n\n\n\n\n\nLooks like detection rate about 50-50 for hexagon tile map, which is better than almost zero for choropleth map.\n\n\nAccount for the pairing\n\n\nCode\nhstudy |&gt;\n  filter(trend == \"three cities\") |&gt;\n  select(type, replicate, detect) |&gt;\n  group_by(type, replicate) |&gt;\n  summarise(pdetect = length(detect[detect == 1])/length(detect)) |&gt;\n  ggplot(aes(x=type, y=pdetect)) +\n    geom_point() +\n    geom_line(aes(group=replicate)) +\n    ylim(c(0,1)) +\n    xlab(\"\") +\n    ylab(\"Proportion detected\")\n\n\n\n\n\n\n\n\n\nFor each data set, the hexagon tile map performed better."
  },
  {
    "objectID": "week7/slides.html#normalising",
    "href": "week7/slides.html#normalising",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Normalising",
    "text": "Normalising"
  },
  {
    "objectID": "week7/slides.html#am-i-short",
    "href": "week7/slides.html#am-i-short",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Am I short?",
    "text": "Am I short?\n\n\nI am 165 cms tall.\n\n\n\nCode\nggplot(df22, aes(x=height)) +\n  geom_histogram(breaks = seq(132.5, 205, 5),\n    colour=\"white\") +\n  geom_vline(xintercept = 165, colour=\"#D55E00\",\n    linewidth=2)\n\n\n\n\n\n\n\n\n\n\n\n\nBut, there are strata in humans, so compared to what would be better?\n\n\n\n\nCode\nggplot(df22, aes(x=height)) +\n  geom_histogram(breaks = seq(137.5, 205, 5),\n    colour=\"white\") +\n  geom_vline(xintercept = 165, colour=\"#D55E00\",\n    linewidth=2) +\n  facet_wrap(~sex, ncol=3, scales=\"free_y\")\n\n\n\n\n\n\n\n\n\n\n\nNope, I’m average height."
  },
  {
    "objectID": "week7/slides.html#normalising-1",
    "href": "week7/slides.html#normalising-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Normalising",
    "text": "Normalising\n\n\nWithin each strata convert values to a z-score.\n\\[ z = \\frac{x-\\bar{x}}{s} \\]\n\ndf22 &lt;- df22 |&gt;\n  group_by(sex) |&gt;\n  mutate(zscore = (height -\n    mean(height))/sd(height))\n\n\n\n\n\n\n\n\n\n\n\n\nfemales: \\(\\bar{x}=\\) 164.43, \\(s=\\) 6.41\nmales: \\(\\bar{x}=\\) 174.23, \\(s=\\) 8.71\nunknown: \\(\\bar{x}=\\) 165.74, \\(s=\\) 18.15\n\nMy z-score is 0.09.\n\n\nRob’s height is 170 cms. His z-score is -0.49.\nI am relatively TALLER than Rob."
  },
  {
    "objectID": "week7/slides.html#baselines",
    "href": "week7/slides.html#baselines",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Baselines",
    "text": "Baselines"
  },
  {
    "objectID": "week7/slides.html#relative-to-a-baseline",
    "href": "week7/slides.html#relative-to-a-baseline",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Relative to a baseline",
    "text": "Relative to a baseline\n\n\n\n\nCode\ndata(anorexia, package=\"MASS\")\nggplot(data=anorexia, \n aes(x=Prewt, y=Postwt, \n    colour=Treat)) + \n coord_equal() +\n xlim(c(70, 110)) + ylim(c(70, 110)) +\n xlab(\"Pre-treatment weight (lbs)\") +  \n ylab(\"Post-treatment weight (lbs)\") +\n geom_abline(intercept=0, slope=1,  \n   colour=\"grey80\", linewidth=1.25) + \n geom_density2d() + \n geom_point(size=3) +\n facet_grid(.~Treat) +\n theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nPrimary comparison is before treatment weight, and after treatment weight.\nThree different treatments.\n\nUnwin, Hofmann and Cook (2013)\n\n\n\nCode\nggplot(data=anorexia, \n  aes(x=Prewt, colour=Treat,\n    y=(Postwt-Prewt)/Prewt*100)) + \n  xlab(\"Pre-treatment weight (lbs)\") +  \n  ylab(\"Percent increase in weight\") +\n  geom_hline(yintercept=0, linewidth=1.25, \n    colour=\"grey80\") + \n  geom_point(size=3) +   \n  facet_grid(.~Treat) +\n theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nCompute the difference\nCompare difference relative to before weight\nBefore weight is used as the baseline"
  },
  {
    "objectID": "week7/slides.html#proportions",
    "href": "week7/slides.html#proportions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Proportions",
    "text": "Proportions"
  },
  {
    "objectID": "week7/slides.html#case-study-tuberculosis-13",
    "href": "week7/slides.html#case-study-tuberculosis-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: tuberculosis (1/3)",
    "text": "Case study: tuberculosis (1/3)\n\n\n\nCode\ntb_oz |&gt;\n  ggplot(aes(x=year, y=count, fill=sex)) +\n    geom_col(position=\"fill\") +\n    scale_fill_discrete_divergingx(palette = \"Zissou 1\") +\n    facet_wrap(~age, ncol=6) +\n    xlab(\"\") + ylab(\"proportion\")\n\n\n\n\n\n\n\n\n\n\nPrimary comparison is sex, relative to yearly trend."
  },
  {
    "objectID": "week7/slides.html#case-study-tuberculosis-23",
    "href": "week7/slides.html#case-study-tuberculosis-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: tuberculosis (2/3)",
    "text": "Case study: tuberculosis (2/3)\n\n\n\nCode\ntb_oz |&gt;\n  ggplot(aes(x=year, y=count, fill=age)) +\n    geom_col(position=\"fill\") +\n    scale_fill_discrete_divergingx(palette = \"Zissou 1\") +\n    facet_wrap(~sex, ncol=2) +\n    xlab(\"\") + ylab(\"proportion\")\n\n\n\n\n\n\n\n\n\n\nPrimary comparison is age, relative to yearly trend."
  },
  {
    "objectID": "week7/slides.html#case-study-tuberculosis-33",
    "href": "week7/slides.html#case-study-tuberculosis-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: tuberculosis (3/3)",
    "text": "Case study: tuberculosis (3/3)\n\n\n\nCode\ntb_oz |&gt;\n  ggplot(aes(x=year, y=count, fill=age)) +\n    geom_col() +\n    scale_fill_discrete_divergingx(palette = \"Zissou 1\") +\n    facet_grid(sex~age, scales=\"free\") +\n    xlab(\"\") + ylab(\"count\") +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nPrimary comparison is year trend, separately for age and sex."
  },
  {
    "objectID": "week7/slides.html#inference",
    "href": "week7/slides.html#inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Inference",
    "text": "Inference"
  },
  {
    "objectID": "week7/slides.html#bootstrap-confidence-intervals",
    "href": "week7/slides.html#bootstrap-confidence-intervals",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Bootstrap confidence intervals",
    "text": "Bootstrap confidence intervals\n\n📊dataR\n\n\n\n\n\nConfidence intervals show what might happen to estimates with different samples,\nwith the same dependence structure.\nSample the current sample, but don’t change anything else.\nReason for sampling with replacement, is to keep sample size the same - we know that variance decreases with smaller sample size.\n\n For choropleth vs hexagon tiles, sample participants with replacement.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhstudy |&gt; filter(trend == \"three cities\") |&gt; count(type, replicate)\n\n# A tibble: 8 × 3\n  type      replicate     n\n  &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Geography         9    10\n2 Geography        10    10\n3 Geography        11    11\n4 Geography        12    11\n5 Hexagons          9    11\n6 Hexagons         10    11\n7 Hexagons         11    10\n8 Hexagons         12    10\n\n\n\n\n\nhstudy_sub &lt;- hstudy |&gt;\n  filter(trend == \"three cities\") |&gt;\n  select(id, type, replicate, detect) \n\n# Function to compute proportions\nprop_func &lt;- function(df) {\n  df_smry &lt;- df |&gt;\n    group_by(type, replicate) |&gt;\n    summarise(pdetect = length(detect[detect == 1])/length(detect)) |&gt; \n    ungroup() |&gt;\n    pivot_wider(names_from = c(type, replicate),\n               values_from = pdetect)\n  df_smry\n}\n\nnboots &lt;- 100\nset.seed(1023)\nbsamps &lt;- tibble(samp=\"0\", prop_func(hstudy_sub))\nfor (i in 1:nboots) {\n  samp_id &lt;- sort(sample(unique(hstudy_sub$id),\n    replace=TRUE))\n  hs_b &lt;- NULL\n  for (j in samp_id) {\n    x &lt;- hstudy_sub |&gt;\n      filter(id == j)\n    hs_b &lt;- bind_rows(hs_b, x)\n  }\n  bsamps &lt;- bind_rows(bsamps,\n    tibble(samp=as.character(i), prop_func(hs_b)))\n}\n\nbsamps_long &lt;- bsamps |&gt;\n  pivot_longer(Geography_9:Hexagons_12, \n    names_to = \"treatments\", \n    values_to = \"pdetect\") |&gt; \n  separate(treatments, into=c(\"type\", \"replicate\"))\n\nggplot() +\n    geom_line(data=filter(bsamps_long, samp != \"0\"),\n      aes(x=type, \n          y=pdetect, \n          group=samp),\n     linewidth=0.5, alpha=0.6, colour=\"grey70\") +\n    geom_line(data=filter(bsamps_long, \n                     samp == \"0\"),\n      aes(x=type, \n          y=pdetect, \n          group=samp),\n     linewidth=2) +\n    facet_wrap(~replicate) +\n    ylim(c(0,1)) +\n    xlab(\"\") +\n    ylab(\"Proportion detected\")"
  },
  {
    "objectID": "week7/slides.html#lineups",
    "href": "week7/slides.html#lineups",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Lineups",
    "text": "Lineups\n\n\n\nLineups show what might happen to estimates with null samples, where (by construction) there is no relationship.\nThus you need to break dependence structure.\n\n For choropleth vs hexagon tiles, randomise the type of plot each participant received. This breaks any dependence between type and detection rate.\n\n\n\nCode\nn_nulls &lt;- 11\nset.seed(1110)\nlsamps &lt;- tibble(samp=\"0\", prop_func(hstudy_sub))\nfor (i in 1:n_nulls) {\n\n  hs_b &lt;- hstudy_sub |&gt;\n    group_by(id) |&gt;\n    mutate(type = sample(type)) |&gt;\n    ungroup()\n  lsamps &lt;- bind_rows(lsamps,\n    tibble(samp=as.character(i), prop_func(hs_b)))\n}\n\nlsamps_long &lt;- lsamps |&gt;\n  pivot_longer(Geography_9:Hexagons_12, \n    names_to = \"treatments\", \n    values_to = \"pdetect\") |&gt; \n  separate(treatments, into=c(\"type\", \"replicate\"))\n\nlsamps_long |&gt; ggplot() +\n    geom_line(aes(x=type, \n          y=pdetect, \n          group=replicate)) +\n    facet_wrap(~samp, ncol=4) +\n    ylim(c(0,1)) +\n    xlab(\"\") +\n    ylab(\"Proportion detected\")"
  },
  {
    "objectID": "week7/slides.html#take-aways",
    "href": "week7/slides.html#take-aways",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Take-aways",
    "text": "Take-aways\n\nIn comparison to what is especially important for exploring observational data.\nAvoid reporting spurious associations by accounting for dependencies correctly.\nAdjust for individual variation, by\n\npairing (or multiple repeated measures)\nrelative to a baseline\non a standard scale\n\nDetermining in comparison to what can be hard."
  },
  {
    "objectID": "week7/slides.html#resources",
    "href": "week7/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nWilke (2019) Fundamentals of Data Visualization Chapters 9, 10, 11\nUnwin (2015) Graphical Data Analysis with R Chapter 10"
  },
  {
    "objectID": "week6/worksheet.html",
    "href": "week6/worksheet.html",
    "title": "ETC5521 Worksheet Week 6",
    "section": "",
    "text": "Exercise 1: Fisherman’s Reach crabs\nMud crabs are delicious to eat! Prof Cook’s father started a crab farm at Fisherman’s Reach, NSW, when he retired. He caught small crabs (with a special license) and nurtured and fed the crabs until they were marketable size. They were then sent to market, like Queen Victoria Market in Melbourne, for people to buy to eat. Mud crabs have a strong and nutty flavour, and a good to eat simply after steaming or boiling.\nEarly in the farming setup, he collected the measurements of 62 crabs of different sizes, because he wanted to learn when was the best time to send the crab to market. Crabs re-shell from time to time. They grow too big for their shell, and need to discard it. Ideally, the crabs should be sent to market just before they re-shell, because they will be crab will be fuller in the shell, less air, less juice and more crab meat.\nNote: In NSW it is legal to sell female mud crabs, as long as they are not carrying eggs. In Queensland, it is illegal to keep and sell female mud crabs. Focusing only on males could be worthwhile.\n\n\n\nCode\nfr_crabs &lt;- read_csv(\"https://ddde.numbat.space/data/fr-crab.csv\") %&gt;%\n  mutate(Sex = factor(Sex, levels=c(1,2),\n                      labels=c(\"m\", \"f\")))\n\n\n\nWhere is Fisherman’s Reach? What would you expect the relationship between Length and Weight of a crab to be?\n\n\nMake a scatterplot of Weight by NSW Length. Describe the relationship. It might be even better if you can add marginal density plots to the sides of the scatterplot. (Aside: Should one variable be considered a dependent variable? If so, make sure this is on the \\(y\\) axis.)\n\n\nExamine transformations to linearise the relationship. (Think about why the relationship between Length and Weight is nonlinear.)\n\n\nIs there possibly a lurking variable? Examine the variables in the data, and use colour in the plot to check for another variable explaining some of the relationship.\n\n\nIf you have determined that the is a lurking variable, make changes in the plots to find the best model of the relationship between Weight and Length.\n\n\nHow would you select the crabs that were close to re-shelling based on this data?\n\n\n\nExercise 2: Bank discrimination\n\n\nCode\ndata(case1202, package = \"Sleuth2\")\n\n\n\nLook at the help page for the case1202 from the Sleuth2 package. What does the variable “Senior” measure? “Exper”? Age?\n\n\nMake all the pairwise scatterplots of Senior, Exper and Age. What do you learn about the relationship between these three pairs of variables? How can the age be 600? Are there some wizards or witches or vampires in the data?\n\n\nColour the observations by Sex. What do you learn?\n\n\nInstead of scatterplots, make faceted histograms of the three variables by Sex. What do you learn about the difference in distribution of these three variables between the sexes.\n\n\nThe data also has 1975 salary and annual salary. Plot these two variables, in two ways: (1) coloured by Sex, and (2) faceted by Sex. Explain the relationships.\n\n\nExamine the annual salary against Age, Senior and Exper, separately by Sex, by adding a fitted linear model to the scatterplot where Sex is mapped to colour. What is the relationship and what do you learn?\n\n\nWhen you use geom_smooth(method=\"lm\") to add a fitted model to the scatterplot, is it adding a model with interactions?\n\n\nThere is danger of misinterpreting differences when only examining marginal plots. What we need to know is: for a person with the same age, same experience, same seniority, is the salary different for men and women. How would you make plots to try to examine this?\n\n\nWould you say that this data provides evidence of sex discrimination?"
  },
  {
    "objectID": "week6/worksheetsol.html",
    "href": "week6/worksheetsol.html",
    "title": "ETC5521 Worksheet Week 6",
    "section": "",
    "text": "Exercise 1: Fisherman’s Reach crabs\nMud crabs are delicious to eat! Prof Cook’s father started a crab farm at Fisherman’s Reach, NSW, when he retired. He caught small crabs (with a special license) and nurtured and fed the crabs until they were marketable size. They were then sent to market, like Queen Victoria Market in Melbourne, for people to buy to eat. Mud crabs have a strong and nutty flavour, and a good to eat simply after steaming or boiling.\nEarly in the farming setup, he collected the measurements of 62 crabs of different sizes, because he wanted to learn when was the best time to send the crab to market. Crabs re-shell from time to time. They grow too big for their shell, and need to discard it. Ideally, the crabs should be sent to market just before they re-shell, because they will be crab will be fuller in the shell, less air, less juice and more crab meat.\nNote: In NSW it is legal to sell female mud crabs, as long as they are not carrying eggs. In Queensland, it is illegal to keep and sell female mud crabs. Focusing only on males could be worthwhile.\n\n\n\nCode\nfr_crabs &lt;- read_csv(\"https://ddde.numbat.space/data/fr-crab.csv\") %&gt;%\n  mutate(Sex = factor(Sex, levels=c(1,2),\n                      labels=c(\"m\", \"f\")))\n\n\n\nWhere is Fisherman’s Reach? What would you expect the relationship between Length and Weight of a crab to be?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nNorth coast of NSW, north-east of Kempsey, on the back-water of the Macleay River. We would expect it to be positive, and maybe nonlinear, because weight is more related to volume of the crab than length, which would be length\\(^p\\).\n\n\n\n\n\nMake a scatterplot of Weight by NSW Length. Describe the relationship. It might be even better if you can add marginal density plots to the sides of the scatterplot. (Aside: Should one variable be considered a dependent variable? If so, make sure this is on the \\(y\\) axis.)\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nWgt should be considered the dependent variable.\n\n\nCode\nggplot(fr_crabs, aes(x = Length.NSW, y = Wgt)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nIt is a little nonlinear, positive and strong relationship. Weight should be considered dependent.\nIf you are unsure about a nonlinear relationship, fit a linear model and look at the residuals. In the plots below you can see the residuals have a U-shape, and also have major heteroskedasticity.\n\n\nCode\nlibrary(broom)\nlibrary(patchwork)\ncr_lm &lt;- lm(Wgt ~ Length.NSW, data = fr_crabs)\nfr_crabs &lt;- augment(cr_lm, fr_crabs)\np1 &lt;- ggplot(fr_crabs, \n             aes(x = Length.NSW, \n                 y = Wgt)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\np2 &lt;- ggplot(fr_crabs, aes(x = Length.NSW, y = .resid)) +\n  geom_point()\np1 + p2 + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamine transformations to linearise the relationship. (Think about why the relationship between Length and Weight is nonlinear.)\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\np1 &lt;- ggplot(fr_crabs, aes(x = Length.NSW, y = Wgt^1/3)) +\n  geom_point() +\n  ylab(\"Cube root Wgt\")\np2 &lt;- ggplot(fr_crabs, aes(x = Length.NSW, y = Wgt)) +\n  geom_point() +\n  scale_y_sqrt() +\n  ylab(\"Square root Wgt\")\np3 &lt;- ggplot(fr_crabs, aes(x = Length.NSW, y = Wgt)) +\n  geom_point() +\n  scale_y_log10() +\n  ylab(\"Log10 Wgt\")\np1 + p2 + p3 + plot_layout(ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs there possibly a lurking variable? Examine the variables in the data, and use colour in the plot to check for another variable explaining some of the relationship.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\np1 &lt;- ggplot(fr_crabs, aes(x = Length.NSW, y = Wgt, colour = Sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  theme(legend.position = \"bottom\")\np2 &lt;- ggplot(fr_crabs, aes(x = Length.NSW, y = Wgt, colour = Sex)) +\n  geom_point() +\n  scale_y_log10() +\n  scale_colour_brewer(palette = \"Dark2\") +\n  theme(legend.position = \"bottom\") +\n  ylab(\"Log10 Wgt\")\np1 + p2 + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you have determined that the is a lurking variable, make changes in the plots to find the best model of the relationship between Weight and Length.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nWhen only looking at males, a cube root transformation works best, and this matches the original thinking that length is related to weight in a cubic relationship, perhaps.\n\n\nCode\nfr_crabs_m &lt;- fr_crabs %&gt;%\n  filter(Sex == \"m\")\ncr_m_lm &lt;- lm(Wgt^(1/3) ~ Length.NSW, data = fr_crabs_m)\nfr_crabs_m &lt;- augment(cr_m_lm, fr_crabs_m)\ncoefs &lt;- tidy(cr_m_lm)\n\np1 &lt;- ggplot(fr_crabs_m, \n       aes(x = Length.NSW, \n           y = Wgt^(1/3))) +\n  geom_point() +\n  geom_abline(intercept=coefs$estimate[1], \n              slope=coefs$estimate[2])\np2 &lt;- ggplot(fr_crabs_m, \n       aes(x = Length.NSW, \n           y = .resid)) +\n  geom_point() \np1 + p2 + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow would you select the crabs that were close to re-shelling based on this data?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nYou would select the bigger crabs that are heavier for their length. If they are heavier, it should mean that they are more fully fitting into their shell.\n\n\n\n\n\n\nExercise 2: Bank discrimination\n\n\nCode\ndata(case1202, package = \"Sleuth2\")\n\n\n\nLook at the help page for the case1202 from the Sleuth2 package. What does the variable “Senior” measure? “Exper”? Age?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nSenior is the seniority of the employee in the company. Experience is the months of prior experience when starting at the company. Age is given in months, which is a bit strange!\n\n\n\n\n\nMake all the pairwise scatterplots of Senior, Exper and Age. What do you learn about the relationship between these three pairs of variables? How can the age be 600? Are there some wizards or witches or vampires in the data?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\np1 &lt;- ggplot(case1202, aes(x = Age, y = Senior)) +\n  geom_point() +\n  theme(aspect.ratio = 1)\np2 &lt;- ggplot(case1202, aes(x = Exper, y = Senior)) +\n  geom_point() +\n  theme(aspect.ratio = 1)\np3 &lt;- ggplot(case1202, aes(x = Age, y = Exper)) +\n  geom_point() +\n  theme(aspect.ratio = 1)\np1 + p2 + p3 + plot_layout(ncol = 3)\n\n\n\n\n\n\n\n\n\nExperience and age have a moderate to strong positive association. There is no apparent relationship between seniority and age, or experience.\nIt would be good to check these last two statements using visual inference.\n\n\nCode\nset.seed(956)\nggplot(lineup(null_permute(\"Exper\"), case1202),\n       aes(x = Exper, y = Senior)) +\n  geom_point() + \n  #geom_density2d_filled() +\n  facet_wrap(~.sample) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColour the observations by Sex. What do you learn?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\np1 &lt;- ggplot(case1202, aes(x = Age, y = Senior, colour = Sex)) +\n  geom_point() +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  theme(legend.position = \"bottom\", aspect.ratio = 1) \np2 &lt;- ggplot(case1202, aes(x = Exper, y = Senior, colour = Sex)) +\n  geom_point() +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  theme(legend.position = \"bottom\", aspect.ratio = 1)\np3 &lt;- ggplot(case1202, aes(x = Age, y = Exper, colour = Sex)) +\n  geom_point() +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  theme(legend.position = \"bottom\", aspect.ratio = 1)\np1 + p2 + p3 + plot_layout(ncol = 3)\n\n\n\n\n\n\n\n\n\nIt’s not clear that there are any patterns here. Maybe some small patterns: (1) that at older age and low seniority, there are only female employees, (2) and older age less experience, there are only female employees.\nCheck this with visual inference.\n\n\nCode\nset.seed(659)\nggplot(lineup(null_permute(\"Sex\"), case1202), \n       aes(x = Age, \n           y = Senior, \n           colour = Sex)) +\n  geom_point(alpha=0.8) +\n  #geom_density2d() +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  facet_wrap(~.sample, ncol=5) +\n  theme_bw() +\n  theme(legend.position = \"none\",\n    axis.text = element_blank(),\n    axis.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of scatterplots, make faceted histograms of the three variables by Sex. What do you learn about the difference in distribution of these three variables between the sexes.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\np1 &lt;- ggplot(case1202, aes(x = Senior)) +\n  geom_histogram(binwidth = 5, colour = \"white\") +\n  facet_wrap(~Sex, ncol = 1, scales=\"free_y\")\np2 &lt;- ggplot(case1202, aes(x = Age)) +\n  geom_histogram(binwidth = 50, colour = \"white\") +\n  facet_wrap(~Sex, ncol = 1, scales=\"free_y\")\np3 &lt;- ggplot(case1202, aes(x = Exper)) +\n  geom_histogram(binwidth = 50, colour = \"white\") +\n  facet_wrap(~Sex, ncol = 1, scales=\"free_y\")\np1 + p2 + p3 + plot_layout(ncol = 3)\n\n\n\n\n\n\n\n\n\nThere are lot of different patterns!\nThe distribution of seniority for women is quite uniform, but for men has a clump around 85-90.\nWith age, there is a bimodal pattern for women, young and older - I wonder if there is a child-bearing years drop out among women. With men, there is a large spike, of young men at 350.\nThe experience that employees come into the company with has a different distribution for men and women. For men there is a peak at 50. For women the peak is at 50, too but it has a much stronger right-tail. This suggests that men are being hired into the company with less experience than the women.\n\n\n\n\n\nThe data also has 1975 salary and annual salary. Plot these two variables, in two ways: (1) coloured by Sex, and (2) faceted by Sex. Explain the relationships.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\nggplot(case1202, aes(x = Sal77, \n                     y = Bsal, \n                     colour=Sex)) +\n  geom_point(size=2, alpha=0.8) +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(case1202, aes(x = Sal77, y = Bsal)) +\n  geom_point() +\n  facet_wrap(~Sex) + \n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nThis is where we see a bigger difference: the women generally have lower salaries than men.\n\n\n\n\n\nExamine the annual salary against Age, Senior and Exper, separately by Sex, by adding a fitted linear model to the scatterplot where Sex is mapped to colour. What is the relationship and what do you learn?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\np1 &lt;- ggplot(case1202, aes(x = Age, \n                           y = Bsal, \n                           colour = Sex)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  scale_colour_brewer(\"\", palette = \"Dark2\") \np2 &lt;- ggplot(case1202, aes(x = Senior, \n                           y = Bsal, \n                           colour = Sex)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  scale_colour_brewer(\"\", palette = \"Dark2\") \np3 &lt;- ggplot(case1202, aes(x = Exper, \n                           y = Bsal, \n                           colour = Sex)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  scale_colour_brewer(\"\", palette = \"Dark2\") \np1 + p2 + p3 + \n  plot_layout(ncol = 3, guides = \"collect\") & \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nFor the same age, seniority, experience women were paid less than the men, on average. The annual salary tends to decline with seniority - that’s a bit surprising. With seniority, there is the same decreasing trend, but women are paid a full $1000 less across the range of seniority, on average. The annual salary for women tends to increase with more experience, and is almost equal between the sexes at the most senior levels.\n\n\n\n\n\nWhen you use geom_smooth(method=\"lm\") to add a fitted model to the scatterplot, is it adding a model with interactions?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nTechnically no, but practically yes! ggplot fits a separate model to each subset, which will fit a different slope and intercept. It also fits a different error model to each subset, and this what makes it different from a single model with interaction terms.\n\n\n\n\n\nThere is danger of misinterpreting differences when only examining marginal plots. What we need to know is: for a person with the same age, same experience, same seniority, is the salary different for men and women. How would you make plots to try to examine this?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nWe can’t get exactly the same age, seniority and experience with a small sample of data, but we could get close to this by binning the variables.\n\n\nCode\ncase1202 &lt;- case1202 %&gt;%\n  mutate(Exper_c = cut(Exper, \n                       breaks=c(-1, 50, 100, 400), \n                       labels=c(\"E-low\", \n                                \"E-med\", \n                                \"E-high\")),\n         Senior_c = cut(Senior, 3, \n                        labels=c(\"S-low\", \n                                 \"S-med\",\n                                 \"S-high\")))\nggplot(case1202, aes(x = Age, \n                           y = Bsal, \n                           colour = Sex)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  facet_grid(Exper_c~Senior_c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWould you say that this data provides evidence of sex discrimination?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe plots suggest that there is evidence of sex discrimination. Particularly, the last comparison, when the data is subdivided into more homogeneous groups, there is still a disparity in salary, in every category."
  },
  {
    "objectID": "week6/slides.html#the-world-is-full-of-obvious-things-which-nobody-by-any-chance-observes",
    "href": "week6/slides.html#the-world-is-full-of-obvious-things-which-nobody-by-any-chance-observes",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "The world is full of obvious things which nobody by any chance observes",
    "text": "The world is full of obvious things which nobody by any chance observes\n Quote from Arthur Conan Doyle, The Hound of the Baskervilles"
  },
  {
    "objectID": "week6/slides.html#outline",
    "href": "week6/slides.html#outline",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Outline",
    "text": "Outline\n\nThe humble but powerful scatterplot\nAdditions and variations\nTransformations to linearity\n(Robust) numerical measures of association\nSimpson’s paradox\nMaking null samples to test for association\nImputing missings\nMultiple associations, exploring network data"
  },
  {
    "objectID": "week6/slides.html#the-scatterplot",
    "href": "week6/slides.html#the-scatterplot",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "The scatterplot",
    "text": "The scatterplot\n\nScatterplots are the natural plot to make to explore association between two continuous (quantitative or numeric) variables.\n\nThey are not just for linear relationships but are useful for examining nonlinear patterns, clustering and outliers.\nWe also can think about scatterplots in terms of statistical distributions: if a histogram shows a marginal distribution, a scatterplot allows us to examine the bivariate distribution of a sample."
  },
  {
    "objectID": "week6/slides.html#history",
    "href": "week6/slides.html#history",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "History",
    "text": "History\n\nDescartes provided the Cartesian coordinate system in the 17th century, with perpendicular lines indicating two axes.\n\n\n\nIt wasn’t until 1832 that the scatterplot appeared, when John Frederick Herschel plotted position and time of double stars.\n\n\n\n\nThis is 200 years after the Cartesian coordinate system, and 50 years after bar charts and line charts appeared, used in the work of William Playfair to examine economic data.\n\n\n\n\nKopf argues that The scatter plot, by contrast, proved more useful for scientists, but it clearly is useful for economics today.\n\n\nhttp://www.datavis.ca/milestones/"
  },
  {
    "objectID": "week6/slides.html#language-and-terminology",
    "href": "week6/slides.html#language-and-terminology",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Language and terminology",
    "text": "Language and terminology\nAre the words “correlation” and “association” interchangeable?\n\nIn the broadest sense correlation is any statistical association, though it commonly refers to the degree to which a pair of variables are linearly related. Wikipedia\n\n \n\nIf the relationship is not linear, call it association, and avoid correlated."
  },
  {
    "objectID": "week6/slides.html#features-of-a-pair-of-continuous-variables-13",
    "href": "week6/slides.html#features-of-a-pair-of-continuous-variables-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Features of a pair of continuous variables (1/3)",
    "text": "Features of a pair of continuous variables (1/3)\n\n\n\n\n\nFeature\nExample\nDescription\n\n\n\n\npositive trend\n\nLow value corresponds to low value, and high to high.\n\n\nnegative trend\n\nLow value corresponds to high value, and high to low.\n\n\nno trend\n\nNo relationship\n\n\nstrong\n\nVery little variation around the trend\n\n\nmoderate\n\nVariation around the trend is almost as much as the trend\n\n\nweak\n\nA lot of variation making it hard to see any trend"
  },
  {
    "objectID": "week6/slides.html#features-of-a-pair-of-continuous-variables-23",
    "href": "week6/slides.html#features-of-a-pair-of-continuous-variables-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Features of a pair of continuous variables (2/3)",
    "text": "Features of a pair of continuous variables (2/3)\n\n\n\n\n\nFeature\nExample\nDescription\n\n\n\n\nlinear form\n\nThe shape is linear\n\n\nnonlinear form\n\nThe shape is more of a curve\n\n\nnonlinear form\n\nThe shape is more of a curve\n\n\noutliers\n\nThere are one or more points that do not fit the pattern on the others\n\n\nclusters\n\nThe observations group into multiple clumps\n\n\ngaps\n\nThere is a gap, or gaps, but its not clumped"
  },
  {
    "objectID": "week6/slides.html#features-of-a-pair-of-continuous-variables-33",
    "href": "week6/slides.html#features-of-a-pair-of-continuous-variables-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Features of a pair of continuous variables (3/3)",
    "text": "Features of a pair of continuous variables (3/3)\n\n\n\n\n\nFeature\nExample\nDescription\n\n\n\n\nbarrier\n\nThere is combination of the variables which appears impossible\n\n\nl-shape\n\nWhen one variable changes the other is approximately constant\n\n\ndiscreteness\n\nRelationship between two variables is different from the overall, and observations are in a striped pattern\n\n\nheteroskedastic\n\nVariation is different in different areas, maybe depends on value of x variable\n\n\nweighted\n\nIf observations have an associated weight, reflect in scatterplot, e.g. bubble chart"
  },
  {
    "objectID": "week6/slides.html#additional-considerations-unwin-2015",
    "href": "week6/slides.html#additional-considerations-unwin-2015",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Additional considerations (Unwin, 2015):",
    "text": "Additional considerations (Unwin, 2015):\n\ncausation: one variable has a direct influence on the other variable, in some way. For example, people who are taller tend to weigh more. The dependent variable is conventionally on the y axis. It’s not generally possible to tell from the plot that the relationship is causal, which typically needs to be argued from other sources of information.\nassociation: variables may be related to one another, but through a different variable, eg ice cream sales are positively correlated with beach drownings, is most likely a temperature relationship.\nconditional relationships: the relationship between variables is conditionally dependent on another, such as income against age likely has a different relationship depending on retired or not."
  },
  {
    "objectID": "week6/slides.html#famous-data-examples",
    "href": "week6/slides.html#famous-data-examples",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Famous data examples",
    "text": "Famous data examples"
  },
  {
    "objectID": "week6/slides.html#famous-scatterplot-examples",
    "href": "week6/slides.html#famous-scatterplot-examples",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Famous scatterplot examples",
    "text": "Famous scatterplot examples\n\n\nAnscombe’s quartet\n\n\n\n\n\n\n\n\n\nAll four sets of Anscombe has same means, standard deviations and correlations, \\(\\bar{x}\\) = 9, \\(\\bar{y}\\) = 7.5, \\(s_x\\) = 3.3, \\(s_y\\) = 2, \\(r\\) = 0.82.\n\n\nNumerical statistics are the same, for very different association.\n\n\nDatasaurus dozen\nAnd similarly all 13 sets of the datasaurus dozen have same means, standard deviations and correlations, \\(\\bar{x}\\) = 54, \\(\\bar{y}\\) = 48, \\(s_x\\) = 17, \\(s_y\\) = 27, \\(r\\) = -0.06."
  },
  {
    "objectID": "week6/slides.html#aspect-ratio",
    "href": "week6/slides.html#aspect-ratio",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Aspect ratio",
    "text": "Aspect ratio\nNotice that the scatterplots of the historical examples were made using equal aspect ratio, that is the plot space is a SQUARE not a rectangle.\nMost software and most scatterplots you see use unequal aspect ratio. This is an ERROR that is ubiquitous!\nWhen showing two variables, to read association effectively REQUIRES the SQUARE (1:1) aspect ratio.\nThese slides have more than 50 uses of aspect.ratio=1.\nMake sure you don’t fall into the trap of failing to control aspect ratio."
  },
  {
    "objectID": "week6/slides.html#scatterplot-case-studies",
    "href": "week6/slides.html#scatterplot-case-studies",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scatterplot case studies",
    "text": "Scatterplot case studies"
  },
  {
    "objectID": "week6/slides.html#case-study-olympics",
    "href": "week6/slides.html#case-study-olympics",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Olympics",
    "text": "Case study: Olympics\n\n🖼dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Warning message: Removed 1346 rows containing missing values (geom_point)\nFeatures:\n\nlinear relationship (expected, more than?)\noutliers\ndiscretization\n\nSubstantial overplotting, &gt;10000 athletes.\nWhat is interesting? Are there some sport(s) where you would expect specific relationships?\n\n\n\n\nIndividual data for the Summer 2012 Olympic Games. Downloaded from http://www.guardian.co.uk/sport/series/london-2012-olympics-data in 2013-03; more recently it has changed to https://www.theguardian.com/sport/series/london-2012-olympics-data.\n\n\nskimr::skim(oly12)\n\n\nData summary\n\n\nName\noly12\n\n\nNumber of rows\n10384\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nDate\n1\n\n\nfactor\n6\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nDOB\n6192\n0.4\n1947-06-01\n1997-07-09\n1986-09-11\n2149\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nName\n0\n1\nFALSE\n10366\nLei: 3, Lin: 3, Ale: 2, Hao: 2\n\n\nCountry\n0\n1\nFALSE\n205\nGre: 523, Uni: 518, Rus: 414, Aus: 399\n\n\nSex\n0\n1\nFALSE\n2\nM: 5756, F: 4628\n\n\nPlaceOB\n0\n1\nFALSE\n4108\nemp: 2690, Seo: 57, Bud: 54, Mos: 50\n\n\nSport\n0\n1\nFALSE\n42\nAth: 2119, Swi: 907, Foo: 596, Row: 524\n\n\nEvent\n0\n1\nFALSE\n763\nMen: 336, Wom: 260, Wom: 210, Men: 206\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAge\n0\n1.00\n26.07\n5.44\n13.0\n22.0\n25.0\n29.0\n71.0\n▆▇▁▁▁\n\n\nHeight\n561\n0.95\n1.77\n0.11\n1.3\n1.7\n1.8\n1.9\n2.2\n▁▃▇▃▁\n\n\nWeight\n1280\n0.88\n72.85\n16.07\n36.0\n61.0\n70.0\n81.0\n218.0\n▇▆▁▁▁\n\n\nGold\n0\n1.00\n0.02\n0.14\n0.0\n0.0\n0.0\n0.0\n2.0\n▇▁▁▁▁\n\n\nSilver\n0\n1.00\n0.02\n0.13\n0.0\n0.0\n0.0\n0.0\n2.0\n▇▁▁▁▁\n\n\nBronze\n0\n1.00\n0.02\n0.14\n0.0\n0.0\n0.0\n0.0\n2.0\n▇▁▁▁▁\n\n\nTotal\n0\n1.00\n0.05\n0.25\n0.0\n0.0\n0.0\n0.0\n5.0\n▇▁▁▁▁\n\n\n\n\n\n\n\n\n\ndata(oly12, package = \"VGAMdata\")\nggplot(oly12, aes(x = Height, y = Weight, label = Sport)) +\n  geom_point()"
  },
  {
    "objectID": "week6/slides.html#try-this",
    "href": "week6/slides.html#try-this",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "🧩 Try this",
    "text": "🧩 Try this\n\nInteractivity can be a useful tool for exploring relationships.\n\nCut and paste the code into your R console, and the resulting plot to examine the sport of the athlete.\n\nlibrary(tidyverse) \nlibrary(plotly) \ndata(oly12, package = \"VGAMdata\") \np &lt;- ggplot(oly12, aes(x = Height, y = Weight, label = Sport)) + \n  geom_point() \nggplotly(p)"
  },
  {
    "objectID": "week6/slides.html#how-many-athletes-in-the-different-sports",
    "href": "week6/slides.html#how-many-athletes-in-the-different-sports",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How many athletes in the different sports?",
    "text": "How many athletes in the different sports?\n\n\n\n\n\n\n\n\n\n\n\nCategories need re-working:\n\nso many different events grouped into athletics\ncycling split among many categories"
  },
  {
    "objectID": "week6/slides.html#consolidate-factor-levels",
    "href": "week6/slides.html#consolidate-factor-levels",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Consolidate factor levels",
    "text": "Consolidate factor levels\nThere are several cycling events that are reasonable to combine into one category. Similarly for gymnastics and athletics.\n\n\noly12 &lt;- oly12 |&gt;\n  mutate(Sport = as.character(Sport)) |&gt;\n  mutate(Sport = ifelse(grepl(\"Cycling\", Sport), \n    \"Cycling\", Sport\n  )) |&gt; \n  mutate(Sport = ifelse(grepl(\"Gymnastics\", Sport),\n    \"Gymnastics\", Sport\n  )) |&gt;\n  mutate(Sport = ifelse(grepl(\"Athletics\", Sport),\n    \"Athletics\", Sport\n  )) |&gt;\n  mutate(Sport = as.factor(Sport))"
  },
  {
    "objectID": "week6/slides.html#drill-down-the-by-sport",
    "href": "week6/slides.html#drill-down-the-by-sport",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Drill down the by sport",
    "text": "Drill down the by sport\n\n🖼learnR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is interesting?\n\nSome sports have no data for height, weight\nThe positive association between height and weight is visible across sports\nNonlinear in wrestling?\nAn outlier in judo, and football, and archery\nMaybe flatter among swimmers\nTaller in basketball, volleyball and handball\nShorter in athletics, weightlifting and wrestling\nLittle variance in tennis players\nThere’s a lot to digest\n\n\n\nRefine plots to make comparisons easier:\n\nRemove sports with missings\nOverlay model to assess linear relationships\nDrill down into other strata, eg male/female athletes\nCompare in small chunks, one group against the rest\n\n\n\n\n\n\nggplot(oly12, aes(x = Height, y = Weight)) +\n  geom_point(alpha = 0.5) + \n  facet_wrap(~Sport, ncol = 8) +\n  theme(aspect.ratio = 1) \n\n  Note: alpha transparency, and aspect ratio"
  },
  {
    "objectID": "week6/slides.html#remove-missings-explore-difference-by-sex",
    "href": "week6/slides.html#remove-missings-explore-difference-by-sex",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Remove missings, explore difference by sex",
    "text": "Remove missings, explore difference by sex\n\n🖼️learnR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Because the focus is now on males vs females association shape within sport, make plots scale separately.\n\nAthletics category should have been broken into several more categories like track, field: a shot-putter has a very different physique to a sprinter.\nGenerally, clustering of male/female athletes\n\n\n\nOutliers: a tall skinny male archer, a medium height very light female athletics athlete, tall light female weightlifter, tall light male volleyballer\nCanoe slalom athletes, divers, cyclists are tiny.\n\n\n\n\n\noly12 |&gt;\n  filter(!(Sport %in% c(\"Boxing\", \"Gymnastics\", \"Synchronised Swimming\", \"Taekwondo\", \"Trampoline\"))) |&gt;\n  mutate(Sport = fct_drop(Sport)) |&gt;\n  ggplot(aes(x = Height, y = Weight, colour = Sex)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~Sport, ncol = 7, scales = \"free\") +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  theme(aspect.ratio = 1, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))"
  },
  {
    "objectID": "week6/slides.html#common-ways-to-augment-scatterplots",
    "href": "week6/slides.html#common-ways-to-augment-scatterplots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Common ways to augment scatterplots",
    "text": "Common ways to augment scatterplots\n\n\n\n\n\nModification\nExample\nPurpose\n\n\n\n\nalpha-blend\n\nalleviate overplotting to examine density at centre\n\n\nmodel overlay\n\nfocus on the trend\n\n\nmodel + data\n\ntrend plus variation\n\n\ndensity\n\noverall distribution, variation and clustering\n\n\nfilled density\n\nhigh density locations in distribution (modes), variation and clustering\n\n\ncolour\n\nrelationship with conditioning and lurking variables"
  },
  {
    "objectID": "week6/slides.html#comparing-association",
    "href": "week6/slides.html#comparing-association",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Comparing association",
    "text": "Comparing association\n\n🖼️R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeightlifters are much heavier relative to height\nSwimmers are leaner relative to height\nTennis players are a bit mixed, shorter tend to be heavier, taller tend to be lighter\n\n\n\n\n\noly12 |&gt;\n  filter(Sport %in% c(\n    \"Swimming\", \"Archery\", \"Basketball\",\n    \"Handball\", \"Hockey\", \"Tennis\",\n    \"Weightlifting\", \"Wrestling\"\n  )) |&gt;\n  filter(Sex == \"F\") |&gt;\n  mutate(Sport = fct_drop(Sport), Sex = fct_drop(Sex)) |&gt;\n  ggplot(aes(x = Height, y = Weight, colour = Sport)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  scale_color_discrete_divergingx(palette = \"Zissou 1\") +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  )"
  },
  {
    "objectID": "week6/slides.html#comparing-spread",
    "href": "week6/slides.html#comparing-spread",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Comparing spread",
    "text": "Comparing spread\n\n🖼️R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModern pentathlon athletes are uniformly height and weight related\nShooters are quite varied in body type\n\n\n\n\n\noly12 |&gt;\n  filter(Sport %in% c(\"Shooting\", \"Modern Pentathlon\", \"Basketball\")) |&gt; \n  filter(Sex == \"F\") |&gt;\n  mutate(Sport = fct_drop(Sport), Sex = fct_drop(Sex)) |&gt;\n  ggplot(aes(x = Height, y = Weight, colour = Sport)) +\n  geom_density2d() + \n  scale_color_discrete_divergingx(palette = \"Zissou 1\") +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  )"
  },
  {
    "objectID": "week6/slides.html#case-study-olympics-1",
    "href": "week6/slides.html#case-study-olympics-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Olympics",
    "text": "Case study: Olympics\nWe learned that association between height and weight is different strata, defined by categorical variables: sport, gender, and possibly country and age, too.\nSome of the association may be due to unmeasured variables, for example, “Athletics” is masking different body types in throwing vs running. This is a lurking variable.\n If you were just given the Height and Weight in this data could you have detected the presence of conditional relationships?\n\nIt may appear as multimodality."
  },
  {
    "objectID": "week6/slides.html#can-you-see-conditional-dependencies",
    "href": "week6/slides.html#can-you-see-conditional-dependencies",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Can you see conditional dependencies?",
    "text": "Can you see conditional dependencies?\n\n🖼R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nThere is a barely hint of multimodality.\nIt’s not easy to detect the presence of the additional variable, and thus accurately describe the relationship between height and weight among Olympic athletes.\n\n\n\n\np1 &lt;- oly12 |&gt;\n  filter(Sport == \"Athletics\") |&gt;\n  ggplot(aes(x = Height, y = Weight)) +\n  geom_point(alpha = 0.2, size = 4) \np2 &lt;- oly12 |&gt;\n  filter(Sport == \"Athletics\") |&gt;\n  ggplot(aes(x = Height, y = Weight)) +\n  geom_density2d_filled() +\n  theme(legend.position = \"none\")\np3 &lt;- oly12 |&gt;\n  filter(Sport == \"Athletics\") |&gt;\n  ggplot(aes(x = Height, y = Weight)) +\n  geom_density2d(binwidth = 0.01) \np4 &lt;- oly12 |&gt;\n  filter(Sport == \"Athletics\") |&gt;\n  ggplot(aes(x = Height, y = Weight)) +\n  geom_density2d(binwidth = 0.001, color = \"white\", size = 0.2) +\n  geom_density2d_filled(binwidth = 0.001) +\n  theme(legend.position = \"none\")\ngrid.arrange(p1, p3, p2, p4, ncol = 2)"
  },
  {
    "objectID": "week6/slides.html#numerical-measures-of-association",
    "href": "week6/slides.html#numerical-measures-of-association",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Numerical measures of association",
    "text": "Numerical measures of association"
  },
  {
    "objectID": "week6/slides.html#correlation",
    "href": "week6/slides.html#correlation",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Correlation",
    "text": "Correlation\n\n\nCorrelation between variables \\(x_1\\) and \\(x_2\\), with \\(n\\) observations in each.\n\n\\[r = \\frac{\\sum_{i=1}^n (x_{i1}-\\bar{x}_1)(x_{i2}-\\bar{x}_2)}{\\sqrt{\\sum_{i=1}^n(x_{i1}-\\bar{x}_1)^2\\sum_{i=1}^n(x_{i2}-\\bar{x}_2)^2}} = \\frac{\\mbox{covariance}(x_1, x_2)}{(n-1)s_{x_1}s_{x_2}}\\]\n\nTest for statistical significance, whether population correlation could be 0 based on observed \\(r\\), using a \\(t_{n-2}\\) distribution:\n\n\\[t=\\frac{r}{\\sqrt{1-r^2}}\\sqrt{n-2}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor(d1$x, d1$y)\n\n[1] 0.52\n\n\n\ncor.test(d1$x, d1$y)\n\n\n    Pearson's product-moment correlation\n\ndata:  d1$x and d1$y\nt = 9, df = 198, p-value = 2e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.41 0.62\nsample estimates:\n cor \n0.52"
  },
  {
    "objectID": "week6/slides.html#problems-with-correlation-12",
    "href": "week6/slides.html#problems-with-correlation-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Problems with correlation (1/2)",
    "text": "Problems with correlation (1/2)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor(d2$x, d2$y)\n\n[1] -0.05\n\n\n\ncor.test(d2$x, d2$y)\n\n\n    Pearson's product-moment correlation\n\ndata:  d2$x and d2$y\nt = -0.7, df = 198, p-value = 0.5\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.187  0.089\nsample estimates:\n  cor \n-0.05 \n\n\n It does not summarise non-linear associations."
  },
  {
    "objectID": "week6/slides.html#problems-with-correlation-22",
    "href": "week6/slides.html#problems-with-correlation-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Problems with correlation (2/2)",
    "text": "Problems with correlation (2/2)\n\n\n\n\n\n\n\n\n\n\n\n\nAll observations\n\n\n$estimate\ncor \n0.3 \n\n$statistic\n  t \n4.4 \n\n$p.value\n[1] 1.6e-05\n\n\n\nWithout outlier\n\n\n$estimate\n   cor \n-0.012 \n\n$statistic\n    t \n-0.17 \n\n$p.value\n[1] 0.87\n\n\n It is affected by extreme values."
  },
  {
    "objectID": "week6/slides.html#perceiving-correlation",
    "href": "week6/slides.html#perceiving-correlation",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Perceiving correlation",
    "text": "Perceiving correlation\n\n🖼️answersR\n\n\n\nLet’s play a game: Guess the correlation!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerally, people don’t do very well at this task. Typically people under-estimate \\(r\\) from scatterplots, particularly when it is around 0.4-0.7. The variation in a scatterplot perceptually doesn’t vary is not linearly with \\(r\\).\nWhen someone says correlation is 0.5 it sounds impressive. BUT when someone shows you a scatterplot of data that has correlation 0.5, you will say that’s a weak relationship.\n\n\n\n\nset.seed(7777)\nvc &lt;- matrix(c(1, 0, 0, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np1 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, 0.4, 0.4, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np2 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, 0.6, 0.6, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np3 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, 0.8, 0.8, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np4 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, -0.2, -0.2, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np5 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, -0.5, -0.5, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np6 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, -0.7, -0.7, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np7 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\nvc &lt;- matrix(c(1, -0.9, -0.9, 1), ncol = 2, byrow = T)\nd &lt;- as_tibble(rmvnorm(500, sigma = vc))\np8 &lt;- ggplot(d, aes(x = V1, y = V2)) +\n  geom_point() +\n  theme_void() +\n  theme(\n    aspect.ratio = 1,\n    plot.background = element_rect(fill = \"gray90\")\n  )\ngrid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, ncol = 4)"
  },
  {
    "objectID": "week6/slides.html#robust-correlation-measures-12",
    "href": "week6/slides.html#robust-correlation-measures-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Robust correlation measures (1/2)",
    "text": "Robust correlation measures (1/2)\n\n\n\n\nSpearman (based on ranks)\n\nSort each variable, and return rank (of actual value)\nCompute correlation between ranks of each variable\n\n\n\n\nset.seed(60)\ndf &lt;- tibble(\n  x = c(round(rnorm(5), 1), 3, 10),\n  y = c(round(rnorm(5), 1), -3, 10)\n) |&gt;\n  mutate(xr = rank(x), yr = rank(y))\ndf\n\n# A tibble: 7 × 4\n      x     y    xr    yr\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.7  -1.7     5     2\n2   0.5   1.1     4     6\n3  -0.6   0.3     2     4\n4  -0.2  -0.9     3     3\n5  -1.7   0.4     1     5\n6   3    -3       6     1\n7  10    10       7     7\n\n\n\n\ncor(df$x, df$y)\n\n[1] 0.79\n\ncor(df$xr, df$yr)\n\n[1] -0.036\n\ncor(df$x, df$y, method = \"spearman\")\n\n[1] -0.036\n\n\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  xlim(c(-3.5, 10.5)) + ylim(c(-3.5, 10.5)) +\n  theme(aspect.ratio=1)"
  },
  {
    "objectID": "week6/slides.html#robust-correlation-measures-22",
    "href": "week6/slides.html#robust-correlation-measures-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Robust correlation measures (2/2)",
    "text": "Robust correlation measures (2/2)\n\n\nKendall \\(\\tau\\) (based on comparing pairs of observations)\n\nSort each variable, and return rank (of actual value)\nFor all pairs of observations \\((x_i, y_i), (x_j, y_j)\\) compare ranks to determine if concordant, relatively \\(x_i &lt; x_j, y_i &lt; y_j\\) or \\(x_i &gt; x_j, y_i &gt; y_j\\), or discordant, \\(x_i &lt; x_j, y_i &gt; y_j\\) or \\(x_i &gt; x_j, y_i &lt; y_j\\).\n\n\n\\[\\tau = \\frac{n_c-n_d}{\\frac12 n(n-1)}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 7 × 4\n      x     y    xr    yr\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.7  -1.7     5     2\n2   0.5   1.1     4     6\n3  -0.6   0.3     2     4\n4  -0.2  -0.9     3     3\n5  -1.7   0.4     1     5\n6   3    -3       6     1\n7  10    10       7     7\n\n\n\n\ncor(df$x, df$y)\n\n[1] 0.79\n\ncor(df$x, df$y, method = \"kendall\")\n\n[1] -0.14"
  },
  {
    "objectID": "week6/slides.html#comparison-of-correlation-measures",
    "href": "week6/slides.html#comparison-of-correlation-measures",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Comparison of correlation measures",
    "text": "Comparison of correlation measures\n\n\n\n\n\nsample\ncorr\nspearman\nkendall\n\n\n\n\n\n0.52\n0.512\n0.355\n\n\n\n-0.05\n-0.087\n-0.073\n\n\n\n0.30\n-0.023\n-0.014\n\n\n\n\n\n\nRobust calculation corrects outlier problems, but nothing measures the non-linear association."
  },
  {
    "objectID": "week6/slides.html#transformations",
    "href": "week6/slides.html#transformations",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Transformations",
    "text": "Transformations\nfor skewness, heteroskedasticity and linearising relationships, and to emphasize association"
  },
  {
    "objectID": "week6/slides.html#circle-of-transformations-for-linearising",
    "href": "week6/slides.html#circle-of-transformations-for-linearising",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Circle of transformations for linearising",
    "text": "Circle of transformations for linearising\n\n\n\n\n\n\n\n\n\n\n\n\nRemember the power ladder:\n-1, 0, 1/3, 1/2, 1, 2, 3, 4\n\n\nLook at the shape of the relationship.\nImagine this to be a number plane, and depending on which quadrant the shape falls in, you either transform \\(x\\) or \\(y\\), up or down the ladder: +,+ both up; +,- x up, y down; -,- both down; -,+ x down, y up\n\n\nIf there is heteroskedasticity, try transforming \\(y\\), may or may not help"
  },
  {
    "objectID": "week6/slides.html#scatterplot-case-studies-1",
    "href": "week6/slides.html#scatterplot-case-studies-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scatterplot case studies",
    "text": "Scatterplot case studies"
  },
  {
    "objectID": "week6/slides.html#case-study-soils-14",
    "href": "week6/slides.html#case-study-soils-14",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Soils (1/4)",
    "text": "Case study: Soils (1/4)\n\n\n\n\n\n\n\n\n\n\n\n\nInterplay between skewness and association\nData is from a soil chemical analysis of a farm field in Iowa. Is there a relationship between Yield and Boron?\n You can get a marginal plot of each variable added to the scatterplot using ggMarginal. This is useful for assessing the skewness in each variable.\n Boron is right-skewed Yield is left-skewed. With skewed distributions in marginal variables it is hard to assess the relationship between the two. Make a transformation to fix, first."
  },
  {
    "objectID": "week6/slides.html#case-study-soils-24",
    "href": "week6/slides.html#case-study-soils-24",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Soils (2/4)",
    "text": "Case study: Soils (2/4)\n\n\n\n\n\n\n\n\n\n\n\n\n \n\np &lt;- ggplot(\n  baker,\n  aes(x = B, y = Corn97BU^2)\n) + \n  geom_point() +\n  xlab(\"log Boron (ppm)\") +\n  ylab(\"Corn Yield^2 (bushells)\") +\n  scale_x_log10() \nggMarginal(p, type = \"density\")"
  },
  {
    "objectID": "week6/slides.html#case-study-soils-34",
    "href": "week6/slides.html#case-study-soils-34",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Soils (3/4)",
    "text": "Case study: Soils (3/4)\n\n\n\n\n\n\n\n\n\n\n\n\n Lurking variable?\n \n\np &lt;- ggplot(\n  baker,\n  aes(x = Fe, y = Corn97BU^2)\n) +\n  geom_density2d(colour = \"orange\") +\n  geom_point() +\n  xlab(\"Iron (ppm)\") + \n  ylab(\"Corn Yield^2 (bushells)\")\nggMarginal(p, type = \"density\")"
  },
  {
    "objectID": "week6/slides.html#case-study-soils-44",
    "href": "week6/slides.html#case-study-soils-44",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Soils (4/4)",
    "text": "Case study: Soils (4/4)\n\n\n\n\n\n\n\n\n\n\n\n\nColour high calcium (&gt;5200ppm) calcium values\n\nggplot(baker, aes(\n  x = Fe, y = Corn97BU^2,\n  colour = ifelse(Ca &gt; 5200, \n    \"high\", \"low\"\n  )\n)) + \n  geom_point() +\n  xlab(\"Iron (ppm)\") +\n  ylab(\"Corn Yield^2 (bushells)\") +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  theme(\n    aspect.ratio = 1,\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  )\n\nIf calcium levels in the soil are high, yield is consistently high. If calcium levels are low, then there is a positive relationship between yield and iron, with higher iron leading to higher yields."
  },
  {
    "objectID": "week6/slides.html#case-study-covid-19",
    "href": "week6/slides.html#case-study-covid-19",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: COVID-19",
    "text": "Case study: COVID-19\n\n🖼️infoR\n\n\n\n\n\n\n\n\n\n\n\n\n\n Bubble plots, size of point is mapped to another variable.\nThis bubble plot here shows total count of COVID-19 incidence (as of Aug 30, 2020) for every county in the USA, inspired by the New York Times coverage.\n\n\n\nload(here(\"data/nyt_covid.rda\"))\nusa &lt;- map_data(\"state\")\nggplot() +\n  geom_polygon(\n    data = usa,\n    aes(x = long, y = lat, group = group),\n    fill = \"grey90\", colour = \"white\"\n  ) +\n  geom_point(\n    data = nyt_county_total,\n    aes(x = lon, y = lat, size = cases),\n    colour = \"red\", shape = 1\n  ) +\n  geom_point(\n    data = nyt_county_total,\n    aes(x = lon, y = lat, size = cases),\n    colour = \"red\", fill = \"red\", alpha = 0.1, shape = 16\n  ) +\n  scale_size(\"\", range = c(1, 30)) +\n  theme_map() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "week6/slides.html#scales-matter",
    "href": "week6/slides.html#scales-matter",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scales matter",
    "text": "Scales matter\n\n\n \n\n\n\n\n\n\n\n\n\n\n  Where has COVID-19 hit the hardest? \nWhere are there more people?   \n\nThis plot tells you NOTHING except where the population centres are in the USA.\n\nTo understand relative incidence/risk, report COVID numbers relative the population. For example, number of cases per 100,000 people."
  },
  {
    "objectID": "week6/slides.html#beyond-quantitative-variables",
    "href": "week6/slides.html#beyond-quantitative-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Beyond quantitative variables",
    "text": "Beyond quantitative variables"
  },
  {
    "objectID": "week6/slides.html#when-variables-are-not-quantitative",
    "href": "week6/slides.html#when-variables-are-not-quantitative",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "When variables are not quantitative",
    "text": "When variables are not quantitative\nWhat do you do if the variables are not continuous/quantitative?\n\nType of variable determines the appropriate mapping.\n\nContinuous and categorical: side-by-side boxplots, side-by-side density plots\nBoth categorical: faceted bar charts, stacked bar charts, mosaic plots, double decker plots\n\n  Stay tuned!"
  },
  {
    "objectID": "week6/slides.html#paradoxes",
    "href": "week6/slides.html#paradoxes",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Paradoxes",
    "text": "Paradoxes"
  },
  {
    "objectID": "week6/slides.html#simpsons-paradox",
    "href": "week6/slides.html#simpsons-paradox",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Simpsons paradox",
    "text": "Simpsons paradox\nThere is an additional variable, which if used for conditioning, changes the association between the variables, you have a paradox."
  },
  {
    "objectID": "week6/slides.html#simpsons-paradox-famous-example-12",
    "href": "week6/slides.html#simpsons-paradox-famous-example-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Simpson’s paradox: famous example (1/2)",
    "text": "Simpson’s paradox: famous example (1/2)\n\n\nDid Berkeley discriminate against female applicants?\nExample from Unwin (2015)"
  },
  {
    "objectID": "week6/slides.html#simpsons-paradox-famous-example-22",
    "href": "week6/slides.html#simpsons-paradox-famous-example-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Simpson’s paradox: famous example (2/2)",
    "text": "Simpson’s paradox: famous example (2/2)\n\nBased on separately examining each department, there is no evidence of discrimination against female applicants.\nExample from Unwin (2015)"
  },
  {
    "objectID": "week6/slides.html#always-examine-the-associations-in-each-strata",
    "href": "week6/slides.html#always-examine-the-associations-in-each-strata",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Always examine the associations in each strata",
    "text": "Always examine the associations in each strata"
  },
  {
    "objectID": "week6/slides.html#is-what-you-see-really-association",
    "href": "week6/slides.html#is-what-you-see-really-association",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Is what you see really association?",
    "text": "Is what you see really association?"
  },
  {
    "objectID": "week6/slides.html#checking-association-with-visual-inference",
    "href": "week6/slides.html#checking-association-with-visual-inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Checking association with visual inference",
    "text": "Checking association with visual inference\n\nSoilsROlympicsR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(\n  lineup(null_permute(\"Corn97BU\"), baker, n = 12),\n  aes(x = B, y = Corn97BU)\n) +\n  geom_point() +\n  facet_wrap(~.sample, ncol = 4)\n\n11 of the panels have had the association broken by permuting one variable. There is no association in these data sets, and hence plots. Does the data plot stand out as being different from the null (no association) plots?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata(oly12, package = \"VGAMdata\")\noly12_sub &lt;- oly12 |&gt;\n  filter(Sport %in% c(\n    \"Swimming\", \"Archery\",\n    \"Hockey\", \"Tennis\"\n  )) |&gt;\n  filter(Sex == \"F\") |&gt;\n  mutate(Sport = fct_drop(Sport), Sex = fct_drop(Sex))\n\nggplot(\n  lineup(null_permute(\"Sport\"), oly12_sub, n = 12),\n  aes(x = Height, y = Weight, colour = Sport)\n) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_colour_brewer(\"\", palette = \"Dark2\") +\n  facet_wrap(~.sample, ncol = 4) +\n  theme(legend.position = \"none\")\n\n\n11 of the panels have had the association broken by permuting the Sport label. There is no difference in the association between weight and height across sports in these data sets, and hence plots. Does the data plot stand out as being different from the null (no association difference between sports) plots?"
  },
  {
    "objectID": "week6/slides.html#handling-and-imputing-missings-12",
    "href": "week6/slides.html#handling-and-imputing-missings-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Handling and imputing missings (1/2)",
    "text": "Handling and imputing missings (1/2)\n\n\nCheck if missings on one variable are related to distribution of the other variable.\n\n\nCode\nggplot(oceanbuoys,\n       aes(x = air_temp_c,\n           y = humidity)) +\n     geom_miss_point()\n\n\n\n\n\n\n\n\n\n\n\nMissings plotted in the margins.\nMissings on humidity only occur for lower values of air tempoerature.\n\nImputing missings, at least for humidity requires using air temperature values.\n\nBut the clustering is due to year\n\n\nCode\nggplot(oceanbuoys,\n       aes(x = air_temp_c,\n           y = humidity)) +\n     geom_miss_point() +\n     facet_wrap(~year, ncol=2) +\n     theme(legend.position = \"none\")"
  },
  {
    "objectID": "week6/slides.html#handling-and-imputing-missings-22",
    "href": "week6/slides.html#handling-and-imputing-missings-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Handling and imputing missings (2/2)",
    "text": "Handling and imputing missings (2/2)\n\n\nUse the mean of complete cases to impute the missings\n\n\nCode\nocean_imp_yr_mean &lt;- oceanbuoys |&gt;\n  select(air_temp_c, humidity, year) |&gt;\n  bind_shadow() |&gt;\n  group_by(year) |&gt;\n  impute_mean_at(vars(air_temp_c, humidity)) |&gt;\n  ungroup() |&gt;\n  add_label_shadow()\n  \nggplot(ocean_imp_yr_mean,\n       aes(x = air_temp_c,\n           y = humidity,\n           colour = any_missing)) + \n  geom_miss_point() +\n  scale_color_discrete_divergingx(palette = \"Zissou 1\") +\n  theme(legend.title = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nUse simulation from a bivariate normal distribution, for each year.\n\n\nCode\nocean_imp_yr_sim &lt;- nabular(oceanbuoys) |&gt;\n  select(air_temp_c, humidity, year) |&gt;\n  bind_shadow() |&gt;\n  add_label_shadow()\n\n# Need to operate on each subset\nocean_imp_yr_sim_93 &lt;- ocean_imp_yr_sim |&gt;\n  filter(year == 1993)\nocean_imp_yr_sim_97 &lt;- ocean_imp_yr_sim |&gt;\n  filter(year == 1997)\n\nocean_imp_yr_sim_93 &lt;- VIM::hotdeck(ocean_imp_yr_sim_93) \nocean_imp_yr_sim_97 &lt;- VIM::hotdeck(ocean_imp_yr_sim_97) \n  \nocean_imp_yr_sim &lt;- bind_rows(ocean_imp_yr_sim_93, ocean_imp_yr_sim_97)  \n\nggplot(ocean_imp_yr_sim,\n       aes(x = air_temp_c,\n           y = humidity,\n           colour = any_missing)) + \n  geom_miss_point() +\n  scale_color_discrete_divergingx(palette = \"Zissou 1\") +\n  theme(legend.title = element_blank(),\n        legend.position = \"none\")"
  },
  {
    "objectID": "week6/slides.html#networks",
    "href": "week6/slides.html#networks",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Networks",
    "text": "Networks"
  },
  {
    "objectID": "week6/slides.html#definition",
    "href": "week6/slides.html#definition",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Definition",
    "text": "Definition\n\n\nWhat do you do if you only receive the association information?\n\nFor example, in the Harry Potter series of books, some characters show support and comfort to other characters at different times. (This example is from Sam Tyner’s geomnet which was originally taken from The Siena Program.)\n\n\nCode\nload(\"../data/hpchars.rda\")\nglimpse(hp.chars)\n\n\nRows: 64\nColumns: 4\n$ name       &lt;chr&gt; \"Adrian Pucey\", \"Alicia Spinnet\", \"Ange…\n$ schoolyear &lt;int&gt; 1989, 1989, 1989, 1991, 1991, 1989, 198…\n$ gender     &lt;fct&gt; M, F, F, M, M, M, M, F, M, M, M, F, M, …\n$ house      &lt;fct&gt; Slytherin, Gryffindor, Gryffindor, Rave…\n\n\n\n\n\n\n\nCode\nload(\"../data/hpedges.rda\")\nglimpse(hp.edges)\n\n\nRows: 434\nColumns: 3\n$ name1 &lt;chr&gt; \"Dean Thomas\", \"Dean Thomas\", \"Dean Thomas\",…\n$ name2 &lt;chr&gt; \"Harry James Potter\", \"Hermione Granger\", \"N…\n$ book  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\n\n\n\nAny data where association between multiple pairs of variables can computed can be examined using networks, and explored using exploratory network analysis."
  },
  {
    "objectID": "week6/slides.html#step-1-explore-the-node-data",
    "href": "week6/slides.html#step-1-explore-the-node-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 1: explore the node data",
    "text": "Step 1: explore the node data\n\nschool yearhousegender\n\n\n\n\n\nCode\nggplot(hp.chars, aes(x=schoolyear)) + \n  geom_bar() +\n  scale_x_continuous(\"\", breaks = 1986:1995)\n\n\n\n\n\n\n\n\n\n\nWhat is school year in HP??\n\n\n\n\n\n\n\nCode\nggplot(hp.chars, aes(x=house)) + \n  geom_bar() +\n  xlab(\"\")\n\n\n\n\n\n\n\n\n\n\n\nMost of the characters are associated with Gryffindor!!\n\n\n\n\n\n\n\n\nCode\nggplot(hp.chars, aes(x=gender)) + \n  geom_bar() +\n  xlab(\"\")\n\n\n\n\n\n\n\n\n\n\n\nIs the book lacking in diversity?\n\n\n\n\n\n\nCode\nggplot(hp.chars, aes(x=house, fill=gender)) + \n  geom_bar(position=\"fill\") +\n  xlab(\"\") + \n  scale_fill_discrete_divergingx(palette=\"Zissou 1\")\n\n\n\n\n\n\n\n\n\n\nJust Slytherin is lacking diversity."
  },
  {
    "objectID": "week6/slides.html#step-2-explore-the-edge-data",
    "href": "week6/slides.html#step-2-explore-the-edge-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 2: explore the edge data",
    "text": "Step 2: explore the edge data\n\n\n\n\n\nCode\np1 &lt;- hp.edges |&gt;\n  count(name1) |&gt;\n  ggplot(aes(x=fct_reorder(name1, n), y=n)) + \n  geom_col() +\n  xlab(\"\") + ylab(\"\") +\n  coord_flip()\n\np2 &lt;- hp.edges |&gt;\n  count(name2) |&gt;\n  ggplot(aes(x=fct_reorder(name2, n), y=n)) + \n  geom_col() +\n  xlab(\"\") + ylab(\"\") +\n  coord_flip()\n#p1 + p2 + plot_layout(ncol=2)\n\n# Construct data to make plot comparison better\ntmp1 &lt;- hp.edges |&gt;\n  count(name1) |&gt;\n  rename(n1 = n,\n         name = name1)\ntmp2 &lt;- hp.edges |&gt;\n  count(name2) |&gt;\n  rename(n2 = n,\n         name = name2)\nhp.edges_count &lt;- full_join(tmp1, tmp2) |&gt;\n  mutate(n1 = if_else(is.na(n1), 0, n1), \n         n2 = if_else(is.na(n2), 0, n2)) |&gt;\n  mutate(n = n1 + n2) \n  \np1 &lt;- hp.edges_count |&gt;\n  ggplot(aes(x=fct_reorder(name, n), y=n1)) + \n  geom_col() +\n  xlab(\"\") + ylab(\"\") + ggtitle(\"Name 1\") +\n  coord_flip() +\n  theme(aspect.ratio = 1.5)\n\np2 &lt;- hp.edges_count |&gt;\n  ggplot(aes(x=fct_reorder(name, n), y=n2)) + \n  geom_col() +\n  xlab(\"\") + ylab(\"\") + ggtitle(\"Name 2\") +\n  coord_flip() +\n  theme(aspect.ratio = 1.5)\n\np3 &lt;- hp.edges_count |&gt;\n  ggplot(aes(x=fct_reorder(name, n), y=n)) + \n  geom_col() +\n  xlab(\"\") + ylab(\"\") + ggtitle(\"Total\") +\n  coord_flip() +\n  theme(aspect.ratio = 1.5)\n\np1 + p2 + p3 + plot_layout(ncol=3)\n\n\n\n\n\n\n\n\n\n\n\nSimilar distribution for who is providing support and who is receiving support. Small differences."
  },
  {
    "objectID": "week6/slides.html#step-3-examine-association",
    "href": "week6/slides.html#step-3-examine-association",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 3: Examine association",
    "text": "Step 3: Examine association\n\nProcess the dataHeatmap\n\n\n\nTypically only get the connected nodes, so you need to expand the data to include all possible connections, along with the association measure (here 1 or 0).\nRows/columns ordered by name. Heatmaps need to have rows and columns ordered in a sensible way.\n\n\n\n\nCode\nlibrary(tidygraph)\nlibrary(ggraph)\n# Expand edge data\nhp.edges_full &lt;- hp.edges |&gt;\n  mutate(n = 1) |&gt;\n  complete(name1, name2, book, fill = list(n = 0))\n# Check\nhp.edges_full |&gt; count(name1, name2) -&gt; x\nhp.edges_full |&gt; count(name1) -&gt; y\nhp.edges_full |&gt; count(name2) -&gt; z\ndplyr::full_join(y, z, by=c(\"name1\"=\"name2\")) -&gt; w\n# It's missing some items!\nname2_miss &lt;- w |&gt;\n  filter(is.na(n.y)) |&gt;\n  pull(name1)\nname1_miss &lt;- w |&gt;\n  filter(is.na(n.x)) |&gt;\n  pull(name1)\n# Ugly manual fix!\nn1_d &lt;- hp.edges_full |&gt;\n  filter(name2 %in% name1_miss) |&gt;\n  rename(name1 = name2,\n         name2 = name1) |&gt;\n  mutate(n = 0) |&gt;\n  select(name1, name2, book, n)\nhp.edges_full &lt;- bind_rows(hp.edges_full, n1_d)\nn2_d &lt;- hp.edges_full |&gt;\n  filter(name1 %in% name2_miss) |&gt;\n  rename(name1 = name2,\n         name2 = name1) |&gt;\n  mutate(n = 0) |&gt;\n  select(name1, name2, book, n)\nhp.edges_full &lt;- bind_rows(hp.edges_full, n2_d)\n  \nhe_allbooks_long &lt;- hp.edges_full |&gt;\n  group_by(name1, name2) |&gt;\n  summarise(n = sum(n)) \n\nhe_order &lt;- he_allbooks_long |&gt;\n  group_by(name1) |&gt;\n  summarise(n = sum(n)) |&gt;\n  arrange(desc(n)) \n\nhe_allbooks_long &lt;- he_allbooks_long |&gt;\n  mutate(name1 = factor(name1, levels=he_order$name1),\n         name2 = factor(name2, levels=he_order$name1))\n\n\n\n\n\n\n\n\nCode\np &lt;- ggplot(he_allbooks_long, \n       aes(x=name1,\n           y=name2, fill=n)) +\n  geom_tile() +\n  scale_fill_continuous_sequential(palette=\"YlGnBu\") +\n  theme(aspect.ratio=1,\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        legend.position = \"none\")\nggplotly(p, width=600, height=600)"
  },
  {
    "objectID": "week6/slides.html#step-4-make-network-summaries",
    "href": "week6/slides.html#step-4-make-network-summaries",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 4: Make network summaries",
    "text": "Step 4: Make network summaries\n\nProcess the dataNetwork\n\n\n\nLay out the nodes based on connections over all books\nSet up the data to allow facetting by book, that shows different connections in different books\n\n\n\n\nCode\nhe_connect &lt;- he_allbooks_long |&gt; \n  filter(n &gt; 0) |&gt;\n  rename(from = name1, to = name2) \nhe_graph &lt;- he_connect |&gt; as_tbl_graph() \n\nhe_g_plt &lt;- ggraph(he_graph, layout = 'tree', circular=TRUE) + \n  geom_edge_link() +\n  geom_node_point() \n  \nhe_nodes &lt;- tibble(x = he_g_plt$data$x,\n                   y = he_g_plt$data$y,\n                   name = levels(he_allbooks_long$name1))\nhe_connect &lt;- he_connect |&gt;\n  mutate(from_num = as.numeric(from),\n         to_num = as.numeric(to))\nhe_edges &lt;- tibble(\n  x = he_g_plt$data$x[he_connect$from_num],\n  y = he_g_plt$data$y[he_connect$from_num],\n  xend = he_g_plt$data$x[he_connect$to_num],\n  yend = he_g_plt$data$y[he_connect$to_num],\n  n = he_connect$n\n)\n  \n# Now set up to facet by book, keeping layout set by \n# the overall count\nbook1 &lt;- hp.edges_full |&gt; \n  filter(book == 1, n &gt; 0) |&gt;\n  mutate(name1 = factor(name1, levels = he_order$name1),\n         name2 = factor(name2, levels = he_order$name1)) |&gt;\n  mutate(from_num = as.numeric(name1),\n         to_num = as.numeric(name2))\n\nbook1_nodes &lt;- he_nodes |&gt;\n  mutate(book = 1)\nbook1_edges &lt;- tibble(\n  x = he_g_plt$data$x[book1$from_num],\n  y = he_g_plt$data$y[book1$from_num],\n  xend = he_g_plt$data$x[book1$to_num],\n  yend = he_g_plt$data$y[book1$to_num],\n  book = 1\n)\n\nbook2 &lt;- hp.edges_full |&gt; \n  filter(book == 2, n &gt; 0) |&gt;\n  mutate(name1 = factor(name1, levels = he_order$name1),\n         name2 = factor(name2, levels = he_order$name1)) |&gt;\n  mutate(from_num = as.numeric(name1),\n         to_num = as.numeric(name2))\n\nbook2_nodes &lt;- he_nodes |&gt;\n  mutate(book = 2)\nbook2_edges &lt;- tibble(\n  x = he_g_plt$data$x[book2$from_num],\n  y = he_g_plt$data$y[book2$from_num],\n  xend = he_g_plt$data$x[book2$to_num],\n  yend = he_g_plt$data$y[book2$to_num],\n  book = 2\n)\n\nbook3 &lt;- hp.edges_full |&gt; \n  filter(book == 3, n &gt; 0) |&gt;\n  mutate(name1 = factor(name1, levels = he_order$name1),\n         name2 = factor(name2, levels = he_order$name1)) |&gt;\n  mutate(from_num = as.numeric(name1),\n         to_num = as.numeric(name2))\n\nbook3_nodes &lt;- he_nodes |&gt;\n  mutate(book = 3)\nbook3_edges &lt;- tibble(\n  x = he_g_plt$data$x[book3$from_num],\n  y = he_g_plt$data$y[book3$from_num],\n  xend = he_g_plt$data$x[book3$to_num],\n  yend = he_g_plt$data$y[book3$to_num],\n  book = 3\n)\n\nbook4 &lt;- hp.edges_full |&gt; \n  filter(book == 4, n &gt; 0) |&gt;\n  mutate(name1 = factor(name1, levels = he_order$name1),\n         name2 = factor(name2, levels = he_order$name1)) |&gt;\n  mutate(from_num = as.numeric(name1),\n         to_num = as.numeric(name2))\n\nbook4_nodes &lt;- he_nodes |&gt;\n  mutate(book = 4)\nbook4_edges &lt;- tibble(\n  x = he_g_plt$data$x[book4$from_num],\n  y = he_g_plt$data$y[book4$from_num],\n  xend = he_g_plt$data$x[book4$to_num],\n  yend = he_g_plt$data$y[book4$to_num],\n  book = 4\n)\n\nbook5 &lt;- hp.edges_full |&gt; \n  filter(book == 5, n &gt; 0) |&gt;\n  mutate(name1 = factor(name1, levels = he_order$name1),\n         name2 = factor(name2, levels = he_order$name1)) |&gt;\n  mutate(from_num = as.numeric(name1),\n         to_num = as.numeric(name2))\n\nbook5_nodes &lt;- he_nodes |&gt;\n  mutate(book = 5)\nbook5_edges &lt;- tibble(\n  x = he_g_plt$data$x[book5$from_num],\n  y = he_g_plt$data$y[book5$from_num],\n  xend = he_g_plt$data$x[book5$to_num],\n  yend = he_g_plt$data$y[book5$to_num],\n  book = 5\n)\n\nbook6 &lt;- hp.edges_full |&gt; \n  filter(book == 6, n &gt; 0) |&gt;\n  mutate(name1 = factor(name1, levels = he_order$name1),\n         name2 = factor(name2, levels = he_order$name1)) |&gt;\n  mutate(from_num = as.numeric(name1),\n         to_num = as.numeric(name2))\n\nbook6_nodes &lt;- he_nodes |&gt;\n  mutate(book = 6)\nbook6_edges &lt;- tibble(\n  x = he_g_plt$data$x[book6$from_num],\n  y = he_g_plt$data$y[book6$from_num],\n  xend = he_g_plt$data$x[book6$to_num],\n  yend = he_g_plt$data$y[book6$to_num],\n  book = 6\n)\n\nall_books_nodes &lt;- bind_rows(book1_nodes, book2_nodes, \n                             book3_nodes, book4_nodes, \n                             book5_nodes, book6_nodes)\nall_books_edges &lt;- bind_rows(book1_edges, book2_edges, \n                           book3_edges, book4_edges, \n                           book5_edges, book6_edges)\n\n\n\n\n\n\n\n\nCode\np2 &lt;- ggplot() +\n  geom_point(data=all_books_nodes, aes(x=x, y=y, label=name)) +\n  geom_segment(data=all_books_edges, \n               aes(x=x, y=y, xend=xend, yend=yend),\n               linewidth = 0.3) +\n  scale_color_viridis_c() +\n  facet_wrap(~book, ncol=3) +\n  theme(aspect.ratio=1,\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        panel.background = element_rect(fill=NA, \n                                        colour=\"black\"),\n        legend.position = \"none\")\nggplotly(p2, tooltip = \"label\", width=900, height=600)"
  },
  {
    "objectID": "week6/slides.html#key-points-from-today",
    "href": "week6/slides.html#key-points-from-today",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Key points from today?",
    "text": "Key points from today?"
  },
  {
    "objectID": "week6/slides.html#resources",
    "href": "week6/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nUnwin (2015) Graphical Data Analysis with R\nGraphics using ggplot2\nWilke (2019) Fundamentals of Data Visualization\nFriendly and Denis Milestones in History of Thematic Cartography, Statistical Graphics and Data Visualisation\nTierney et al (2023) Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.\nSam Tyner’s exploring network data with geomnet"
  },
  {
    "objectID": "week5/worksheet.html",
    "href": "week5/worksheet.html",
    "title": "ETC5521 Worksheet Week 5",
    "section": "",
    "text": "1. Understanding the velocity of galaxies\nLoad the galaxies data in the MASS package and answer the following questions based on this dataset.\n\n\nCode\ndata(galaxies, package = \"MASS\")\n\n\nYou can access documentation of the data (if available) using the help function specifying the package name in the argument.\n\n\nCode\nhelp(galaxies, package = \"MASS\")\n\n\n\nWhat does the data contain? And what is the data source?\n\n\nBased on the description in the R Help for the data, what would be an appropriate null distribution of this data?\n\n\nHow many observations are there?\n\n\nIf the data is multimodal, which of the following displays do you think would be the best? Which would not be at all useful?\n\n\nhistogram\nboxplot\ndensity plot\nviolin plot\njittered dot plot\nletter value plot\n\n\nMake these plots for the data. Experiment with different binwidths for the histogram and different bandwiths for the density plot. Were you right in your thinking about which would be the best?\n\n\nFit your best mixture model to the data, and simulate 19 nulls to make a lineup. Did you do a good job in matching the distribution, ie does the data plot stand out or not? (Extra bonus: What is the model that you have created? Can you make a plot to show how it looks relative to the observed data?)\n\nThis code might be helpful to get you started. This code generates a jittered dotplot, but you can use your preferred type from part e.\n\n\nCode\n# Fit a mixture model\nlibrary(mixtools)\ngalaxies_fit &lt;- normalmixEM(galaxies, k=3)\n\nset.seed(1138)\ngalaxies_sim1 &lt;- rnormmix(n=length(galaxies), \n              lambda=galaxies_fit$lambda, \n              mu=galaxies_fit$mu,\n              sigma=galaxies_fit$sigma)\n\n\n\n\nCode\n# Plot your data\nggplot(tibble(galaxies_sim1), aes(x=galaxies_sim1)) +\n  geom_quasirandom(aes(x=1, y=galaxies_sim1)) + \n  coord_flip() +\n  theme(\n    aspect.ratio = 0.7,\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n\n\nCode\n# Generate null plots and make a lineup\ngalaxies_null &lt;- tibble(.sample=1, galaxies=galaxies_sim1)\nfor (i in 2:19) {\n  gsim &lt;- rnormmix(n=length(galaxies), \n              lambda=galaxies_fit$lambda, \n              mu=galaxies_fit$mu,\n              sigma=galaxies_fit$sigma)\n  galaxies_null &lt;- bind_rows(galaxies_null,\n                             tibble(.sample=i, galaxies=gsim))\n}\ngalaxies_null &lt;- bind_rows(galaxies_null,\n                             tibble(.sample=20,\n                                    galaxies=galaxies))\n# Randomise .sample  to hide data plot\ngalaxies_null$.sample &lt;- rep(sample(1:20, 20), rep(82, 20))\nggplot(tibble(galaxies_null), aes(x=galaxies)) +\n  geom_quasirandom(aes(x=1, y=galaxies)) + \n  facet_wrap(~.sample, ncol=5) +\n  coord_flip() +\n  theme(\n    aspect.ratio = 0.7,\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n\n\n2. What is the best transformation to make?\nFor each of the variables in the data, which-transform.csv, decide on an appropriate transformation to make the distribution more symmetric for five of the variables and remove discreteness on one variable."
  },
  {
    "objectID": "week5/worksheetsol.html",
    "href": "week5/worksheetsol.html",
    "title": "ETC5521 Worksheet Week 5",
    "section": "",
    "text": "1. Understanding the velocity of galaxies\nLoad the galaxies data in the MASS package and answer the following questions based on this dataset.\n\n\nCode\ndata(galaxies, package = \"MASS\")\n\n\nYou can access documentation of the data (if available) using the help function specifying the package name in the argument.\n\n\nCode\nhelp(galaxies, package = \"MASS\")\n\n\n\nWhat does the data contain? And what is the data source?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\ndata(galaxies, package = \"MASS\")\nglimpse(galaxies)\n\n\n num [1:82] 9172 9350 9483 9558 9775 ...\n\n\nThe data contains velocities in km/sec of 82 galaxies from 6 well-separated conic sections of an unfilled survey of the Corona Borealis region. The original data is from Postman et al. (1986) and this data is from Roeder with 83rd observation removed from the original data as well as typo for the 78th observation.\n\nPostman, M., Huchra, J. P. and Geller, M. J. (1986) Probes of large-scale structures in the Corona Borealis region. Astronomical Journal 92, 1238–1247\nRoeder, K. (1990) Density estimation with confidence sets exemplified by superclusters and voids in galaxies. Journal of the American Statistical Association 85, 617–624.\n\n\n\n\n\n\nBased on the description in the R Help for the data, what would be an appropriate null distribution of this data?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe description in the R help for the data says Multimodality in such surveys is evidence for voids and superclusters in the far universe.\nDeciding on an appropriate null hypothesis is always tricky. If we wanted to test the statement that the data is multimodal, we could compare against a unimodal distribution, either a normal or an exponential depending on what shape we might expect.\nHowever, the published work has already made a claim that the data is multimodal, so it would be interesting to determine if we can generate samples from a multimodal distribution that are indistinguishable from the data.\n\\(H_0:\\) The distribution is multimodal. \\(H_a:\\) The distribution is something other than multimodal.\n\n\n\n\n\nHow many observations are there?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThere are 82 observations.\n\n\n\n\n\nIf the data is multimodal, which of the following displays do you think would be the best? Which would not be at all useful?\n\n\nhistogram\nboxplot\ndensity plot\nviolin plot\njittered dot plot\nletter value plot\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nIf you said a density plot, jittered dot plot, or a histogram, you’re on the right track, because each can give a fine resolution for showing modes. (The violin plot is not any different from a density plot, when only looking at one variable.)\n\n\n\n\n\nMake these plots for the data. Experiment with different binwidths for the histogram and different bandwiths for the density plot. Were you right in your thinking about which would be the best?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\ng &lt;- ggplot(tibble(galaxies), aes(galaxies)) +\n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\ng1 &lt;- g + geom_histogram(binwidth = 1000, color = \"white\") \ng2 &lt;- g + geom_boxplot() \ng3 &lt;- g + geom_density() \ng4 &lt;- g + geom_violin(aes(x=galaxies, y=1), draw_quantiles = c(0.25, 0.5, 0.75))\ng5 &lt;- g + geom_quasirandom(aes(x=1, y=galaxies)) + coord_flip() \ng6 &lt;- g + geom_lv(aes(x=1, y=galaxies)) + coord_flip() \n\ng1 + g2 + g3 + g4 + g5 + g6 + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFit your best mixture model to the data, and simulate 19 nulls to make a lineup. Did you do a good job in matching the distribution, ie does the data plot stand out or not? (Extra bonus: What is the model that you have created? Can you make a plot to show how it looks relative to the observed data?)\n\nThis code might be helpful to get you started. This code generates a jittered dotplot, but you can use your preferred type from part e.\n\n\nCode\n# Fit a mixture model\nlibrary(mixtools)\ngalaxies_fit &lt;- normalmixEM(galaxies, k=3)\n\nset.seed(1138)\ngalaxies_sim1 &lt;- rnormmix(n=length(galaxies), \n              lambda=galaxies_fit$lambda, \n              mu=galaxies_fit$mu,\n              sigma=galaxies_fit$sigma)\n\n\n\n\nCode\n# Plot your data\nggplot(tibble(galaxies_sim1), aes(x=galaxies_sim1)) +\n  geom_quasirandom(aes(x=1, y=galaxies_sim1)) + \n  coord_flip() +\n  theme(\n    aspect.ratio = 0.7,\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n\n\nCode\n# Generate null plots and make a lineup\ngalaxies_null &lt;- tibble(.sample=1, galaxies=galaxies_sim1)\nfor (i in 2:19) {\n  gsim &lt;- rnormmix(n=length(galaxies), \n              lambda=galaxies_fit$lambda, \n              mu=galaxies_fit$mu,\n              sigma=galaxies_fit$sigma)\n  galaxies_null &lt;- bind_rows(galaxies_null,\n                             tibble(.sample=i, galaxies=gsim))\n}\ngalaxies_null &lt;- bind_rows(galaxies_null,\n                             tibble(.sample=20,\n                                    galaxies=galaxies))\n# Randomise .sample  to hide data plot\ngalaxies_null$.sample &lt;- rep(sample(1:20, 20), rep(82, 20))\nggplot(tibble(galaxies_null), aes(x=galaxies)) +\n  geom_quasirandom(aes(x=1, y=galaxies)) + \n  facet_wrap(~.sample, ncol=5) +\n  coord_flip() +\n  theme(\n    aspect.ratio = 0.7,\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nnumber of iterations= 59 \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# To make a rough plot of your model\nplot(galaxies_fit, whichplots=2)\n\n\n\n\n\n\n\n\n\nThe lambda value provides the proportion of mixing, from three normal samples. The mu and sigma give the mean and standard deviations for each of the distributions.\n\n\n\n\n\n\n2. What is the best transformation to make?\nFor each of the variables in the data, which-transform.csv, decide on an appropriate transformation to make the distribution more symmetric for five of the variables and remove discreteness on one variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nRemember the power ladder, go down to fix right-skew, and up to fix left-skew. For multi-modal find an explanatory variable, or do a severe quantile transformation."
  },
  {
    "objectID": "week5/slides.html#quantitative-variables",
    "href": "week5/slides.html#quantitative-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Quantitative variables",
    "text": "Quantitative variables"
  },
  {
    "objectID": "week5/slides.html#features-of-a-single-quantitative-variable",
    "href": "week5/slides.html#features-of-a-single-quantitative-variable",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Features of a single quantitative variable",
    "text": "Features of a single quantitative variable\n\n\n\n\n\nFeature\nExample\nDescription\n\n\n\n\nAsymmetry\n\nThe distribution is not symmetrical.\n\n\nOutliers\n\nSome observations are that are far from the rest.\n\n\nMultimodality\n\nThere are more than one \"peak\" in the observations.\n\n\nGaps\n\nSome continuous interval that are contained within the range but no observations exists.\n\n\nHeaping\n\nSome values occur unexpectedly often.\n\n\nDiscretized\n\nOnly certain values are found, e.g. due to rounding.\n\n\nImplausible\n\nValues outside of plausible or likely range."
  },
  {
    "objectID": "week5/slides.html#numerical-features-of-a-single-quantitative-variables",
    "href": "week5/slides.html#numerical-features-of-a-single-quantitative-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Numerical features of a single quantitative variables",
    "text": "Numerical features of a single quantitative variables\n\n\nA measure of central tendency, e.g. mean, median and mode\nA measure of dispersion (also called variability or spread), e.g. variance, standard deviation and interquartile range\nThere are other measures, e.g. skewness and kurtosis that measures “tailedness”, but these are not as common as the measures of first two\nThe mean is also the first moment and variance, skewness and kurtosis are second, third, and fourth central moments\n\nSignificance tests or hypothesis tests\n\nTesting for \\(H_0: \\mu = \\mu_0\\) vs. \\(H_1: \\mu \\neq \\mu_0\\) (often \\(\\mu_0 = 0\\))\nThe \\(t\\)-test is commonly used if the underlying data are believed to be normally distributed"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-18",
    "href": "week5/slides.html#australian-federal-election-18",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (1/8)",
    "text": "2019 Australian Federal Election (1/8)\n\n\nContext\n\nThere are 151 seats in the House of Representative for the 2019 Australian federal election\nThe major parties in Australia are:\n\nthe Coalition, comprising of the:\n\nLiberal,\nLiberal National (Qld),\nNational, and\nCountry Liberal (NT) parties, and\n\nthe Australian Labor party\n\nThe Greens party is a small but notable party\n\n\n\nSource: PRObono"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-28",
    "href": "week5/slides.html#australian-federal-election-28",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (2/8)",
    "text": "2019 Australian Federal Election (2/8)\n\n\n\n\n\n\n\n\nData source: Australian Electoral Commission. (2019)"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-38",
    "href": "week5/slides.html#australian-federal-election-38",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (3/8)",
    "text": "2019 Australian Federal Election (3/8)\nWhat is the number of the seats won in the House of Representatives by parties?\n\n📊dataR\n\n\n\n\n\n\n\n\n\nParty\n# of seats\n\n\n\n\nCoalition\n77\n\n\nLiberal\n44\n\n\nLiberal National Party Of Queensland\n23\n\n\nThe Nationals\n10\n\n\nAustralian Labor Party\n68\n\n\nThe Greens\n1\n\n\nCentre Alliance\n1\n\n\nKatter's Australian Party (Kap)\n1\n\n\nIndependent\n3\n\n\n\n\n\n\nWhat does this table tell you?\n\n\nThe Coalition won the government\nLabor and Coalition hold majority of the seats in the House of Representatives (lower house)\nParties such as The Greens, Centre Alliance and Katter’s Australian Party (KAP) won only a single seat\n\nOnly?\n\n\nWait… Did the parties compete in all electoral districts?\n\n\n\n\n\nskimr::skim(df1)\n\n── Data Summary ────────────────────────\n                           Values\nName                       df1   \nNumber of rows             1207  \nNumber of columns          18    \n_______________________          \nColumn type frequency:           \n  character                11    \n  numeric                  7     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────\n   skim_variable   n_missing complete_rate min max empty\n 1 StateAb                 0         1       2   3     0\n 2 DivisionID              0         1       3   3     0\n 3 DivisionNm              0         1       4  15     0\n 4 CandidateID             0         1       3   5     0\n 5 Surname                 0         1       2  18     0\n 6 GivenNm                 0         1       1  25     0\n 7 BallotPosition          0         1       1   3     0\n 8 Elected                 0         1       1   1     0\n 9 HistoricElected         0         1       1   1     0\n10 PartyAb               151         0.875   2   4     0\n11 PartyNm                 2         0.998   5  61     0\n   n_unique whitespace\n 1        8          0\n 2      151          0\n 3      151          0\n 4     1057          0\n 5      890          0\n 6      613          0\n 7       14          0\n 8        2          0\n 9        2          0\n10       40          0\n11       45          0\n\n── Variable type: numeric ──────────────────────────────────\n  skim_variable    n_missing complete_rate     mean       sd\n1 OrdinaryVotes            0             1 10401.   12446.  \n2 AbsentVotes              0             1   511.     569.  \n3 ProvisionalVotes         0             1    41.4     51.7 \n4 PrePollVotes             0             1   514.     607.  \n5 PostalVotes              0             1  1033.    1476.  \n6 TotalVotes               0             1 12501.   14860.  \n7 Swing                    0             1     1.07     4.26\n     p0     p25     p50      p75    p100 hist \n1 167   1867    4317    14768.   54535   ▇▁▁▁▁\n2  13    117     246      711     3287   ▇▂▁▁▁\n3   0      8      20       56      444   ▇▁▁▁▁\n4  11    108.    211      761     5248   ▇▂▁▁▁\n5  14    181     317     1216.    9837   ▇▁▁▁▁\n6 250   2348    5196    18142    61202   ▇▁▁▁▁\n7 -28.1   -0.73    1.21     2.75    43.5 ▁▆▇▁▁\n\n\n\n\n\ndf1 &lt;- read_csv(here::here(\"data/HouseFirstPrefsByCandidateByVoteTypeDownload-24310.csv\"),\n  skip = 1,\n  col_types = cols(\n    .default = col_character(),\n    OrdinaryVotes = col_double(),\n    AbsentVotes = col_double(),\n    ProvisionalVotes = col_double(),\n    PrePollVotes = col_double(),\n    PostalVotes = col_double(),\n    TotalVotes = col_double(),\n    Swing = col_double()\n  )\n)\n\n\nrecode_party_names &lt;- c(\n  \"Australian Labor Party (Northern Territory) Branch\" = \"Australian Labor Party\",\n  \"Labor\" = \"Australian Labor Party\",\n  \"The Greens (Vic)\" = \"The Greens\",\n  \"The Greens (Wa)\" = \"The Greens\",\n  \"Katter's Australian Party (KAP)\" = \"Katter's Australian Party\",\n  \"Country Liberals (Nt)\" = \"Country Liberals (NT)\"\n)\n\n\ntdf1 &lt;- df1 |&gt;\n  filter(Elected == \"Y\") |&gt;\n  mutate(\n    PartyNm = str_to_title(PartyNm),\n    PartyNm = recode(PartyNm, !!!recode_party_names)\n  ) |&gt;\n  count(PartyNm, sort = TRUE) |&gt;\n  slice(2:4, 1, 8, 6, 7, 5)\n\n\ndata.frame(PartyNm = \"Coalition\", n = sum(tdf1$n[1:3])) |&gt;\n  rbind(tdf1) |&gt;\n  knitr::kable(col.names = c(\"Party\", \"# of seats\")) |&gt;\n  kableExtra::kable_classic() |&gt;\n  kableExtra::kable_styling(    \n    full_width = FALSE,\n    font_size = 24\n  ) |&gt;\n  kableExtra::add_indent(2:4) |&gt;\n  kableExtra::row_spec(2:4, color = \"#C8C8C8\")"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-48",
    "href": "week5/slides.html#australian-federal-election-48",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (4/8)",
    "text": "2019 Australian Federal Election (4/8)\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you notice from this table?\n\n\n\nThe Greens are represented in every electoral districts\nUnited Australia Party is the only other non-major party to be represented in every electoral district\nKAP is represented in 7 electoral districts\nCentre Alliance is only represented in 3 electoral districts!\n\nLet’s have a closer look at the Greens party…\n\n\n\n\n\n\ntdf2 &lt;- df1 |&gt;\n  mutate(\n    PartyNm = str_to_title(PartyNm),\n    PartyNm = recode(PartyNm, !!!recode_party_names)\n  ) |&gt;\n  count(PartyNm, sort = TRUE)\n\n\n\nYou can omit table_options and toggle_select or have a look at the source Rmd to find out what it is\n\ntdf2 |&gt;\n  DT::datatable(\n    rownames = FALSE,\n    escape = FALSE,\n    width = \"900px\",\n    options = table_options(\n      scrollY = \"400px\",\n      title = \"Australian Federal Election 2019 - Party Distribution\",\n      csv = \"aus-election-2019-party-dist\"\n    ),\n    elementId = \"tab1B\",\n    colnames = c(\"Party\", \"# of electorates\"),\n    callback = toggle_select\n  )"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-58",
    "href": "week5/slides.html#australian-federal-election-58",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (5/8)",
    "text": "2019 Australian Federal Election (5/8)\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does this graph tell you?\n\n\n\nMajority of the country does not have first preference for the Greens\nSome constituents are slightly more supportive than the others\n\nWhat further questions does it raise?\nNotes:\n\nAustralia uses full-preference instant-runoff voting in single member seats\nFollowing the full allocation of preferences, it is possible to derive a two-party-preferred figure, where the votes have been allocated between the two main candidates in the election.\nIn Australia, this is usually between the candidates from the Coalition parties and the Australian Labor Party.\n\n\n\n\n\n\n\nskimr::skim(tdf3)\n\n── Data Summary ────────────────────────\n                           Values\nName                       tdf3  \nNumber of rows             151   \nNumber of columns          6     \n_______________________          \nColumn type frequency:           \n  character                3     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────\n  skim_variable n_missing complete_rate min max empty\n1 DivisionID            0             1   3   3     0\n2 DivisionNm            0             1   4  15     0\n3 State                 0             1   2   3     0\n  n_unique whitespace\n1      151          0\n2      151          0\n3        8          0\n\n── Variable type: numeric ──────────────────────────────────\n  skim_variable n_missing complete_rate     mean      sd\n1 votes_GRN             0             1  9821.   5581.  \n2 votes_total           0             1 99925.   9801.  \n3 perc_GRN              0             1     9.87    5.63\n        p0      p25       p50      p75     p100 hist \n1  2744     6555      8676     11532.   45876   ▇▂▁▁▁\n2 51009    96372.   100936    105588   116216   ▁▁▁▇▅\n3     2.89     6.43      8.55     11.4     47.8 ▇▂▁▁▁\n\n\n\n\n\ntdf3 &lt;- df1 |&gt;\n  group_by(DivisionID) |&gt;\n  summarise(\n    DivisionNm = unique(DivisionNm),\n    State = unique(StateAb),\n    votes_GRN = TotalVotes[which(PartyAb == \"GRN\")],\n    votes_total = sum(TotalVotes)\n  ) |&gt;\n  mutate(perc_GRN = votes_GRN / votes_total * 100)\n\n\ntdf3 |&gt;\n  ggplot(aes(perc_GRN)) +\n  geom_histogram(color = \"white\", fill = \"#00843D\") +\n  labs(\n    x = \"First preference votes %\",\n    y = \"Count\",\n    title = \"Greens party\"\n  )"
  },
  {
    "objectID": "week5/slides.html#formulating-questions-for-eda-vs-making-observations-from-a-plot",
    "href": "week5/slides.html#formulating-questions-for-eda-vs-making-observations-from-a-plot",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Formulating questions for EDA vs making observations from a plot",
    "text": "Formulating questions for EDA vs making observations from a plot\n\n\n\nBEFORE plotting or making summaries think broad (open-ended) questions about the distribution of values\nQuestions with simple answers (i.e. yes or no) less helpful in encouraging exploration using graphics\nFor example,\n\nWhat is the distribution of the first preference vote percentages for the Labor party across Australia?\nIs it evenly spread across electorates or are there clusters of popularity?\n\n\n\n\n\nAFTER plotting or making summaries think  was this what you expected, are there any surprises. Detail what you learn, and how you should follow up on these observations.\n\n\n\n\n\n\n\n\n\n\nIs the outlying observation the electoral district that won the seat?"
  },
  {
    "objectID": "week5/slides.html#visual-inference",
    "href": "week5/slides.html#visual-inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference",
    "text": "Visual inference\n\n\nSuitable null models for a single variable all focus on potential distributions.\nTypical plot description:\n\nggplot(data, aes(x=var1)) +\n  geom_histogram()\n\n Is the distribution consistent with a sample from a\n\nnormal distribution?\nuniform distribution?\nskewed distribution?\nMANY OTHER POTENTIAL DISTRIBUTIONS\n\n\nPotential simulation methods from specific distributions\n\n# Symmetric, unimodal, bell-shaped\nnull_dist(\"var1\", \"norm\")\nnull_dist(\"var1\", \"cauchy\")\nnull_dist(\"var1\", \"t\")\n\n# Skewed right\nnull_dist(\"var1\", \"exp\")\nnull_dist(\"var1\", \"chisq\")\nnull_dist(\"var1\", \"gamma\")\n\n# Constant \nnull_dist(\"var1\", \"uniform\")"
  },
  {
    "objectID": "week5/slides.html#lineup-of-greens-first-preference-percentages",
    "href": "week5/slides.html#lineup-of-greens-first-preference-percentages",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Lineup of Greens first preference percentages",
    "text": "Lineup of Greens first preference percentages\n\n📊ExplanationR\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the exponential distribution as the null says that we expect most electorates to have small tallies for Greens, and only a few electorates will have large, potentially winnable tallies.\nNOTE: We’ve already seen the data so we can’t be impartial judges for choosing the most different plot. We can use the null plots to check whether the small mode of moderately high tallies is unusual if the tallies really are samples from an exponential distribution.\n\n\n\nlibrary(nullabor)\nset.seed(241)\nggplot(lineup(null_dist(\"perc_GRN\", \"exp\"), tdf3, n=10),\n       aes(x=perc_GRN)) +\n  geom_histogram(color = \"white\", fill = \"#00843D\", bins = 30) +\n  facet_wrap(~.sample, ncol=5, scales=\"free\") +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.grid.major = element_blank())"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-68",
    "href": "week5/slides.html#australian-federal-election-68",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (6/8)",
    "text": "2019 Australian Federal Election (6/8)\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% of first preference for the Greens\n\n\n\n\nState\nMean\nMedian\nSD\nIQR\nSkewness\nKurtosis\n\n\n\n\nACT\n16.4\n14.0\n5.6\n5.20\n0.65\n1.5\n\n\nVIC\n11.4\n8.6\n8.2\n6.72\n2.60\n11.4\n\n\nWA\n11.0\n10.8\n3.0\n3.12\n0.80\n3.0\n\n\nQLD\n9.8\n8.8\n5.1\n4.75\n1.09\n3.9\n\n\nTAS\n9.7\n9.3\n4.0\n0.98\n0.33\n2.5\n\n\nNT\n9.6\n9.6\n2.5\n1.75\n0.00\n1.0\n\n\nSA\n9.1\n8.9\n3.0\n3.41\n0.38\n2.9\n\n\nNSW\n8.1\n6.6\n4.1\n3.95\n1.50\n4.9\n\n\nNational\n9.9\n8.5\n5.6\n5.00\n2.67\n15.8\n\n\n\n\n\n\n\nWhy are the means and the medians different?\nHow are the standard deviations and the interquartile ranges similar or different?\nAre there some other numerical statistics we should show?\n\n\n\n\n\ntdf3 &lt;- df1 |&gt;\n  group_by(DivisionID) |&gt;\n  summarise(\n    DivisionNm = unique(DivisionNm),\n    State = unique(StateAb),\n    votes_GRN = TotalVotes[which(PartyAb == \"GRN\")],\n    votes_total = sum(TotalVotes)\n  ) |&gt;\n  mutate(perc_GRN = votes_GRN / votes_total * 100)\n\n\n\n\ntdf3 |&gt;\n  group_by(State) |&gt;\n  summarise(\n    mean = mean(perc_GRN),\n    median = median(perc_GRN),\n    sd = sd(perc_GRN),\n    iqr = IQR(perc_GRN),\n    skewness = moments::skewness(perc_GRN),\n    kurtosis = moments::kurtosis(perc_GRN)\n  ) |&gt;\n  arrange(desc(mean)) |&gt;\n  rbind(data.frame(\n    State = \"National\",\n    mean = mean(tdf3$perc_GRN),\n    median = median(tdf3$perc_GRN),\n    sd = sd(tdf3$perc_GRN),\n    iqr = IQR(tdf3$perc_GRN),\n    skewness = moments::skewness(tdf3$perc_GRN),\n    kurtosis = moments::kurtosis(tdf3$perc_GRN)\n  )) |&gt;\n  knitr::kable(col.names = c(\"State\", \"Mean\", \"Median\", \"SD\", \"IQR\", \"Skewness\", \"Kurtosis\"), digits = 3) |&gt;\n  kableExtra::kable_classic() |&gt;\n  kableExtra::add_header_above(c(\" \", \"% of first preference for the Greens\" = 4, \" \" = 2)) |&gt;\n  kableExtra::row_spec(9, extra_css = \"border-top: 2px solid black;\")"
  },
  {
    "objectID": "week5/slides.html#robust-measure-of-central-tendency",
    "href": "week5/slides.html#robust-measure-of-central-tendency",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Robust measure of central tendency",
    "text": "Robust measure of central tendency\n\n\n\nMean is a non-robust measure of location.\nMedian is the 50% quantile of the observations\nTrimmed mean is the sample mean after discarding observations at the tails.\nWinsorized mean is the sample mean after replacing observations at the tails with the minimum or maximum of the observations that remain.\n\nBoth trimmed and Winsorized mean trimmed 20% of the tails.\n\n\n     \n\n\n\n\n\n\nPlot\nMean\nMedian\nTrimmed Mean\nWinsorized Mean\n\n\n\n\n1\n0.109\n0.114\n0.120\n0.103\n\n\n2\n0.054\n-0.045\n-0.016\n-0.029\n\n\n3\n1.177\n0.729\n0.820\n0.888\n\n\n4\n0.533\n0.541\n0.543\n0.542\n\n\n5\n0.468\n0.329\n0.355\n0.390\n\n\n6\n5.626\n6.656\n5.918\n5.688"
  },
  {
    "objectID": "week5/slides.html#robust-measure-of-dispersion",
    "href": "week5/slides.html#robust-measure-of-dispersion",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Robust measure of dispersion",
    "text": "Robust measure of dispersion\n\n\n\nStandard deviation or its square, variance, is a popular choice of measure of dispersion but is not robust to outliers\nStandard deviation for sample \\(x_1, ..., x_n\\) is\n\n\\[\\sqrt{\\sum_{i=1}^n \\frac{(x_i - \\bar{x})^2}{n - 1}}\\]\n\nInterquartile range difference between 1st and 3rd quartile, more robust measure of spread\nMedian absolute deviance (MAD) is even more robust\n\n\\[\\text{median}(|x_i - \\text{median}(x_i)|)\\]\n\n\n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure of dispersion\n\n\n\n\nPlot\nSD\nIQR\nMAD\nSkewness\nKurtosis\n\n\n\n\n1\n0.90\n1.19\n0.87\n-0.072\n3.0\n\n\n2\n0.99\n1.41\n1.08\n0.358\n2.2\n\n\n3\n1.33\n1.18\n0.79\n1.944\n7.2\n\n\n4\n0.29\n0.45\n0.34\n-0.126\n1.8\n\n\n5\n0.47\n0.50\n0.34\n1.691\n6.4\n\n\n6\n2.78\n5.36\n2.98\n-0.351\n1.7"
  },
  {
    "objectID": "week5/slides.html#inference-for-robust-statistics",
    "href": "week5/slides.html#inference-for-robust-statistics",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Inference for robust statistics",
    "text": "Inference for robust statistics\n\n\nWe have seen the re-sampling methods simulation and permutation used for generating null plots in a lineup. Re-sampling methods can be used with numeric statistics also.\nSimulation from distribution, can be used to to check for outliers.\n\n\n\n\n\n\n\n\n\nWe can also compute how many simulated values are more than the observed which gives a simulation \\(p\\)-value: 0.61.\n\n\nFor sample means, conventional tests provide a means for assessing what might be observed if different samples were taken.\nBootstrapping the current sample, can be used for robust statistics. If we have a sample of values:\n\n\n[1] 2 2 3 6 7 7 8 8\n\n\nto bootstrap sample with replacement:\n\nsort(sample(x, replace=TRUE))\n\n[1] 2 2 3 3 7 7 7 7\n\nsort(sample(x, replace=TRUE))\n\n[1] 2 3 6 6 6 6 8 8\n\n\n\nHere’s an example of bootstrapping to get a confidence interval for a median.\n\n\n[1] \"Median: 6.34\"\n\n\n[1] \"95% CI: ( 4.99 , 9.16 )\""
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-78",
    "href": "week5/slides.html#australian-federal-election-78",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (7/8)",
    "text": "2019 Australian Federal Election (7/8)\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere are these electorates?\nThe width of the boxplot is proportional to the number of electoral districts in the corresponding state (which is roughly proportional to the population).\n\n\n\n\ntdf3 &lt;- df1 |&gt;\n  group_by(DivisionID) |&gt;\n  summarise(\n    DivisionNm = unique(DivisionNm),\n    State = unique(StateAb),\n    votes_GRN = TotalVotes[which(PartyAb == \"GRN\")],\n    votes_total = sum(TotalVotes)\n  ) |&gt;\n  mutate(perc_GRN = votes_GRN / votes_total * 100)\n\n\n\n\ntdf3 |&gt;\n  mutate(State = fct_reorder(State, perc_GRN)) |&gt;\n  ggplot(aes(perc_GRN, State)) +\n  geom_boxplot(varwidth = TRUE) +\n  labs(\n    x = \"First preference votes %\",\n    y = \"Count\",\n    title = \"Greens party\"\n  )"
  },
  {
    "objectID": "week5/slides.html#outliers",
    "href": "week5/slides.html#outliers",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Outliers",
    "text": "Outliers\n\nOutliers are observations that are significantly different from the majority.\n\n\n\n\n\nOutliers can occur by chance in almost all distributions, but could be indicative of:\n\na measurement error,\na different population, or\nan issue with the sampling process."
  },
  {
    "objectID": "week5/slides.html#closer-look-at-the-boxplot",
    "href": "week5/slides.html#closer-look-at-the-boxplot",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Closer look at the boxplot",
    "text": "Closer look at the boxplot\n\n\nObservations that are outside the range of lower to upper fence (1.5 times the box length) are often referred to as outliers.\nPlotting boxplots for data from a skewed distribution will almost always show these “outliers” but these are not necessarily outliers.\nSome definitions of outliers assume a symmetrical population distribution (e.g. in boxplots or observations a certain standard deviations away from the mean) and these definitions are ill-suited for asymmetrical distributions.\nDeclaring observations outliers typically requires additional data context.\n\n\nWhat cannot be seen from boxplots?"
  },
  {
    "objectID": "week5/slides.html#australian-federal-election-88",
    "href": "week5/slides.html#australian-federal-election-88",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election (8/8)",
    "text": "2019 Australian Federal Election (8/8)\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow what do you notice from this graph that you didn’t notice before?\n\n\nOnly two electoral districts in NT.\nAnd only 3 and 5 electoral districts in ACT and TAS, respectively!\nBoxplots requires 5 points!\nWe should have summarised the number of electoral districts for each state with numerical statistics as a first step.\nAlso the outlier (yes, safe to call this an outlier!) and the cluster in the Victoria electorates.\n\n\n\n\n\n\ntdf3 &lt;- df1 |&gt;\n  group_by(DivisionID) |&gt;\n  summarise(\n    DivisionNm = unique(DivisionNm),\n    State = unique(StateAb),\n    votes_GRN = TotalVotes[which(PartyAb == \"GRN\")],\n    votes_total = sum(TotalVotes)\n  ) |&gt;\n  mutate(perc_GRN = votes_GRN / votes_total * 100)\n\n\n\n\ntdf3 |&gt;\n  mutate(State = fct_reorder(State, perc_GRN)) |&gt;\n  ggplot(aes(perc_GRN, State)) +\n  ggbeeswarm::geom_quasirandom(groupOnX = FALSE, varwidth = TRUE) +\n  labs(\n    x = \"First preference votes %\",\n    y = \"State\",\n    title = \"Greens party\"\n  )"
  },
  {
    "objectID": "week5/slides.html#both-numerical-and-graphical-summaries-can-reveal-andor-hide-aspects-of-the-data",
    "href": "week5/slides.html#both-numerical-and-graphical-summaries-can-reveal-andor-hide-aspects-of-the-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Both numerical and graphical summaries can reveal and/or hide aspects of the data",
    "text": "Both numerical and graphical summaries can reveal and/or hide aspects of the data"
  },
  {
    "objectID": "week5/slides.html#transformations",
    "href": "week5/slides.html#transformations",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Transformations",
    "text": "Transformations"
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-16",
    "href": "week5/slides.html#melbourne-housing-prices-16",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (1/6)",
    "text": "Melbourne Housing Prices (1/6)\n\n\n\n\n\n\n\nSuburb\nRooms\nType\nPrice ($)\nDate\n\n\n\n\nAbbotsford\n3\nHome\n1,490,000\n2017-04-01\n\n\nAbbotsford\n3\nHome\n1,220,000\n2017-04-01\n\n\nAbbotsford\n3\nHome\n1,420,000\n2017-04-01\n\n\nAberfeldie\n3\nHome\n1,515,000\n2017-04-01\n\n\nAirport West\n2\nHome\n670,000\n2017-04-01\n\n\nAirport West\n2\nTownhouse\n530,000\n2017-04-01\n\n\nAirport West\n2\nUnit\n540,000\n2017-04-01\n\n\nAirport West\n3\nHome\n715,000\n2017-04-01\n\n\nAlbanvale\n6\nHome\nNA\n2017-04-01\n\n\nAlbert Park\n3\nHome\n1,925,000\n2017-04-01\n\n\nAlbion\n3\nUnit\n515,000\n2017-04-01\n\n\nAlbion\n4\nHome\n717,000\n2017-04-01\n\n\nAlphington\n2\nHome\n1,675,000\n2017-04-01\n\n\nAlphington\n4\nHome\n2,008,000\n2017-04-01\n\n\nAltona\n2\nHome\n860,000\n2017-04-01\n\n\nAltona Meadows\n4\nHome\nNA\n2017-04-01\n\n\nAltona North\n3\nHome\n720,000\n2017-04-01\n\n\nArmadale\n2\nUnit\n836,000\n2017-04-01\n\n\nArmadale\n2\nHome\n2,110,000\n2017-04-01\n\n\nArmadale\n3\nHome\n1,386,000\n2017-04-01\n\n\n\n\n\n\n\nThis data was scraped each week from domain.com.au from 2016-01-28 to 2018-10-13\nIn total there are 63,023 observations\nAll variables shown (there are more variables not shown here), except price, have complete records\nThe are 48,433 property prices across Melbourne (roughly 23% missing)\n\nData source: Tony Pio (2018) Melbourne Housing Market\nHow would you explore this data first?\n\nYes, with an overview plot."
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-26",
    "href": "week5/slides.html#melbourne-housing-prices-26",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (2/6)",
    "text": "Melbourne Housing Prices (2/6)\n\n📊dataRlineupR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs missingness more likely for expensive houses?\n\nCheck with a lineup\n\n\n\n\n\n\n\n\n\n\n\n\nTo impute missings other variables will need to be used.\n\nNote: Houses with more than 8 rooms removed. Why?\n\n\n\n\n\ndf2 &lt;- read_csv(here::here(\"data/MELBOURNE_HOUSE_PRICES_LESS.csv\"),\n  col_types = cols(\n    .default = col_character(),\n    Rooms = col_double(),\n    Price = col_double(),\n    Date = col_date(format = \"%d/%m/%Y\"),\n    Propertycount = col_double(),\n    Distance = col_double()\n  )\n)\n\n\nskimr::skim(df2)\n\n── Data Summary ────────────────────────\n                           Values\nName                       df2   \nNumber of rows             63023 \nNumber of columns          13    \n_______________________          \nColumn type frequency:           \n  character                8     \n  Date                     1     \n  numeric                  4     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────\n  skim_variable n_missing complete_rate min max empty\n1 Suburb                0             1   3  18     0\n2 Address               0             1   7  27     0\n3 Type                  0             1   1   1     0\n4 Method                0             1   1   2     0\n5 SellerG               0             1   1  27     0\n6 Postcode              0             1   4   4     0\n7 Regionname            0             1  16  26     0\n8 CouncilArea           0             1  17  30     0\n  n_unique whitespace\n1      380          0\n2    57754          0\n3        3          0\n4        9          0\n5      476          0\n6      225          0\n7        8          0\n8       34          0\n\n── Variable type: Date ─────────────────────────────────────\n  skim_variable n_missing complete_rate min       \n1 Date                  0             1 2016-01-28\n  max        median     n_unique\n1 2018-10-13 2017-09-03      112\n\n── Variable type: numeric ──────────────────────────────────\n  skim_variable n_missing complete_rate      mean         sd\n1 Rooms                 0         1          3.11      0.958\n2 Price             14590         0.768 997898.   593499.   \n3 Propertycount         0         1       7618.     4424.   \n4 Distance              0         1         12.7       7.59 \n     p0    p25      p50       p75       p100 hist \n1     1      3      3         4         31   ▇▁▁▁▁\n2 85000 620000 830000   1220000   11200000   ▇▁▁▁▁\n3    39   4380   6795     10412      21650   ▅▇▅▂▁\n4     0      7     11.4      16.7       64.1 ▇▆▁▁▁\n\n\n\n\n\ndf2 |&gt;\n  select(Suburb, Rooms, Type, Price, Date) |&gt;\n  arrange(Suburb, Date) |&gt;\n  visdat::vis_miss()\n\n\ndf2 |&gt;\n  mutate(miss = ifelse(is.na(Price), \n    \"Missing\", \"Recorded\")) |&gt;\n  count(Rooms, miss) |&gt;\n  filter(Rooms &lt; 8) |&gt;\n  group_by(miss) |&gt;\n  mutate(perc = n / sum(n) * 100) |&gt;\n  ggplot(aes(as.factor(Rooms), perc, fill = miss)) +\n    geom_col(position = \"dodge\") +\n    scale_fill_viridis_d(begin=0.3, end=0.7) +\n    labs(x = \"Rooms\", y = \"Percentage\", fill = \"Price\") +\n    theme(aspect.ratio = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs there a suspicious plot?\n\n\n\nBreak the association between Rooms and Missing/Not on Price, because the null hypothesis is that there is no difference in missing status for price based on the size of the house. Why?\n\nlibrary(nullabor)\ndf2_d &lt;- df2 |&gt;\n  mutate(miss = ifelse(is.na(Price), \"Missing\", \"Recorded\")) |&gt;\n  select(Rooms, miss) |&gt;\n  filter(Rooms &lt; 8)\ndf2_l &lt;- lineup(null_permute(\"miss\"), df2_d, n=10, pos=7) \ndf2_l_agg &lt;- df2_l |&gt;\n  group_by(.sample) |&gt;\n  count(Rooms, miss) |&gt;\n  ungroup() |&gt;\n  group_by(miss) |&gt;\n  mutate(perc = n / sum(n) * 100) |&gt;\n  mutate(Rooms = as.factor(Rooms))\nggplot(df2_l_agg, aes(x=Rooms, y=perc, fill = miss)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_viridis_d(begin=0.3, end=0.7) +\n  facet_wrap(~.sample, ncol=5) +\n  theme(legend.position = \"none\", \n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.grid.major.x = element_blank())"
  },
  {
    "objectID": "week5/slides.html#your-turn",
    "href": "week5/slides.html#your-turn",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "🧩 Your turn",
    "text": "🧩 Your turn\n\n\nWhat might be alternative plots? Especially to reveal the relationship more clearly."
  },
  {
    "objectID": "week5/slides.html#check-the-support-of-your-data",
    "href": "week5/slides.html#check-the-support-of-your-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Check the support of your data",
    "text": "Check the support of your data\n\nIf you have too few measurements in any region (extreme), summaries for these regions will be unreliable.\n\n\nFor quantitative variables, it may be necessary to remove extremes.\nIf the variable is categorical it might be best to combine levels.\nIt is important to script so decisions can be reversed or rare events are not ignored.\n\n We removed houses with 8 or more rooms. What other way might we have handled these houses?"
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-36",
    "href": "week5/slides.html#melbourne-housing-prices-36",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (3/6)",
    "text": "Melbourne Housing Prices (3/6)\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat can we say from this plot?\n\n\nThe housing prices are right-skewed\nThere appears to be a lot of outlying housing prices (how can we tell?)\n\n\nNote: We determined that it is likely that more higher price houses have not disclosed the sale price. The distribution of price will need to be checked again after imputation.\n\n\n\n\n\n\ndf2 &lt;- read_csv(here::here(\"data/MELBOURNE_HOUSE_PRICES_LESS.csv\"),\n  col_types = cols(\n    .default = col_character(),\n    Rooms = col_double(),\n    Price = col_double(),\n    Date = col_date(format = \"%d/%m/%Y\"),\n    Propertycount = col_double(),\n    Distance = col_double()\n  )\n)\n\n\n\n\ndf2 |&gt;\n  ggplot(aes(Price / 1e6)) +\n  geom_histogram(color = \"white\") +\n  labs(\n    x = \"Price (mil)\",\n    y = \"Count\"\n  )"
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-46",
    "href": "week5/slides.html#melbourne-housing-prices-46",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (4/6)",
    "text": "Melbourne Housing Prices (4/6)\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe x-axis has been \\(\\log_{10}\\)-transformed in this plot\nThe plot appears more symmetrical now\nWhat is a useful measure of central tendency here?\n\n\n\n\n\ndf2 &lt;- read_csv(here::here(\"data/MELBOURNE_HOUSE_PRICES_LESS.csv\"),\n  col_types = cols(\n    .default = col_character(),\n    Rooms = col_double(),\n    Price = col_double(),\n    Date = col_date(format = \"%d/%m/%Y\"),\n    Propertycount = col_double(),\n    Distance = col_double()\n  )\n)\n\n\n\n\ndf2 |&gt;\n  ggplot(aes(Price / 1e6)) +\n  geom_histogram(color = \"white\") +\n  labs(\n    x = \"Price (mil)\",\n    y = \"Count\"\n  ) +\n  scale_x_log10()"
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-56",
    "href": "week5/slides.html#melbourne-housing-prices-56",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (5/6)",
    "text": "Melbourne Housing Prices (5/6)\n\nCentral tendencyR\n\n\nWith no transformation:\n\n\n\n\n\nMean\nMedian\nTrimmed Mean\nWinsorised Mean\n\n\n\n\n$997,898\n$830,000\n$871,375\n$903,823\n\n\n\n\n\n\nWith log transformation (and back-transformed to original scale):\n\n\n\n\n\nMean\nMedian\nTrimmed Mean\nWinsorised Mean\n\n\n\n\n$874,166\n$830,000\n$847,973\n$859,325\n\n\n\n\n\n\n\n\ndf2 |&gt;\n  filter(!is.na(Price)) |&gt;\n  summarise(\n    Mean = scales::dollar(mean(Price)),\n    Median = scales::dollar(median(Price)),\n    `Trimmed Mean` = scales::dollar(mean(Price, trim = 0.2)),\n    `Winsorised Mean` = scales::dollar(psych::winsor.mean(Price))\n  ) |&gt;\n  knitr::kable(align = \"r\") |&gt;\n  kableExtra::kable_classic() |&gt;\n  kableExtra::kable_styling(full_width=FALSE)\n\n\ndf2 |&gt;\n  filter(!is.na(Price)) |&gt;\n  mutate(lPrice = log10(Price)) |&gt;\n  summarise(\n    Mean = scales::dollar(10^mean(lPrice)),\n    Median = scales::dollar(10^median(lPrice)),\n    `Trimmed Mean` = scales::dollar(10^mean(lPrice, trim = 0.2)),\n    `Winsorised Mean` = scales::dollar(10^psych::winsor.mean(lPrice))\n  ) |&gt;\n  knitr::kable(align = \"r\") |&gt;\n  kableExtra::kable_classic() |&gt;\n  kableExtra::kable_styling(full_width=FALSE)"
  },
  {
    "objectID": "week5/slides.html#general-rules-for-transform-quantitative-variables",
    "href": "week5/slides.html#general-rules-for-transform-quantitative-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "General rules for transform quantitative variables",
    "text": "General rules for transform quantitative variables\n\n\nNon-shape changing, scaling:\n\nstandardise to mean 0, sd 1\nstandardise to min 0, max 1\nz-score\n\nShape changing, transformations: Remember the ladder of power transformations. (eg transforming left-skewed to more uniform using \\(^2\\))\n\n\n\n\n\n\n\n\n\n\nDistribution changing: quantile\n\n\n\n\n\n\n\n\n\nSome features cannot be fixed: gaps, multimodality, heaping. You need to find some explaining variable.\nSome features can be artificially fixed: discreteness. If regularly discretized, add random uniform noise to spread equally between gaps."
  },
  {
    "objectID": "week5/slides.html#multi-modality",
    "href": "week5/slides.html#multi-modality",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Multi-modality",
    "text": "Multi-modality"
  },
  {
    "objectID": "week5/slides.html#melbourne-housing-prices-66",
    "href": "week5/slides.html#melbourne-housing-prices-66",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Melbourne Housing Prices (6/6)",
    "text": "Melbourne Housing Prices (6/6)\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution from side-by-side univariate plots shows that higher number of rooms generally are pricier.\nThis strata could be responsible for multimodality in price distribution, even though it is not visible in the histogram.\nAccounting for rooms is important.\n\n\n\n\n\ndf2 &lt;- read_csv(here::here(\"data/MELBOURNE_HOUSE_PRICES_LESS.csv\"),\n  col_types = cols(\n    .default = col_character(),\n    Rooms = col_double(),\n    Price = col_double(),\n    Date = col_date(format = \"%d/%m/%Y\"),\n    Propertycount = col_double(),\n    Distance = col_double()\n  )\n)\n\n\n\n\ndf2 |&gt;\n  filter(Rooms &lt; 8) |&gt;\n  ggplot(aes(x=as.factor(Rooms), y=Price / 1e6, )) +\n  ggbeeswarm::geom_quasirandom(varwidth=TRUE, alpha=0.3) +\n  scale_y_log10() +\n  labs(y = \"Price (mil)\", x = \"# of Rooms\")"
  },
  {
    "objectID": "week5/slides.html#bins-and-bandwidths-more-details",
    "href": "week5/slides.html#bins-and-bandwidths-more-details",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Bins and Bandwidths: More details",
    "text": "Bins and Bandwidths: More details"
  },
  {
    "objectID": "week5/slides.html#hidalgo-stamps-thickness",
    "href": "week5/slides.html#hidalgo-stamps-thickness",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hidalgo stamps thickness",
    "text": "Hidalgo stamps thickness\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFamous historical example\n\nA stamp collector, Walton von Winkle, bought several collections of Mexican stamps from 1872-1874 and measured the thickness of all of them.\nThe different bandwidth for the density plot suggests different possibilities for number of modes.\n\nWhich do you think most accurately reflects what’s in the data?\n\n\n\n\nload(here::here(\"data/Hidalgo1872.rda\"))\nskimr::skim(Hidalgo1872)\n\n── Data Summary ────────────────────────\n                           Values     \nName                       Hidalgo1872\nNumber of rows             485        \nNumber of columns          3          \n_______________________               \nColumn type frequency:                \n  numeric                  3          \n________________________              \nGroup variables            None       \n\n── Variable type: numeric ──────────────────────────────────\n  skim_variable n_missing complete_rate   mean      sd    p0\n1 thickness             0         1     0.0860 0.0150  0.06 \n2 thicknessA          195         0.598 0.0922 0.0162  0.068\n3 thicknessB          289         0.404 0.0768 0.00508 0.06 \n     p25   p50   p75  p100 hist \n1 0.075  0.08  0.098 0.131 ▅▇▃▂▁\n2 0.0772 0.092 0.105 0.131 ▇▃▆▃▂\n3 0.072  0.078 0.08  0.097 ▁▃▇▁▁\n\n\n\n\n\nhid &lt;- ggplot(Hidalgo1872, aes(x=thickness, y=25)) +\n  geom_quasirandom(width=15, alpha=0.5, size=1) +\n  labs(x = \"Thickness (0.001 mm)\", y = \"Density\") + \n  ylim(c(0, 70)) +\n  theme(aspect.ratio = 0.6) \nhid_p1 &lt;- hid +\n  geom_density(aes(x=thickness), alpha=0.5,\n    color = \"#E16A86\", bw = 0.01, linewidth=2,\n    inherit.aes = FALSE)\nhid_p2 &lt;- hid +\n  geom_density(aes(x=thickness), alpha=0.5,\n    color = \"#E16A86\", bw = 0.0075, linewidth=2,\n    inherit.aes = FALSE) \nhid_p3 &lt;- hid +\n  geom_density(aes(x=thickness), alpha=0.5,\n    color = \"#E16A86\", bw = 0.004, linewidth=2,\n    inherit.aes = FALSE) \nhid_p4 &lt;- hid +\n  geom_density(aes(x=thickness), alpha=0.5,\n    color = \"#E16A86\", bw = 0.001, linewidth=2,\n    inherit.aes = FALSE) \nhid_p1 + hid_p2 + hid_p3 + hid_p4 + plot_layout(ncol=2)"
  },
  {
    "objectID": "week5/slides.html#olive-oil-content",
    "href": "week5/slides.html#olive-oil-content",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Olive oil content",
    "text": "Olive oil content\n\n\n\n\n\n\n\n\n\n\n\nWhat do you see?\n\nMixture of discreteness and normal shape of continuous values. Why might this happen?\n\n\n\nCheck if there is a difference in the strata (here 1 thru 9), implying measurement policy differences."
  },
  {
    "objectID": "week5/slides.html#re-focus",
    "href": "week5/slides.html#re-focus",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Re-focus",
    "text": "Re-focus"
  },
  {
    "objectID": "week5/slides.html#movie-length",
    "href": "week5/slides.html#movie-length",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Movie length",
    "text": "Movie length\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpon further exploration, you can find the two movies that are well over 16 hours long are “Cure for Insomnia”, “Four Stars”, and “Longest Most Meaningless Movie in the World”\nWe can restrict our attention to films under 3 hours:\n\n\n\n\n\n\n\n\n\n\n\nNotice that there is a peak at particular times. Why do you think so?\n\n\n\n\n\n\ndata(movies, package = \"ggplot2movies\")\nskimr::skim(movies)\n\n── Data Summary ────────────────────────\n                           Values\nName                       movies\nNumber of rows             58788 \nNumber of columns          24    \n_______________________          \nColumn type frequency:           \n  character                2     \n  numeric                  22    \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────\n  skim_variable n_missing complete_rate min max empty\n1 title                 0             1   1 121     0\n2 mpaa                  0             1   0   5 53864\n  n_unique whitespace\n1    56007          0\n2        5          0\n\n── Variable type: numeric ──────────────────────────────────\n   skim_variable n_missing complete_rate          mean\n 1 year                  0        1          1976.    \n 2 length                0        1            82.3   \n 3 budget            53573        0.0887 13412513.    \n 4 rating                0        1             5.93  \n 5 votes                 0        1           632.    \n 6 r1                    0        1             7.01  \n 7 r2                    0        1             4.02  \n 8 r3                    0        1             4.72  \n 9 r4                    0        1             6.37  \n10 r5                    0        1             9.80  \n11 r6                    0        1            13.0   \n12 r7                    0        1            15.5   \n13 r8                    0        1            13.9   \n14 r9                    0        1             8.95  \n15 r10                   0        1            16.9   \n16 Action                0        1             0.0797\n17 Animation             0        1             0.0628\n18 Comedy                0        1             0.294 \n19 Drama                 0        1             0.371 \n20 Documentary           0        1             0.0591\n21 Romance               0        1             0.0807\n22 Short                 0        1             0.161 \n             sd   p0      p25       p50        p75\n 1       23.7   1893   1958      1983       1997  \n 2       44.3      1     74        90        100  \n 3 23350085.       0 250000   3000000   15000000  \n 4        1.55     1      5         6.1        7  \n 5     3830.       5     11        30        112  \n 6       10.9      0      0         4.5        4.5\n 7        5.96     0      0         4.5        4.5\n 8        6.45     0      0         4.5        4.5\n 9        7.59     0      0         4.5        4.5\n10        9.73     0      4.5       4.5       14.5\n11       11.0      0      4.5      14.5       14.5\n12       11.6      0      4.5      14.5       24.5\n13       11.3      0      4.5      14.5       24.5\n14        9.44     0      4.5       4.5       14.5\n15       15.7      0      4.5      14.5       24.5\n16        0.271    0      0         0          0  \n17        0.243    0      0         0          0  \n18        0.455    0      0         0          1  \n19        0.483    0      0         0          1  \n20        0.236    0      0         0          0  \n21        0.272    0      0         0          0  \n22        0.367    0      0         0          0  \n          p100 hist \n 1      2005   ▁▁▃▃▇\n 2      5220   ▇▁▁▁▁\n 3 200000000   ▇▁▁▁▁\n 4        10   ▁▃▇▆▁\n 5    157608   ▇▁▁▁▁\n 6       100   ▇▁▁▁▁\n 7        84.5 ▇▁▁▁▁\n 8        84.5 ▇▁▁▁▁\n 9       100   ▇▁▁▁▁\n10       100   ▇▁▁▁▁\n11        84.5 ▇▂▁▁▁\n12       100   ▇▃▁▁▁\n13       100   ▇▃▁▁▁\n14       100   ▇▁▁▁▁\n15       100   ▇▃▁▁▁\n16         1   ▇▁▁▁▁\n17         1   ▇▁▁▁▁\n18         1   ▇▁▁▁▃\n19         1   ▇▁▁▁▅\n20         1   ▇▁▁▁▁\n21         1   ▇▁▁▁▁\n22         1   ▇▁▁▁▂\n\n\n\n\n\nggplot(movies, aes(length)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Length of movie (minutes)\", y = \"Frequency\") +\n  theme(aspect.ratio = 0.6)\n\nggplot(movies, aes(length)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Length of movie (minutes)\", y = \"Frequency\") +\n  scale_x_log10() +\n  theme(aspect.ratio = 0.6)\n\n\nmovies |&gt;\n  filter(length &lt; 180) |&gt;\n  ggplot(aes(length)) +\n  geom_histogram(binwidth = 1, fill = \"#795549\", color = \"black\") +\n  labs(x = \"Length of movie (minutes)\", y = \"Frequency\")"
  },
  {
    "objectID": "week5/slides.html#categorical-variables",
    "href": "week5/slides.html#categorical-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Categorical variables",
    "text": "Categorical variables"
  },
  {
    "objectID": "week5/slides.html#there-are-two-types-of-categorical-variables",
    "href": "week5/slides.html#there-are-two-types-of-categorical-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "There are two types of categorical variables",
    "text": "There are two types of categorical variables\n\nNominal where there is no intrinsic ordering to the categories E.g. blue, grey, black, white.\n\nOrdinal where there is a clear order to the categories. E.g. Strongly disagree, disagree, neutral, agree, strongly agree."
  },
  {
    "objectID": "week5/slides.html#categorical-variables-in-r",
    "href": "week5/slides.html#categorical-variables-in-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Categorical variables in R",
    "text": "Categorical variables in R\n\n\n\nIn R, categorical variables may be encoded as factors.\n\n\ndata &lt;- c(2, 2, 1, 1, 3, 3, 3, 1)\nfactor(data)\n\n[1] 2 2 1 1 3 3 3 1\nLevels: 1 2 3\n\n\n\nYou can easily change the labels of the variables:\n\n\nfactor(data, labels = c(\"I\", \"II\", \"III\"))\n\n[1] II  II  I   I   III III III I  \nLevels: I II III\n\n\n\n\nOrder of the factors are determined by the input:\n\n\n# numerical input are ordered in increasing order \nfactor(c(1, 3, 10))\n\n[1] 1  3  10\nLevels: 1 3 10\n\n# character input are ordered by first char, alphabetically \nfactor(c(\"1\", \"3\", \"10\"))\n\n[1] 1  3  10\nLevels: 1 10 3\n\n# you can specify order of levels explicitly \nfactor(c(\"1\", \"3\", \"10\"),\n  levels = c(\"1\", \"3\", \"10\")\n)\n\n[1] 1  3  10\nLevels: 1 3 10"
  },
  {
    "objectID": "week5/slides.html#numerical-summaries-counts-proportions-percentages-and-odds",
    "href": "week5/slides.html#numerical-summaries-counts-proportions-percentages-and-odds",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Numerical summaries: counts, proportions, percentages and odds",
    "text": "Numerical summaries: counts, proportions, percentages and odds\n\n\nTuberculosis counts in Australia\n\n\nCode\noptions(digits=2)\ntb_oz |&gt;\n  filter(year &gt;= 2000) |&gt;\n  mutate(p = count/sum(count),\n         pct = p*100, \n         odds = count/count[year==2000]) |&gt;\n  print(n=100)\n\n\n# A tibble: 22 × 7\n   country   iso3   year count      p   pct  odds\n   &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Australia AUS    2000   982 0.0522  5.22 1    \n 2 Australia AUS    2001   953 0.0507  5.07 0.970\n 3 Australia AUS    2002  1008 0.0536  5.36 1.03 \n 4 Australia AUS    2003   926 0.0493  4.93 0.943\n 5 Australia AUS    2004  1036 0.0551  5.51 1.05 \n 6 Australia AUS    2005  1030 0.0548  5.48 1.05 \n 7 Australia AUS    2006  1127 0.0600  6.00 1.15 \n 8 Australia AUS    2007  1081 0.0575  5.75 1.10 \n 9 Australia AUS    2008  1182 0.0629  6.29 1.20 \n10 Australia AUS    2009  1176 0.0626  6.26 1.20 \n11 Australia AUS    2010  1146 0.0610  6.10 1.17 \n12 Australia AUS    2011  1202 0.0640  6.40 1.22 \n13 Australia AUS    2012  1259 0.0670  6.70 1.28 \n14 Australia AUS    2013   512 0.0272  2.72 0.521\n15 Australia AUS    2014   474 0.0252  2.52 0.483\n16 Australia AUS    2015   438 0.0233  2.33 0.446\n17 Australia AUS    2016   481 0.0256  2.56 0.490\n18 Australia AUS    2017   524 0.0279  2.79 0.534\n19 Australia AUS    2018   502 0.0267  2.67 0.511\n20 Australia AUS    2019   554 0.0295  2.95 0.564\n21 Australia AUS    2020   609 0.0324  3.24 0.620\n22 Australia AUS    2021   593 0.0316  3.16 0.604\n\n\n\nFor qualitative data, compute\n\ncount/frequency,\nproportion/percentage\nand sometimes, an odds ratio. Here we have used ratio relative to the count in year 2000.\n\n Note: For exploration, no rounding of digits was done, but to report you would need to make the numbers pretty."
  },
  {
    "objectID": "week5/slides.html#australian-federal-election",
    "href": "week5/slides.html#australian-federal-election",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "2019 Australian Federal Election",
    "text": "2019 Australian Federal Election\n\n📊dataR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSorting levels sensibly is (almost) always better when plotting\n\n\n\n\ndf1 &lt;- read_csv(here::here(\"data/HouseFirstPrefsByCandidateByVoteTypeDownload-24310.csv\"),\n  skip = 1,\n  col_types = cols(\n    .default = col_character(),\n    OrdinaryVotes = col_double(),\n    AbsentVotes = col_double(),\n    ProvisionalVotes = col_double(),\n    PrePollVotes = col_double(),\n    PostalVotes = col_double(),\n    TotalVotes = col_double(),\n    Swing = col_double()\n  )\n)\n\n\ntdf3 &lt;- df1 |&gt;\n  group_by(DivisionID) |&gt;\n  summarise(\n    DivisionNm = unique(DivisionNm),\n    State = unique(StateAb),\n    votes_GRN = TotalVotes[which(PartyAb == \"GRN\")],\n    votes_total = sum(TotalVotes)\n  ) |&gt;\n  mutate(perc_GRN = votes_GRN / votes_total * 100)\n\n\n\n\ntdf3 |&gt;\n  ggplot(aes(perc_GRN, State)) +\n  ggbeeswarm::geom_quasirandom(groupOnX = FALSE, varwidth = TRUE) +\n  labs(\n    x = \"First preference votes %\",\n    y = \"State\",\n    title = \"Greens party\"\n  )\n\n\ntdf3 |&gt;\n  mutate(State = fct_reorder(State, perc_GRN)) |&gt;\n  ggplot(aes(perc_GRN, State)) +\n  ggbeeswarm::geom_quasirandom(groupOnX = FALSE, varwidth = TRUE) +\n  labs(\n    x = \"First preference votes %\",\n    y = \"State\",\n    title = \"Greens party\"\n  )"
  },
  {
    "objectID": "week5/slides.html#order-nominal-variables-meaningfully",
    "href": "week5/slides.html#order-nominal-variables-meaningfully",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Order nominal variables meaningfully",
    "text": "Order nominal variables meaningfully\n Coding tip: use below functions to easily change the order of factor levels\n \n\nstats::reorder(factor, value, mean)\nforcats::fct_reorder(factor, value, median)\nforcats::fct_reorder2(factor, value1, value2, func)"
  },
  {
    "objectID": "week5/slides.html#visual-inference-1",
    "href": "week5/slides.html#visual-inference-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference",
    "text": "Visual inference\n\n\nTypical plot description:\n\nggplot(data, aes(x=var1)) +\n  geom_col()\n\nggplot(data, aes(x=var1)) +\n  geom_bar()\n\n Is the distribution consistent with a sample from a binomial distribution with a given p?\n\nPotential simulation method from binomial\n\n# Only one option\nnull_dist(\"var1\", \"binom\", \n  list(size=n, p=phat))"
  },
  {
    "objectID": "week5/slides.html#lineup-of-tuberculosis-count-between-sexes",
    "href": "week5/slides.html#lineup-of-tuberculosis-count-between-sexes",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Lineup of tuberculosis count between sexes",
    "text": "Lineup of tuberculosis count between sexes\n\n📊Use conventional testR\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor this problem there is nothing more to learn from a lineup that what can be learned from a conventional hypothesis test of \\(H_0: p=0.5\\).\n\nbinom.test(tb_oz_2012$count, n, p = 0.5, alternative = \"two.sided\")\n\n\n    Exact binomial test\n\ndata:  tb_oz_2012$count\nnumber of successes = 314, number of trials = 997,\np-value &lt;2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.29 0.34\nsample estimates:\nprobability of success \n                  0.31 \n\n\n\n\n\nlibrary(nullabor)\nset.seed(252)\ntb_oz_2012 &lt;- tb |&gt;\n  filter(iso3 == \"AUS\",\n         year == 2012) |&gt;\n  select(iso3, year, new_sp_m04:new_ep_m65) |&gt;\n  pivot_longer(cols=new_sp_m04:new_ep_m65, \n               names_to = \"var\", \n               values_to = \"count\") |&gt;\n  separate(var, into=c(\"new\", \"type\", \"sexage\")) |&gt;\n  select(-new) |&gt;\n  filter(!(sexage %in% c(\"sexunk014\", \"sexunk04\", \n                         \"sexunk15plus\", \"sexunk514\",\n                         \"f15plus\", \"m15plus\"))) |&gt;\n  group_by(sexage) |&gt;\n  summarise(count = sum(count, na.rm=TRUE)) |&gt;\n  mutate(sex=str_sub(sexage, 1, 1),\n         age=str_sub(sexage, 2, str_length(sexage))) |&gt;\n  group_by(sex) |&gt;\n  summarise(count=sum(count)) |&gt;\n  ungroup() |&gt;\n  mutate(sex01 = ifelse(sex==\"m\", 0, 1)) |&gt;\n  select(-sex)\n\nggplot(lineup(null_dist(\"count\", \"binom\", \n                        list(size=sum(tb_oz_2012$count),\n                               p=0.5)), \n              tb_oz_2012, n=10),\n       aes(x=sex01, y=count)) +\n  geom_col() +\n  facet_wrap(~.sample, ncol=5) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.grid.major = element_blank())"
  },
  {
    "objectID": "week5/slides.html#key-points",
    "href": "week5/slides.html#key-points",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Key points",
    "text": "Key points\n\nMake many plots\nDrill down to subsets provided by other variables\nChanging bins or binwidth/bandwidth in histogram, violin or density plots can paint a different picture\nTransformation can re-focus to different aspects of the same data\nCompute a variety of statistics, compare and contrast values\nConsider different representations of categorical variables\n\nreordering meaningfully,\nlumping low frequencies together,\nplot or table, pie or barplot,\ninclude a missing category\n\nBe careful in making conclusions when there are sub-regions with few observations"
  },
  {
    "objectID": "week5/slides.html#imputing-missings-for-univariate-distributions",
    "href": "week5/slides.html#imputing-missings-for-univariate-distributions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Imputing missings for univariate distributions",
    "text": "Imputing missings for univariate distributions\n\n\nQuantitative variable: Simulate from a fitted distribution.\n\ndf2 &lt;- df2 |&gt;\n  mutate(lPrice = log10(Price),\n         price_miss = ifelse(is.na(Price), \"yes\", \"no\"))\n\ndf2_smry &lt;- df2 |&gt;\n  summarise(m = mean(lPrice, na.rm=TRUE),\n            s = sd(lPrice, na.rm=TRUE))\nset.seed(1003)  \ndf2 &lt;- df2 |&gt;\n  rowwise() |&gt;\n  mutate(lPrice = ifelse(price_miss == \"yes\", \n    rnorm(1, df2_smry$m, df2_smry$s), lPrice)) |&gt;\n  mutate(Price = ifelse(price_miss == \"yes\", 10^lPrice, Price))\n\n\n\n\n\n\n\n\n\n\n\nCategorical variable: Simulate from multinomial.\n\n\n# A tibble: 7 × 2\n  age   count\n  &lt;chr&gt; &lt;dbl&gt;\n1 1524     27\n2 2534     48\n3 3544     15\n4 4554     11\n5 5564      9\n6 65       15\n7 u        12\n\n\n [1] 5 3 1 1 2 1 1 2 1 1 1 1\n\n\n# A tibble: 7 × 2\n  age   count\n  &lt;chr&gt; &lt;dbl&gt;\n1 1524     35\n2 2534     50\n3 3544     16\n4 4554     11\n5 5564     10\n6 65       15\n7 u        12\n\n\nimputeMulti library can automate for multiple variables."
  },
  {
    "objectID": "week5/slides.html#resources",
    "href": "week5/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\n\nUnwin (2015) Graphical Data Analysis with R\nHarrison, David, and Daniel L. Rubinfeld (1978) Hedonic Housing Prices and the Demand for Clean Air, Journal of Environmental Economics and Management 5 81-102. Original data.\nGilley, O.W. and R. Kelley Pace (1996) On the Harrison and Rubinfeld Data. Journal of Environmental Economics and Management 31 403-405. Provided corrections and examined censoring.\nMaindonald, John H. and Braun, W. John (2020). DAAG: Data Analysis and Graphics Data and Functions. R package version 1.24\nBritish Board of Trade (1990), Report on the Loss of the ‘Titanic’ (S.S.). British Board of Trade Inquiry Report (reprint). Gloucester, UK: Allan Sutton Publishing\nHand, D. J., Daly, F., McConway, K., Lunn, D. and Ostrowski, E. eds (1993) A Handbook of Small Data Sets. Chapman & Hall, Data set 285 (p. 229)  Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0\nFleiss JL (1993): The statistical basis of meta-analysis. Statistical Methods in Medical Research 2 121–145 Balduzzi S, Rücker G, Schwarzer G (2019), How to perform a meta-analysis with R: a practical tutorial, Evidence-Based Mental Health.\nJosse et al (2022) R-miss-tastic, https://rmisstastic.netlify.app"
  },
  {
    "objectID": "week4/worksheet.html",
    "href": "week4/worksheet.html",
    "title": "ETC5521 Worksheet Week 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\n\nCode\npenguins_nona &lt;- penguins |&gt;\n  select(bill_len, bill_dep, \n         flipper_len, body_mass, \n         species, sex) |&gt;\n  na.omit()\npenguins_fit &lt;- lm(body_mass~flipper_len,\n                   data=penguins_nona)\ntidy(penguins_fit)\nglance(penguins_fit)\npenguins_m &lt;- augment(penguins_fit, penguins_nona)\n#ggplot(penguins_m, aes(x=flipper_len, y=.resid)) +\n#  geom_hline(yintercept=1, colour=\"grey70\") +\n#  geom_point() +\n#  theme(aspect.ratio=1)"
  },
  {
    "objectID": "week4/worksheet.html#objectives",
    "href": "week4/worksheet.html#objectives",
    "title": "ETC5521 Worksheet Week 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\n\nCode\npenguins_nona &lt;- penguins |&gt;\n  select(bill_len, bill_dep, \n         flipper_len, body_mass, \n         species, sex) |&gt;\n  na.omit()\npenguins_fit &lt;- lm(body_mass~flipper_len,\n                   data=penguins_nona)\ntidy(penguins_fit)\nglance(penguins_fit)\npenguins_m &lt;- augment(penguins_fit, penguins_nona)\n#ggplot(penguins_m, aes(x=flipper_len, y=.resid)) +\n#  geom_hline(yintercept=1, colour=\"grey70\") +\n#  geom_point() +\n#  theme(aspect.ratio=1)"
  },
  {
    "objectID": "week4/worksheet.html#tasks",
    "href": "week4/worksheet.html#tasks",
    "title": "ETC5521 Worksheet Week 4",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\nCan we believe what we see?\n\nIn the previous week’s worksheet we subjectively evaluated the residual plot to determine if the model was a good fit or not. We’ll use randomisation to check any observations we made from the residual plot. The code below makes a lineup of the true plot against plots made with rotation residuals (nulls/good). When you run the code you will get a line decrypt(\"....\"), which you can copy and paste back in to the console window to get the location of the true plot (in case you forgot which it is). Does the true plot look like the null plots? If not, describe how it differs.\n\n\n\nCode\nggplot(lineup(null_lm(body_mass~flipper_len, method=\"rotate\"),\n              penguins_m),\n       aes(x=flipper_len, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n# Alternatively, we can use permutation\nggplot(lineup(null_permute(\"flipper_len\"),\n              penguins_m),\n       aes(x=flipper_len, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\nggplot(penguins_m, aes(x=flipper_len, y=.resid, colour=species)) +\n  geom_hline(yintercept=1, colour=\"grey70\") +\n  geom_point() +\n  theme(aspect.ratio=1)\n\n\n\nPick one group, males or females, and one of Adelie, Chinstrap or Gentoo, and choose two of the four measurements. Fit a linear model, and do a lineup of the residuals. Can you tell which is the true plot? Show your lineup to your tutorial partner or someone else nearby and ask them\n\n\nto pick the plot that is most different.\nexplain why they picked that plot.\n\nUsing your decrypt() code locate the true plot. Is the true plot different from the nulls?\nDid you or your friend choose the data plot? Was it identifiable from the lineup or indistinguishable from the null plots?"
  },
  {
    "objectID": "week4/worksheetsol.html",
    "href": "week4/worksheetsol.html",
    "title": "ETC5521 Worksheet Week 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\n\nCode\npenguins_nona &lt;- penguins |&gt;\n  select(bill_len, bill_dep, \n         flipper_len, body_mass, \n         species, sex) |&gt;\n  na.omit()\npenguins_fit &lt;- lm(body_mass~flipper_len,\n                   data=penguins_nona)\ntidy(penguins_fit)\nglance(penguins_fit)\npenguins_m &lt;- augment(penguins_fit, penguins_nona)\n#ggplot(penguins_m, aes(x=flipper_len, y=.resid)) +\n#  geom_hline(yintercept=1, colour=\"grey70\") +\n#  geom_point() +\n#  theme(aspect.ratio=1)"
  },
  {
    "objectID": "week4/worksheetsol.html#objectives",
    "href": "week4/worksheetsol.html#objectives",
    "title": "ETC5521 Worksheet Week 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\n\nCode\npenguins_nona &lt;- penguins |&gt;\n  select(bill_len, bill_dep, \n         flipper_len, body_mass, \n         species, sex) |&gt;\n  na.omit()\npenguins_fit &lt;- lm(body_mass~flipper_len,\n                   data=penguins_nona)\ntidy(penguins_fit)\nglance(penguins_fit)\npenguins_m &lt;- augment(penguins_fit, penguins_nona)\n#ggplot(penguins_m, aes(x=flipper_len, y=.resid)) +\n#  geom_hline(yintercept=1, colour=\"grey70\") +\n#  geom_point() +\n#  theme(aspect.ratio=1)"
  },
  {
    "objectID": "week4/worksheetsol.html#tasks",
    "href": "week4/worksheetsol.html#tasks",
    "title": "ETC5521 Worksheet Week 4",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\nCan we believe what we see?\n\nIn the previous week’s worksheet we subjectively evaluated the residual plot to determine if the model was a good fit or not. We’ll use randomisation to check any observations we made from the residual plot. The code below makes a lineup of the true plot against plots made with rotation residuals (nulls/good). When you run the code you will get a line decrypt(\"....\"), which you can copy and paste back in to the console window to get the location of the true plot (in case you forgot which it is). Does the true plot look like the null plots? If not, describe how it differs.\n\n\n\nCode\nggplot(lineup(null_lm(body_mass~flipper_len, method=\"rotate\"),\n              penguins_m),\n       aes(x=flipper_len, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n# Alternatively, we can use permutation\nggplot(lineup(null_permute(\"flipper_len\"),\n              penguins_m),\n       aes(x=flipper_len, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\nggplot(penguins_m, aes(x=flipper_len, y=.resid, colour=species)) +\n  geom_hline(yintercept=1, colour=\"grey70\") +\n  geom_point() +\n  theme(aspect.ratio=1)\n\n\n\nPick one group, males or females, and one of Adelie, Chinstrap or Gentoo, and choose two of the four measurements. Fit a linear model, and do a lineup of the residuals. Can you tell which is the true plot? Show your lineup to your tutorial partner or someone else nearby and ask them\n\n\nto pick the plot that is most different.\nexplain why they picked that plot.\n\nUsing your decrypt() code locate the true plot. Is the true plot different from the nulls?\nDid you or your friend choose the data plot? Was it identifiable from the lineup or indistinguishable from the null plots?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nThe true plot looks a little different from the nulls. It has more of a V shape, which might suggest that the model fits poorly, that the smaller penguins have a different relationship than the larger penguins. It is evidence to support fitting separate models to the sexes and species.\nSomething like the following\n\n\n\nCode\npenguins_f_adelie &lt;- penguins_nona |&gt;\n  filter(species == \"Adelie\",\n         sex == \"female\")\npenguins_f_adelie_bl_bd_fit &lt;- \n  lm(bill_dep ~ bill_len,\n     data=penguins_f_adelie)\npenguins_f_adelie_bl_bd_m &lt;-\n  augment(penguins_f_adelie_bl_bd_fit)\nggplot(lineup(null_lm(bill_dep ~ \n                        bill_len,\n                      method=\"rotate\"),\n              penguins_f_adelie_bl_bd_m),\n       aes(x=bill_len, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n\nI would not be able to distinguish which is the true plot in this lineup."
  },
  {
    "objectID": "week4/slides.html#what-this-class-is-about",
    "href": "week4/slides.html#what-this-class-is-about",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What this class is about",
    "text": "What this class is about"
  },
  {
    "objectID": "week4/slides.html#revisiting-hypothesis-testing",
    "href": "week4/slides.html#revisiting-hypothesis-testing",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Revisiting hypothesis testing",
    "text": "Revisiting hypothesis testing"
  },
  {
    "objectID": "week4/slides.html#frequentist-hypothesis-testing-framework",
    "href": "week4/slides.html#frequentist-hypothesis-testing-framework",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "(Frequentist) hypothesis testing framework",
    "text": "(Frequentist) hypothesis testing framework\n\n\nSuppose \\(X\\) is the number of heads out of \\(n\\) independent tosses.\nLet \\(p\\) be the probability of getting a  for this coin.\n\nHypotheses\n\\(H_0: p = 0.5\\) vs. \\(H_a: p &gt; 0.5\\). Note \\(p_0=0.5\\).  Alternative \\(H_a\\) is saying we believe that the coin is biased to heads. \nNOTE: Alternative needs to be decided before seeing data.\n\nAssumptions Each toss is independent with equal chance of getting a head.\n\n\nTest statistic\n\\(X \\sim B(n, p)\\). Recall \\(E(X\\mid H_0) = np_0\\). We observe \\(n, x, \\widehat{p}\\). Test statistic is \\(\\widehat{p} - p_0\\).\n\n\nP-value  (or critical value or confidence interval) \\(P(X ~ \\geq ~ x\\mid H_0)\\)\n\n\nConclusion Reject null hypothesis when the \\(p\\)-value is less than some significance level \\(\\alpha\\). Usually \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "week4/slides.html#testing-coin-bias-14",
    "href": "week4/slides.html#testing-coin-bias-14",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Testing coin bias (1/4)",
    "text": "Testing coin bias (1/4)\n\nSuppose I have a coin that I’m going to flip  \nIf the coin is unbiased, what is the probability it will show heads?\n\n\n\nYup, the probability should be 0.5.\nSo how would I test if a coin is biased or unbiased?\nWe’ll collect some data."
  },
  {
    "objectID": "week4/slides.html#testing-coin-bias-24",
    "href": "week4/slides.html#testing-coin-bias-24",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Testing coin bias (2/4)",
    "text": "Testing coin bias (2/4)\n\nExperiment 1: I flipped the coin 10 times and this is the result:\n\n\n\nset.seed(924)\nsamp10 &lt;- sample(rep(c(head, tail), c(7, 3)))\ncat(paste0(samp10, collapse = \"\"))\n\n\n\n\nThe result is 7 head and 3 tails. So 70% are heads.\nDo you believe the coin is biased based on this data?"
  },
  {
    "objectID": "week4/slides.html#testing-coin-bias-34",
    "href": "week4/slides.html#testing-coin-bias-34",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Testing coin bias (3/4)",
    "text": "Testing coin bias (3/4)\n\nExperiment 2: Suppose now I flip the coin 100 times and this is the outcome:\n\n\nsamp100 &lt;- sample(rep(c(head, tail), c(70, 30)))\ncat(paste0(samp100, collapse = \"\"))\n\n\n\nWe observe 70 heads and 30 tails. So again 70% are heads.\nBased on this data, do you think the coin is biased?"
  },
  {
    "objectID": "week4/slides.html#testing-coin-bias-44",
    "href": "week4/slides.html#testing-coin-bias-44",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Testing coin bias (4/4)",
    "text": "Testing coin bias (4/4)\nCalculate it\n\n\nExperiment 1 (n=10)\n\nWe observed \\(x=7\\), or \\(\\widehat{p} = 0.7\\).\nAssuming \\(H_0\\) is true, we expect \\(np=10\\times 0.5=5\\).\nCalculate the \\(P(X \\geq 7)\\)\n\n \n\n\nsum(dbinom(7:10, 10, 0.5))\n\n[1] 0.17\n\n\n\n\nExperiment 2 (n=100)\n\nWe observed \\(x=70\\), or \\(\\widehat{p} = 0.7\\).\nAssuming \\(H_0\\) is true, we expect \\(np=100\\times 0.5=50\\).\nCalculate the \\(P(X \\geq 70)\\)\n\n \n\n\nsum(dbinom(70:100, 100, 0.5))\n\n[1] 3.9e-05"
  },
  {
    "objectID": "week4/slides.html#why-is-the-null-hypothesis-always-specific",
    "href": "week4/slides.html#why-is-the-null-hypothesis-always-specific",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Why is the null hypothesis always specific?",
    "text": "Why is the null hypothesis always specific?\nYou need to be able to calculate the probability of something happening, if the null was true."
  },
  {
    "objectID": "week4/slides.html#judicial-system",
    "href": "week4/slides.html#judicial-system",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Judicial system",
    "text": "Judicial system\n\n\n\n\n\n\n\n\n\nEvidence by test statistic Judgement by \\(p\\)-value, critical value or confidence interval\n\n\nDoes the test statistic have to be numerical?"
  },
  {
    "objectID": "week4/slides.html#visual-inference",
    "href": "week4/slides.html#visual-inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference",
    "text": "Visual inference"
  },
  {
    "objectID": "week4/slides.html#visual-inference-1",
    "href": "week4/slides.html#visual-inference-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference",
    "text": "Visual inference\n\n\n\nHypothesis testing in a visual inference framework is where:\n\nthe test statistic is a plot and\njudgement is by human visual perception.\n\n\n\nWhy is the plot a test statistic? We’ll see why soon.\n\n\n\n\nYou, we, me actually do visual inference many times but generally in an informal fashion.\nThe problem with doing this is we are making an inference on whether the plot has any patterns based on a single data plot.\nThe single data plot needs to be examined in the context of what might this look like if different samples were shown."
  },
  {
    "objectID": "week4/slides.html#reasons-to-use-visual-inference",
    "href": "week4/slides.html#reasons-to-use-visual-inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Reasons to use visual inference",
    "text": "Reasons to use visual inference\n\nData plots tend to be over-interpreted.\nReading data plots requires calibration."
  },
  {
    "objectID": "week4/slides.html#visual-inference-more-formally",
    "href": "week4/slides.html#visual-inference-more-formally",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference more formally",
    "text": "Visual inference more formally\n\n\n\nState your null and alternate hypotheses.\nDefine a visual test statistic, \\(V(.)\\), i.e. a function of a sample to a plot.\nDefine a method to generate null data, \\(\\boldsymbol{y}_0\\).\n\\(V(\\boldsymbol{y})\\) maps the actual data, \\(\\boldsymbol{y}\\), to the plot. We call this the data plot.\n\\(V(\\boldsymbol{y}_0)\\) maps a null data to a plot of the same form. We call this the null plot. We repeat this \\(m - 1\\) times to generate \\(m-1\\) null plots.\nA lineup displays these \\(m\\) plots in a random order.\nAsk \\(n\\) human viewers to select a plot in the lineup that looks different to others without any context given."
  },
  {
    "objectID": "week4/slides.html#visual-inference-more-formally-1",
    "href": "week4/slides.html#visual-inference-more-formally-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference more formally",
    "text": "Visual inference more formally\n\n\n\nState your null and alternate hypotheses.\nDefine a visual test statistic, \\(V(.)\\), i.e. a function of a sample to a plot.\nDefine a method to generate null data, \\(\\boldsymbol{y}_0\\).\n\\(V(\\boldsymbol{y})\\) maps the actual data, \\(\\boldsymbol{y}\\), to the plot. We call this the data plot.\n\\(V(\\boldsymbol{y}_0)\\) maps a null data to a plot of the same form. We call this the null plot. We repeat this \\(m - 1\\) times to generate \\(m-1\\) null plots.\nA lineup displays these \\(m\\) plots in a random order.\nAsk \\(n\\) human viewers to select a plot in the lineup that looks different to others without any context given.\n\n\n\nSuppose \\(x\\) out of \\(n\\) people detected the data plot from a lineup, then\n\nthe visual inference p-value is given as \\[P(X \\geq x)\\] where \\(X \\sim B(n, 1/m)\\), and\nthe power of a lineup is estimated as \\(x/n\\)."
  },
  {
    "objectID": "week4/slides.html#two-residual-plots-examples-seen-last-week",
    "href": "week4/slides.html#two-residual-plots-examples-seen-last-week",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Two residual plots examples seen last week",
    "text": "Two residual plots examples seen last week"
  },
  {
    "objectID": "week4/slides.html#lineup-which-plot-has-a-pattern-that-is-different-from-other-plots",
    "href": "week4/slides.html#lineup-which-plot-has-a-pattern-that-is-different-from-other-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Lineup: Which plot has a pattern that is different from other plots?",
    "text": "Lineup: Which plot has a pattern that is different from other plots?\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals from dist~speed using datasets::cars (week 3).\n\nlm(dist ~ speed, data = cars)\n\n\n\nThis is a lineup of the residual plot\nWhich plot (if any) looks different from the others?\nWhy do you think it looks different?\n\n\n\n\n&gt; decrypt(\"clZx bKhK oL 3OHohoOL 0B\")\n[1] \"True data in position  11\"\n  How do we calculate statistical significance from this?"
  },
  {
    "objectID": "week4/slides.html#visual-inference-p-value-or-see-value",
    "href": "week4/slides.html#visual-inference-p-value-or-see-value",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference \\(p\\)-value (or “see”-value)",
    "text": "Visual inference \\(p\\)-value (or “see”-value)\n\n\n\nSo \\(x\\) out of \\(n\\) people chose the data plot.\nSo the visual inference \\(p\\)-value is \\(P(X \\geq x)\\) where \\(X \\sim B(n, 1/10)\\).\nIn R, this is\n\n1 - pbinom(x - 1, n, 1/20) \n# OR \nnullabor::pvisual(x, n, 20)\n\nThe calculation is made with the assumption that the chance of a single observer randomly chooses the true plot is 1/20.\n\n\nSuppose \\(x=2\\) out of \\(n=16\\) people chose plot 11 (previous slide).\nThe probability that this happens by random guessing (p-value) is\n\n1 - pbinom(2 - 1, 16, 1/20)\n\n[1] 0.19\n\nnullabor::pvisual(2, 16, 20)\n\n     x simulated binom\n[1,] 2       0.2  0.19"
  },
  {
    "objectID": "week4/slides.html#lineup-which-plot-has-a-pattern-that-is-different-from-other-plots-1",
    "href": "week4/slides.html#lineup-which-plot-has-a-pattern-that-is-different-from-other-plots-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Lineup: Which plot has a pattern that is different from other plots?",
    "text": "Lineup: Which plot has a pattern that is different from other plots?\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals from log-transformed price~carat ggplot2::diamonds (week 3).\n\nd_fit &lt;- lm(lprice ~ lcarat, data=diamonds)\n\n\n\nThis is a lineup of the residual plot for the model where both carat and price are log-transformed\nWhich plot (if any) looks different from the others?\nWhy do you think it looks different?\n\n\n\n\n&gt; decrypt(\"clZx bKhK oL 3OHohoOL 0Q\")\n[1] \"True data in position  15\""
  },
  {
    "objectID": "week4/slides.html#visual-inference-p-value-or-see-value-1",
    "href": "week4/slides.html#visual-inference-p-value-or-see-value-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Visual inference \\(p\\)-value (or “see”-value)",
    "text": "Visual inference \\(p\\)-value (or “see”-value)\n\n\nSuppose \\(x=8\\) out of \\(n=12\\) people chose plot 15 (previous slide).\nThe probability that this happens by random guessing (p-value) is\n\n1 - pbinom(8 - 1, 12, 1/20)\n\n[1] 1.6e-08\n\nnullabor::pvisual(8, 12, 20)\n\n     x simulated   binom\n[1,] 8         0 1.6e-08\n\n\n\nThis is basically impossible to happen by chance.\n Next, how the residuals are different from “good” residuals has to be determined by the follow-up question: how did you decide your chosen plot was different?\n Plot 15 has a different variance pattern, it’s not the regular up-down pattern seen in the other plots. This suggests that there is some heteroskedasticity in the data that is not captured by the error distribution in the model."
  },
  {
    "objectID": "week4/slides.html#new-residual-plot-examples",
    "href": "week4/slides.html#new-residual-plot-examples",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "New residual plot examples",
    "text": "New residual plot examples"
  },
  {
    "objectID": "week4/slides.html#residual-plot-13",
    "href": "week4/slides.html#residual-plot-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plot (1/3)",
    "text": "Residual plot (1/3)\n\n\n Is there a problem with the model?"
  },
  {
    "objectID": "week4/slides.html#residual-plot-23",
    "href": "week4/slides.html#residual-plot-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plot (2/3)",
    "text": "Residual plot (2/3)\n\n\n Is there a problem with the model?"
  },
  {
    "objectID": "week4/slides.html#residual-plot-33",
    "href": "week4/slides.html#residual-plot-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plot (3/3)",
    "text": "Residual plot (3/3)\n\n\n Is there a problem with the model?"
  },
  {
    "objectID": "week4/slides.html#residual-plots-need-context",
    "href": "week4/slides.html#residual-plots-need-context",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plots need context",
    "text": "Residual plots need context\n You are asked to decide IF THERE IS NO PATTERN. This is hard!  \nResidual plots are better when viewed in the context of good residual plots, where we know the assumptions of the model are satisfied."
  },
  {
    "objectID": "week4/slides.html#which-is-the-worst-residual-plot",
    "href": "week4/slides.html#which-is-the-worst-residual-plot",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Which is the worst residual plot?",
    "text": "Which is the worst residual plot?\n\n\n\n\n\n\n\n\n\n\n\n\n19 of these plots are good residual (null) plots."
  },
  {
    "objectID": "week4/slides.html#section",
    "href": "week4/slides.html#section",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "All of the residual plots shown slides 22-24 were NULL plots."
  },
  {
    "objectID": "week4/slides.html#section-1",
    "href": "week4/slides.html#section-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The actual residual plot is"
  },
  {
    "objectID": "week4/slides.html#its-not-only-for-residual-plots",
    "href": "week4/slides.html#its-not-only-for-residual-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "It’s not only for residual plots",
    "text": "It’s not only for residual plots"
  },
  {
    "objectID": "week4/slides.html#sports-analytics-basketball",
    "href": "week4/slides.html#sports-analytics-basketball",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Sports analytics: basketball",
    "text": "Sports analytics: basketball\n\n\n Which plot is most different?"
  },
  {
    "objectID": "week4/slides.html#time-series-cross-currency-rates",
    "href": "week4/slides.html#time-series-cross-currency-rates",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Time series: cross-currency rates",
    "text": "Time series: cross-currency rates\n\n\n Which plot is most different?"
  },
  {
    "objectID": "week4/slides.html#association-cars",
    "href": "week4/slides.html#association-cars",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Association: cars",
    "text": "Association: cars\n\n\n Which plot is most different?"
  },
  {
    "objectID": "week4/slides.html#spatial-analysis-cancer-incidence",
    "href": "week4/slides.html#spatial-analysis-cancer-incidence",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Spatial analysis: cancer incidence",
    "text": "Spatial analysis: cancer incidence\n\n\n Which plot is most different?\n From Steff Kobakian’s Master’s thesis"
  },
  {
    "objectID": "week4/slides.html#reading-any-plot-is-easier-in-the-context-of-null-plots",
    "href": "week4/slides.html#reading-any-plot-is-easier-in-the-context-of-null-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Reading any plot is easier in the context of null plots",
    "text": "Reading any plot is easier in the context of null plots"
  },
  {
    "objectID": "week4/slides.html#why-is-a-data-plot-a-statistic",
    "href": "week4/slides.html#why-is-a-data-plot-a-statistic",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Why is a data plot a statistic?",
    "text": "Why is a data plot a statistic?"
  },
  {
    "objectID": "week4/slides.html#why-is-a-data-plot-a-statistic-12",
    "href": "week4/slides.html#why-is-a-data-plot-a-statistic-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Why is a data plot a statistic? (1/2)",
    "text": "Why is a data plot a statistic? (1/2)\n\nThe concept of tidy data matches elementary statistics\nTabular form puts variables in columns and observations in rows\n\n\\[X = \\left[ \\begin{array}{rrrr}\n           X_1 & X_2 & ... & X_p\n           \\end{array} \\right] \\\\\n  = \\left[ \\begin{array}{rrrr}\n           X_{11} & X_{12} & ... & X_{1p} \\\\\n           X_{21} & X_{22} & ... & X_{2p} \\\\\n           \\vdots & \\vdots & \\ddots& \\vdots \\\\\n           X_{n1} & X_{n2} & ... & X_{np}\n           \\end{array} \\right]\\]\n\nVariables can have distributions, e.g. \\(X_1 \\sim N(0,1), ~~X_2 \\sim \\text{Exp}(1) ...\\)"
  },
  {
    "objectID": "week4/slides.html#why-is-a-data-plot-a-statistic-22",
    "href": "week4/slides.html#why-is-a-data-plot-a-statistic-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Why is a data plot a statistic? (2/2)",
    "text": "Why is a data plot a statistic? (2/2)\n\n\nA statistic is a function on the values of items in a sample, e.g. for \\(n\\) iid random variates \\(\\bar{X}_1=\\sum_{i=1}^n X_{i1}\\), \\(s_1^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_{i1}-\\bar{X}_1)^2\\)\nWe study the behaviour of the statistic over all possible samples of size \\(n\\).\nThe grammar of graphics is the mapping of (random) variables to graphical elements, making plots of data into statistics\n\n\n\n\n\nExample 1:\n\nggplot(threept_sub, \n       aes(x=angle, y=r)) + \n  geom_point(alpha=0.3)\n\n\n\nangle is mapped to the x axis\nr is mapped to the y axis\n\n\n\n\nExample 2:\n\nggplot(penguins, \n      aes(x=bl, \n          y=fl, \n          colour=species)) +\n  geom_point()\n\n\n\nbl is mapped to the x axis\nfl is mapped to the y axis\nspecies is mapped to colour\n\n\n\n\nExample 3:\n\nggplot(aud, aes(x=date, y=rate)) + \n  geom_line() \n\n\n\ndate is mapped to the x axis\nrate is mapped to the y axis\ndisplayed as a line geom"
  },
  {
    "objectID": "week4/slides.html#determining-the-null-hypothesis",
    "href": "week4/slides.html#determining-the-null-hypothesis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Determining the null hypothesis",
    "text": "Determining the null hypothesis"
  },
  {
    "objectID": "week4/slides.html#what-is-the-null-hypothesis-12",
    "href": "week4/slides.html#what-is-the-null-hypothesis-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is the null hypothesis? (1/2)",
    "text": "What is the null hypothesis? (1/2)\nTo determine the null hypothesis, you need to think about what pattern would NOT be interesting.\n\n\nA\nggplot(data) + \n  geom_point(aes(x=x1, y=x2))\n\nB\nggplot(data) + \n  geom_point(aes(x=x1, \n    y=x2, colour=cl))\n\nC\nggplot(data) + \n  geom_histogram(aes(x=x1))\n\nD\nggplot(data) + \n  geom_boxplot(aes(x=cl, y=x1))\n\n\n\n🤔 Which of these plot definitions would most match to a null hypothesis stating there is no difference in the distribution between the groups?"
  },
  {
    "objectID": "week4/slides.html#what-is-the-null-hypothesis-22",
    "href": "week4/slides.html#what-is-the-null-hypothesis-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is the null hypothesis? (2/2)",
    "text": "What is the null hypothesis? (2/2)\n\n\n\nA\n\\(H_o:\\) no association between x1 and x2\n \nB\n\\(H_o:\\) no difference in association of between x1 and x2 between levels of cl\n\nC\n\\(H_o:\\) the distribution of x1 is XXX\n \nD\n\\(H_o:\\) no difference in the distribution of x1 between levels of cl"
  },
  {
    "objectID": "week4/slides.html#how-do-you-generate-null-samples",
    "href": "week4/slides.html#how-do-you-generate-null-samples",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How do you generate null samples",
    "text": "How do you generate null samples"
  },
  {
    "objectID": "week4/slides.html#primary-null-generating-mechanisms",
    "href": "week4/slides.html#primary-null-generating-mechanisms",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Primary null-generating mechanisms",
    "text": "Primary null-generating mechanisms\n\nNull samples can be generated using two basic approaches:\n\nPermutation: randomizing the order of one of the variables breaks association, but keeps marginal distributions the same.\nSimulation: from a given distribution, or model. Assumption is that the data comes from that model.\n\napplied to subsets, or conditioning on other variables. Simulation may require computing summary statistics from the data to use as parameter estimates."
  },
  {
    "objectID": "week4/slides.html#association-cars-1",
    "href": "week4/slides.html#association-cars-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Association: cars",
    "text": "Association: cars\n\n\n\n\n\n\n\n\n\n\n\n\n\nNull plots generated by permuting x variable."
  },
  {
    "objectID": "week4/slides.html#time-series-cross-currency-rates-1",
    "href": "week4/slides.html#time-series-cross-currency-rates-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Time series: cross-currency rates",
    "text": "Time series: cross-currency rates\n\n\n\n\n\n\n\n\n\n\n\n\n Nulls generated by simulating from an ARIMA model."
  },
  {
    "objectID": "week4/slides.html#beyond-p-value-to-power",
    "href": "week4/slides.html#beyond-p-value-to-power",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Beyond \\(p\\)-value to power",
    "text": "Beyond \\(p\\)-value to power"
  },
  {
    "objectID": "week4/slides.html#what-is-power",
    "href": "week4/slides.html#what-is-power",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is power?",
    "text": "What is power?\n\n\n\nA statistic is said to be more powerful than another statistic if it has a higher probability of correctly rejecting the null hypothesis when the alternative hypothesis is true.\nThe effectiveness of two plots designs for the same data can be compared by computing power from a lineup.\nThe power of a lineup is calculated as \\(x/n\\) where \\(x\\) is the number of people who detected the data plot out of \\(n\\) people."
  },
  {
    "objectID": "week4/slides.html#which-of-these-plots-is-more-effective-for-assessing-difference-between-groups",
    "href": "week4/slides.html#which-of-these-plots-is-more-effective-for-assessing-difference-between-groups",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Which of these plots is more effective for assessing difference between groups?",
    "text": "Which of these plots is more effective for assessing difference between groups?"
  },
  {
    "objectID": "week4/slides.html#computing-the-power",
    "href": "week4/slides.html#computing-the-power",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Computing the power",
    "text": "Computing the power\nNote: Different people evaluated each lineup.\n\n\n\n\n\n\n\n\n\nPlot type\n\\(x\\)\n\\(n\\)\nPower\n\n\n\n\ngeom_point\n\\(x_1=4\\)\n\\(n_1=23\\)\n\\(x_1 / n_1=0.174\\)\n\n\ngeom_boxplot\n\\(x_2=5\\)\n\\(n_2=25\\)\n\\(x_2 / n_2=0.185\\)\n\n\ngeom_violin\n\\(x_3=6\\)\n\\(n_3=29\\)\n\\(x_3 / n_3=0.206\\)\n\n\nggbeeswarm::geom_quasirandom\n\\(x_4=8\\)\n\\(n_4=24\\)\n\\(x_4 / n_4=0.333\\)\n\n\n\n\n\n\nThe plot type with a higher power is preferable\nYou can use this framework to find the optimal plot design"
  },
  {
    "objectID": "week4/slides.html#using-the-nullabor",
    "href": "week4/slides.html#using-the-nullabor",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Using the nullabor 📦",
    "text": "Using the nullabor 📦"
  },
  {
    "objectID": "week4/slides.html#section-2",
    "href": "week4/slides.html#section-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "When you run the example yourself, you get a decrypt code line, that you run after deciding on a plot to print the location of the data plot amongst the nulls.\n\nplot is a scatterplot, null hypothesis is there is no association between the two variables mapped to the x, y axes\nnull generating mechanism: permutation\n\n\n\nset.seed(20190709)\nggplot(lineup(null_permute('mpg'), mtcars), \n  aes(x=mpg, y=wt)) +\n  geom_point() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())"
  },
  {
    "objectID": "week4/slides.html#some-considerations-in-visual-inference",
    "href": "week4/slides.html#some-considerations-in-visual-inference",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Some considerations in visual inference",
    "text": "Some considerations in visual inference\n\nIn practice you don’t want to bias the judgement of the human viewers so for a proper visual inference:\n\nyou should not show the data plot before the lineup\nyou should not give the context of the data\nyou should remove labels and other identifying information from plots\n\nThese methods can be used whenever formal inference is not possible/available, for EDA or IDA or diagnosing models.\nThe data collection is vital for good inference: bad data leads to bad inference.\nDetermining how to generate null samples can be complicated. We’ll see more examples throughout the next few weeks."
  },
  {
    "objectID": "week4/slides.html#resources",
    "href": "week4/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nBuja, Andreas, Dianne Cook, Heike Hofmann, Michael Lawrence, Eun-Kyung Lee, Deborah F. Swayne, and Hadley Wickham. 2009. “Statistical Inference for Exploratory Data Analysis and Model Diagnostics.” Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences 367 (1906): 4361–83.\nWickham, Hadley, Dianne Cook, Heike Hofmann, and Andreas Buja. 2010. “Graphical Inference for Infovis.” IEEE Transactions on Visualization and Computer Graphics 16 (6): 973–79.\nHofmann, H., L. Follett, M. Majumder, and D. Cook. 2012. “Graphical Tests for Power Comparison of Competing Designs.” IEEE Transactions on Visualization and Computer Graphics 18 (12): 2441–48.\nMajumder, M., Heiki Hofmann, and Dianne Cook. 2013. “Validation of Visual Statistical Inference, Applied to Linear Models.” Journal of the American Statistical Association 108 (503): 942–56."
  },
  {
    "objectID": "week3/worksheet.html",
    "href": "week3/worksheet.html",
    "title": "ETC5521 Worksheet Week 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\ninstall.packages(c(\"tidyverse\", \"ggbeeswarm\", \"broom\", \"visdat\"))"
  },
  {
    "objectID": "week3/worksheet.html#objectives",
    "href": "week3/worksheet.html#objectives",
    "title": "ETC5521 Worksheet Week 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\ninstall.packages(c(\"tidyverse\", \"ggbeeswarm\", \"broom\", \"visdat\"))"
  },
  {
    "objectID": "week3/worksheet.html#tasks",
    "href": "week3/worksheet.html#tasks",
    "title": "ETC5521 Worksheet Week 3",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. Take a glimpse of the penguins data. What types are variables are present in the data?\n\n\n2. How was this data collected? You will need to read the documentation for the palmerpenguins package, or see if AI knows.\n\n\n3. Using the visdat package make an overview plot to examine types of variables and for missing values.\n\n\n4. Check the distributions of each species on each of the size variables, using a jittered dotplot, using the geom_quasirandom() function in the ggbeeswarm package. There seems to be some bimodality in some species on some variables eg bill_len. Why do you think this might be? Check your thinking by making a suitable plot.\n\n\n5. Is there any indication of outliers from the jittered dotplots of different variables?\n\n\n6. Make a scatterplot of body_mass_g vs flipper_length_mm for all the penguins. What do the vertical stripes indicate? Are there any other unusual patterns to note, such as outliers or clustering or nonlinearity?\n\n\n7. How well can penguin body mass be predicted based on the flipper length? Fit a linear model to check. Report the equation, the \\(R^2\\), \\(\\sigma\\), and make a residual plot of residuals vs flipper_length_mm. From the residual plot, are there any concerns about the model fit?"
  },
  {
    "objectID": "week3/worksheetsol.html",
    "href": "week3/worksheetsol.html",
    "title": "ETC5521 Worksheet Week 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\ninstall.packages(c(\"tidyverse\", \"ggbeeswarm\", \"broom\", \"visdat\"))"
  },
  {
    "objectID": "week3/worksheetsol.html#objectives",
    "href": "week3/worksheetsol.html#objectives",
    "title": "ETC5521 Worksheet Week 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns.\n\ninstall.packages(c(\"tidyverse\", \"ggbeeswarm\", \"broom\", \"visdat\"))"
  },
  {
    "objectID": "week3/worksheetsol.html#tasks",
    "href": "week3/worksheetsol.html#tasks",
    "title": "ETC5521 Worksheet Week 3",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. Take a glimpse of the penguins data. What types are variables are present in the data?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nglimpse(penguins)\n\n\nRows: 344\nColumns: 8\n$ species     &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ad…\n$ island      &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Tor…\n$ bill_len    &lt;dbl&gt; 39, 40, 40, NA, 37, 39, 39, 39, 34, 42, 38, 38, 41, 39, 35…\n$ bill_dep    &lt;dbl&gt; 19, 17, 18, NA, 19, 21, 18, 20, 18, 20, 17, 17, 18, 21, 21…\n$ flipper_len &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180,…\n$ body_mass   &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, …\n$ sex         &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, …\n$ year        &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\n\n\n\n2. How was this data collected? You will need to read the documentation for the palmerpenguins package, or see if AI knows.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nDetails are at https://allisonhorst.github.io/palmerpenguins/articles/intro.html, and you learn “These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. The data were imported directly from the Environmental Data Initiative (EDI) Data Portal.” It is necessary to also read the original data collection article https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081 to obtain details of the sampling. Breeding pairs of penguins were included based on sampling of nests where pairs of adults were present, were chosen and marked, before onset of egg-laying.\n\n\n\n\n\n\n3. Using the visdat package make an overview plot to examine types of variables and for missing values.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThere are three factor variables - species, island, sex - and three integer variables - flipper_len, body_mass and year - and two numeric variables - bill_len and bill_dep.\nIt is interesting that flipper_len and body_mass are reported without decimal places, and thus are integers, whereas bill_len and bill_dep are reported with one decimal place, and thus are doubles. Both are numeric variables, though.\nFour variables have missing values: sex, bill_len, bill_dep, flipper_len, body_mass. There are more missings on sex. The other missings occur together, the penguins are missing on all five variables.\n\n\nCode\nlibrary(visdat)\nvis_dat(penguins)\n\n\n\n\n\n\n\n\n\nCode\np_nomiss &lt;- penguins |&gt;\n  select(species, bill_len, bill_dep, \n         flipper_len, body_mass) |&gt;\n  na.omit()\n\n\n\n\n\n\n\n\n4. Check the distributions of each species on each of the size variables, using a jittered dotplot, using the geom_quasirandom() function in the ggbeeswarm package. There seems to be some bimodality in some species on some variables eg bill_len. Why do you think this might be? Check your thinking by making a suitable plot.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nParticularly on bill_len multimodality can be seen in the Chinstrap and Gentoo penguins. This corresponds to differences in the two species. Differences can be seen in the sexes for all of the variables and species, but it is only big enough in bill_len to be noticeable as bimodality.\n\n\nCode\nlibrary(ggbeeswarm)\nggplot(penguins, aes(x=species, \n                     y=bill_len)) +\n  geom_quasirandom() \n\n\n\n\n\n\n\n\n\nCode\nggplot(penguins, aes(x=species, \n                     y=bill_len, \n                     colour=sex)) +\n  geom_quasirandom() +\n  scale_color_brewer(\"\", palette=\"Dark2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Is there any indication of outliers from the jittered dotplots of different variables?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nOutliers can be seen on bill_len for Chinstrap and Gentoo. Interestingly, there is one female penguins with a really big bill, bigger than all the males even.\nAlso on flipper_len there is one female penguins with a much smaller value than others.\n\n\n\n\n\n\n6. Make a scatterplot of body_mass_g vs flipper_length_mm for all the penguins. What do the vertical stripes indicate? Are there any other unusual patterns to note, such as outliers or clustering or nonlinearity?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe striping corresponds to rounded values of flipper length, that have been reported to the nearest mm. It appears that it is done across all measuring as it is present for both sexes, for all species, for each island and each year. Otherwise there is nothing much to report as unusual.\n\n\nCode\npenguins |&gt;\n  select(flipper_len, body_mass, species) |&gt;\n  na.omit() |&gt;\n  ggplot(aes(x=flipper_len,\n             y=body_mass, \n             colour=species)) +\n  geom_point() +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nCode\npenguins |&gt;\n  na.omit() |&gt;\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~island, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nCode\npenguins |&gt;\n  na.omit() |&gt;\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~sex, ncol=2, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nCode\npenguins |&gt;\n  na.omit() |&gt;\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~species, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nCode\npenguins |&gt;\n  na.omit() |&gt;\n  ggplot(aes(x=flipper_len,\n             y=body_mass)) +\n  geom_point() +\n  facet_wrap(~year, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\n⚠️ Note: That using na.omit() is VERY DANGEROUS. You should very rarely use it and only in very clear situations like this data.\n\n\n\n\n\n\n7. How well can penguin body mass be predicted based on the flipper length? Fit a linear model to check. Report the equation, the \\(R^2\\), \\(\\sigma\\), and make a residual plot of residuals vs flipper_length_mm. From the residual plot, are there any concerns about the model fit?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe model fit statistics suggest it is a reasonably good model, with flipper length explaining about 76% of body mass. From the estimated standard deviation of the error, \\(\\sigma=393\\), we could say that the estimated body mass is likely accurate to within 800g. (Assuming normal distribution and 95% of observations within two \\(\\sigma\\).)\nThe residual suggests no major problems. There is a little heteroskedasticity. Perhaps if you look carefully, though, it might indicate that different models should have been fitted for the smaller penguins and the bigger penguins, perhaps models separately for sex and species would be advised.\n\n\nCode\nlibrary(broom)\npenguins_nona &lt;- penguins |&gt;\n  select(flipper_len, body_mass, species) |&gt;\n  na.omit()\npenguins_fit &lt;- lm(body_mass~flipper_len,\n                   data=penguins_nona)\ntidy(penguins_fit)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -5781.     306.       -18.9 5.59e- 55\n2 flipper_len     49.7      1.52      32.7 4.37e-107\n\n\nCode\nglance(penguins_fit)\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.759         0.758  394.     1071. 4.37e-107     1 -2528. 5063. 5074.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nCode\npenguins_m &lt;- augment(penguins_fit)\nggplot(penguins_m, aes(x=flipper_len, y=.resid)) +\n  geom_hline(yintercept=1, colour=\"grey70\") +\n  geom_point() +\n  theme(aspect.ratio=1)"
  },
  {
    "objectID": "week3/slides.html#the-role-of-initial-data-analysis",
    "href": "week3/slides.html#the-role-of-initial-data-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "The role of initial data analysis",
    "text": "The role of initial data analysis\n\n\nThe first thing to do with data is to look at them …. usually means tabulating and plotting the data in many different ways to see what’s going on. With the wide availability of computer packages and graphics nowadays there is no excuse for ducking the labour of this preliminary phase, and it may save some red faces later.\nCrowder, M. J. & Hand, D. J. (1990) “Analysis of Repeated Measures”"
  },
  {
    "objectID": "week3/slides.html#initial-data-analysis-and-confirmatory-analysis",
    "href": "week3/slides.html#initial-data-analysis-and-confirmatory-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Initial Data Analysis and Confirmatory Analysis",
    "text": "Initial Data Analysis and Confirmatory Analysis\n\n\n\nPrior to conducting a confirmatory data analysis, it is important to conduct an initial data analysis (IDA).\n\n\n\nConfirmatory data analysis (CDA) is focused on statistical inference and includes procedures for:\n\nhypothesis testing,\npredictive modelling,\nparameter estimation including uncertainty,\nmodel selection.\n\n\n\n\n\n\nIDA includes:\n\ndescribing the data and collection procedures\nscrutinise data for errors, outliers, missing observations\ncheck assumptions for confirmatory data analysis\n\n\n\nIDA is sometimes called preliminary data analysis.\n\n\n\n\nIDA is related to exploratory data analysis (EDA) in the sense that it is primarily conducted graphically, and there are few formal tests available."
  },
  {
    "objectID": "week3/slides.html#taxonomies-are-useful-but-rarely-perfect",
    "href": "week3/slides.html#taxonomies-are-useful-but-rarely-perfect",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Taxonomies are useful but rarely perfect",
    "text": "Taxonomies are useful but rarely perfect"
  },
  {
    "objectID": "week3/slides.html#objectives-of-ida",
    "href": "week3/slides.html#objectives-of-ida",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Objectives of IDA?",
    "text": "Objectives of IDA?\n\nThe main objective for IDA is to intercept any problems in the data that might adversely affect the confirmatory data analysis.\n\n\n\nThe role of CDA is to answer the intended question(s) that the data were collected for.\n\n\nIDA is often unreported in the data analysis reports or scientific papers, for various reasons. It might not have been done, or it may have been conducted but there was no space in the paper to report on it."
  },
  {
    "objectID": "week3/slides.html#ida-in-government-statistics",
    "href": "week3/slides.html#ida-in-government-statistics",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA in government statistics",
    "text": "IDA in government statistics\nThe purpose of data cleaning is to bring data up to a level of quality such that it can reliably be used for the production of statistical models or statements.\nA statistical value chain is constructed by defining a number of meaningful intermediate data products, for which a chosen set of quality attributes are well described.\n\n\n\n\nvan der Loo & de Jonge (2018) Statistical Data Cleaning with Applications in R"
  },
  {
    "objectID": "week3/slides.html#ida-in-health-and-medical-data",
    "href": "week3/slides.html#ida-in-health-and-medical-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA in health and medical data",
    "text": "IDA in health and medical data\n\nHuebner et al (2018)’s six steps of IDA: (1) Metadata setup, (2) Data cleaning, (3) Data screening, (4) Initial reporting, (5) Refining and updating the analysis plan, (6) Reporting IDA in documentation."
  },
  {
    "objectID": "week3/slides.html#heed-these-words",
    "href": "week3/slides.html#heed-these-words",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Heed these words",
    "text": "Heed these words\n\n\n\nIDA prepares an analyst for CDA. One needs to be careful about NOT compromising the inference.\n\nHow do you compromise inference?\n\n\nChange your inference or questions based on what you find in IDA.\nOutlier removal or not.\nMissing value imputation choices.\nTreatment of zeros.\nHandling of variable type, categorical temporal.\nLack of multivariate relationship checking, including subsets based on levels of categorical variables.\nChoosing variables and observations.\n\n\n\n\nHow do you avoid these errors?\n\nDocument ALL the IDA, using a reproducible analysis script.\nPre-register your CDA plan, so that your CDA questions do not change.\nDecisions made on outlier removal, variable selection, recoding, sampling, handling of zeros have known affects on results, and are justifiable.\n\nInsure yourself against accusations of data snooping, data dredging, data fishing."
  },
  {
    "objectID": "week3/slides.html#data-screening",
    "href": "week3/slides.html#data-screening",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data screening",
    "text": "Data screening"
  },
  {
    "objectID": "week3/slides.html#data-screening-1",
    "href": "week3/slides.html#data-screening-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data screening",
    "text": "Data screening\n\n\n\nIt’s important to check how the data are understood by the computer.\n\nthat is, checking for data type:\n\nWas the date read in as character?\nWas a factor read in as numeric?\n\n\n\nAlso important for making inference is to know whether the data supports making broader conclusions. How was the data collected? Is it clear what the population of interest is, and that the data is a representative sample of this population?"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-12",
    "href": "week3/slides.html#example-checking-the-data-type-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type (1/2)",
    "text": "Example: Checking the data type (1/2)\n\n\nlecture3-example.xlsx\n\n\n\n\n\nlibrary(readxl)\nlibrary(here)\ndf &lt;- read_excel(here(\"data/lecture3-example.xlsx\"))\ndf\n\n# A tibble: 5 × 4\n     id date                loc       temp\n  &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;    &lt;dbl&gt;\n1     1 2010-01-03 00:00:00 New York  42  \n2     2 2010-02-03 00:00:00 New York  41.4\n3     3 2010-03-03 00:00:00 New York  38.5\n4     4 2010-04-03 00:00:00 New York  41.1\n5     5 2010-05-03 00:00:00 New York  39.8\n\n\n\nWhat problems are there with the computer’s interpretation of data type?\nWhat context specific issues indicate incorrect computer interpretation?"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-22",
    "href": "week3/slides.html#example-checking-the-data-type-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type (2/2)",
    "text": "Example: Checking the data type (2/2)\n\n\n\nlibrary(lubridate)\ndf &lt;- read_excel(here(\"data/lecture3-example.xlsx\"), \n                 col_types = c(\"text\", \n                               \"date\", \n                               \"text\",\n                               \"numeric\"))\n\ndf |&gt; \n  mutate(id = as.factor(id),\n         date = ydm(date)) |&gt;\n  mutate(\n         day = day(date),\n         month = month(date),\n         year = year(date)) \n\n# A tibble: 5 × 7\n  id    date       loc       temp   day month  year\n  &lt;fct&gt; &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1     2010-03-01 New York  42       1     3  2010\n2 2     2010-03-02 New York  41.4     2     3  2010\n3 3     2010-03-03 New York  38.5     3     3  2010\n4 4     2010-03-04 New York  41.1     4     3  2010\n5 5     2010-03-05 New York  39.8     5     3  2010\n\n\n\n\nid is now a factor instead of integer\nday, month and year are now extracted from the date\nIs it okay now?\n\n\n\n\nIn the United States, it’s common to use the date format MM/DD/YYYY (gasps) while the rest of the world commonly uses DD/MM/YYYY or better still YYYY/MM/DD.\n\n\n\n\nIt’s highly probable that the dates are 1st-5th March and not 3rd of Jan-May.\n\n\n\n\nYou can validate interpretation of temperature using weather database."
  },
  {
    "objectID": "week3/slides.html#example-specifying-the-data-type-with-r",
    "href": "week3/slides.html#example-specifying-the-data-type-with-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Specifying the data type with R",
    "text": "Example: Specifying the data type with R\n\n\n\nYou can robustify your workflow by ensuring you have a check for the expected data type in your code.\n\n\nxlsx_df &lt;- read_excel(here(\"data/lecture3-example.xlsx\"),\n                 col_types = c(\"text\", \"date\", \"text\", \"numeric\"))  |&gt; \n  mutate(id = as.factor(id), \n         date = as.character(date),\n         date = as.Date(date, format = \"%Y-%d-%m\"))\n\n\n\nread_csv has a broader support for col_types\n\n\ncsv_df &lt;- read_csv(here::here(\"data/lecture3-example.csv\"),\n                 col_types = cols(\n                      id = col_factor(),\n                      date = col_date(format = \"%m/%d/%y\"),\n                      loc = col_character(),\n                      temp = col_double()))\n\n\nThe checks (or coercions) ensure that even if the data are updated, you can have some confidence that any data type error will be picked up before further analysis."
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-with-r",
    "href": "week3/slides.html#example-checking-the-data-type-with-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type with R",
    "text": "Example: Checking the data type with R\nYou can have a quick glimpse of the data type with:\n\ndplyr::glimpse(xlsx_df)\n\nRows: 5\nColumns: 4\n$ id   &lt;fct&gt; 1, 2, 3, 4, 5\n$ date &lt;date&gt; 2010-03-01, 2010-03-02, 2010-03-03, 2010-03-0…\n$ loc  &lt;chr&gt; \"New York\", \"New York\", \"New York\", \"New Yor…\n$ temp &lt;dbl&gt; 42, 41, 38, 41, 40\n\ndplyr::glimpse(csv_df)\n\nRows: 5\nColumns: 4\n$ id   &lt;fct&gt; 1, 2, 3, 4, 5\n$ date &lt;date&gt; 2010-03-01, 2010-03-02, 2010-03-03, 2010-03-0…\n$ loc  &lt;chr&gt; \"New York\", \"New York\", \"New York\", \"New Yor…\n$ temp &lt;dbl&gt; 42, 41, 38, 41, 40"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-visually",
    "href": "week3/slides.html#example-checking-the-data-type-visually",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type visually",
    "text": "Example: Checking the data type visually\n\n\nYou can also visualise the data type with:\n\nlibrary(visdat)\nvis_dat(xlsx_df)\n\n\n\n\n\n\n\n\n\n\nlibrary(inspectdf)\ninspect_types(xlsx_df)  |&gt; \n  show_plot()"
  },
  {
    "objectID": "week3/slides.html#data-cleaning",
    "href": "week3/slides.html#data-cleaning",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning",
    "text": "Data cleaning"
  },
  {
    "objectID": "week3/slides.html#data-cleaning-12",
    "href": "week3/slides.html#data-cleaning-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning (1/2)",
    "text": "Data cleaning (1/2)\nData quality checks should be one of the first steps in the data analysis to assess any problems with the data.\nThese include using common or domain knowledge to check if the recorded data have sensible values.\n\n\nAre positive values, e.g. height and weight, recorded as positive values with a plausible range?\nIf the data are counts, do the recorded values contain non-integer values?\nFor compositional data, do the values add up to 100% (or 1)? If not, is that a measurement error or due to rounding? Or is another variable missing?\nDoes the data contain only positives, ie disease occurrences, or warranty claims? If so, what would the no report group look like?"
  },
  {
    "objectID": "week3/slides.html#data-cleaning-22",
    "href": "week3/slides.html#data-cleaning-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning (2/2)",
    "text": "Data cleaning (2/2)\nIn addition, numerical or graphical summaries may reveal that there is unwanted structure in the data, for example,\n\n\n\nDoes the treatment group have different demographic characteristics to the control group?\nAre the distributions similar between the training and test sets?\nAre there sufficient measurements for each level of categorical variable, or across the range of numerical variables?\n\n\n\n\nDoes the distribution of the data imply violations of assumptions for the CDA, such as\n\nnon-normality,\ndiscrete rather real-valued, or\ndifferent variance in different domains?\n\n\n\n\n\nData scrutinizing is a process that you get better at with practice and have familiarity with the domain area."
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-quality",
    "href": "week3/slides.html#example-checking-the-data-quality",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data quality",
    "text": "Example: Checking the data quality\n\n\n\n\n# A tibble: 9 × 4\n  id    date       loc        temp\n  &lt;fct&gt; &lt;date&gt;     &lt;chr&gt;     &lt;dbl&gt;\n1 1     2010-03-01 New York   42  \n2 2     2010-03-02 New York   41.4\n3 3     2010-03-03 New York   38.5\n4 4     2010-03-04 New York   41.1\n5 5     2010-03-05 New York   39.8\n6 6     2020-03-01 Melbourne  30.6\n7 7     2020-03-02 Melbourne  17.9\n8 8     2020-03-03 Melbourne  18.6\n9 9     2020-03-04 &lt;NA&gt;       21.3\n\n\n\n\nNumerical or graphical summaries or even just eye-balling the data helps to uncover some data quality issues.\nAny issues here?\n\n\n\n\nThere’s a missing value in loc.\nTemperature is in Farenheit for New York but Celsius in Melbourne (you can validate this again using external sources)."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-17",
    "href": "week3/slides.html#case-study-world-development-indicators-17",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (1/7)",
    "text": "Case study: World development indicators (1/7)\n\n\n\noptions(width=80)\nraw_dat &lt;- read_csv(here(\"data/world-development-indicators.csv\"), \n                    na = \"..\", n_max = 11935)\nglimpse(raw_dat)\n\nRows: 11,935\nColumns: 54\n$ `Country Name`  &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Argentina\", \"A…\n$ `Country Code`  &lt;chr&gt; \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\"…\n$ `Series Name`   &lt;chr&gt; \"Adolescent fertility rate (births per 1,000 women age…\n$ `Series Code`   &lt;chr&gt; \"SP.ADO.TFRT\", \"NV.AGR.TOTL.ZS\", \"ER.H2O.FWTL.ZS\", \"SH…\n$ `1969 [YR1969]` &lt;dbl&gt; 6.4e+01, 9.2e+00, NA, NA, 3.3e+00, NA, 2.2e+01, NA, NA…\n$ `1970 [YR1970]` &lt;dbl&gt; 6.5e+01, 9.6e+00, NA, NA, 3.5e+00, NA, 2.5e+01, NA, NA…\n$ `1971 [YR1971]` &lt;dbl&gt; 6.7e+01, 1.1e+01, NA, NA, 3.7e+00, NA, 2.4e+01, 8.7e+0…\n$ `1972 [YR1972]` &lt;dbl&gt; 6.8e+01, 1.1e+01, NA, NA, 3.6e+00, NA, 1.9e+01, 9.2e+0…\n$ `1973 [YR1973]` &lt;dbl&gt; 7.1e+01, 1.2e+01, NA, NA, 3.7e+00, NA, 2.7e+01, 9.6e+0…\n$ `1974 [YR1974]` &lt;dbl&gt; 7.5e+01, 1.0e+01, NA, NA, 3.7e+00, NA, 3.0e+01, 9.9e+0…\n$ `1975 [YR1975]` &lt;dbl&gt; 7.8e+01, 6.6e+00, NA, NA, 3.6e+00, NA, 2.9e+01, 1.0e+0…\n$ `1976 [YR1976]` &lt;dbl&gt; 8.1e+01, 8.2e+00, NA, NA, 3.8e+00, NA, 2.0e+01, 1.0e+0…\n$ `1977 [YR1977]` &lt;dbl&gt; 8.4e+01, 8.1e+00, 9.5e+00, NA, 3.7e+00, NA, 2.6e+01, 1…\n$ `1978 [YR1978]` &lt;dbl&gt; 8.2e+01, 7.5e+00, NA, NA, 3.8e+00, NA, 2.9e+01, 1.1e+0…\n$ `1979 [YR1979]` &lt;dbl&gt; 8.0e+01, 7.8e+00, NA, NA, 4.0e+00, NA, 3.1e+01, 1.2e+0…\n$ `1980 [YR1980]` &lt;dbl&gt; 7.8e+01, 6.4e+00, NA, NA, 3.9e+00, NA, 3.3e+01, 1.2e+0…\n$ `1981 [YR1981]` &lt;dbl&gt; 7.6e+01, 6.5e+00, NA, NA, 3.6e+00, NA, 4.8e+01, 1.2e+0…\n$ `1982 [YR1982]` &lt;dbl&gt; 7.4e+01, 9.6e+00, NA, NA, 3.6e+00, NA, 4.6e+01, 1.2e+0…\n$ `1983 [YR1983]` &lt;dbl&gt; 7.4e+01, 8.7e+00, NA, NA, 3.6e+00, NA, 4.6e+01, 1.3e+0…\n$ `1984 [YR1984]` &lt;dbl&gt; 7.4e+01, 8.3e+00, NA, NA, 3.6e+00, NA, 4.2e+01, 1.3e+0…\n$ `1985 [YR1985]` &lt;dbl&gt; 7.4e+01, 7.6e+00, NA, NA, 3.3e+00, NA, 3.3e+01, 1.3e+0…\n$ `1986 [YR1986]` &lt;dbl&gt; 7.4e+01, 7.8e+00, NA, NA, 3.4e+00, NA, 3.3e+01, 1.3e+0…\n$ `1987 [YR1987]` &lt;dbl&gt; 7.3e+01, 8.1e+00, NA, NA, 3.7e+00, NA, 4.8e+01, 1.4e+0…\n$ `1988 [YR1988]` &lt;dbl&gt; 7.3e+01, 9.0e+00, NA, NA, 3.8e+00, NA, 4.3e+01, 1.4e+0…\n$ `1989 [YR1989]` &lt;dbl&gt; 7.3e+01, 9.6e+00, NA, NA, 3.6e+00, NA, 8.0e+01, 1.3e+0…\n$ `1990 [YR1990]` &lt;dbl&gt; 7.3e+01, 8.1e+00, NA, 9.7e+01, 3.4e+00, NA, 3.2e+01, 1…\n$ `1991 [YR1991]` &lt;dbl&gt; 7.3e+01, 6.7e+00, NA, NA, 3.5e+00, NA, 2.3e+01, 1.3e+0…\n$ `1992 [YR1992]` &lt;dbl&gt; 7.3e+01, 6.0e+00, NA, 9.6e+01, 3.6e+00, NA, 2.2e+01, 1…\n$ `1993 [YR1993]` &lt;dbl&gt; 7.3e+01, 5.1e+00, NA, NA, 3.5e+00, NA, 2.6e+01, 1.5e+0…\n$ `1994 [YR1994]` &lt;dbl&gt; 7.2e+01, 5.1e+00, NA, NA, 3.5e+00, NA, 2.7e+01, 1.6e+0…\n$ `1995 [YR1995]` &lt;dbl&gt; 7.1e+01, 5.4e+00, NA, 9.8e+01, 3.7e+00, NA, 2.8e+01, 1…\n$ `1996 [YR1996]` &lt;dbl&gt; 7.0e+01, 5.6e+00, NA, NA, 3.8e+00, NA, 2.8e+01, 1.7e+0…\n$ `1997 [YR1997]` &lt;dbl&gt; 7.0e+01, 5.2e+00, 9.8e+00, 9.7e+01, 3.9e+00, NA, 3.0e+…\n$ `1998 [YR1998]` &lt;dbl&gt; 6.9e+01, 5.3e+00, NA, 9.8e+01, 3.9e+00, NA, 3.3e+01, 2…\n$ `1999 [YR1999]` &lt;dbl&gt; 6.8e+01, 4.5e+00, NA, 9.8e+01, 4.0e+00, NA, 3.6e+01, 2…\n$ `2000 [YR2000]` &lt;dbl&gt; 6.7e+01, 4.7e+00, NA, 9.9e+01, 3.8e+00, NA, 3.4e+01, 2…\n$ `2001 [YR2001]` &lt;dbl&gt; 6.6e+01, 4.6e+00, NA, 9.8e+01, 3.6e+00, 6.5e+01, 3.7e+…\n$ `2002 [YR2002]` &lt;dbl&gt; 6.5e+01, 1.0e+01, NA, 9.9e+01, 3.3e+00, NA, 6.2e+01, 2…\n$ `2003 [YR2003]` &lt;dbl&gt; 6.5e+01, 1.0e+01, NA, 9.9e+01, 3.5e+00, NA, 5.1e+01, 2…\n$ `2004 [YR2004]` &lt;dbl&gt; 6.4e+01, 8.4e+00, NA, 9.9e+01, 4.1e+00, NA, 4.2e+01, 2…\n$ `2005 [YR2005]` &lt;dbl&gt; 6.4e+01, 7.9e+00, NA, 9.9e+01, 4.1e+00, 7.9e+01, 3.5e+…\n$ `2006 [YR2006]` &lt;dbl&gt; 6.4e+01, 6.9e+00, NA, 9.9e+01, 4.4e+00, NA, 2.8e+01, 2…\n$ `2007 [YR2007]` &lt;dbl&gt; 6.4e+01, 7.5e+00, NA, 9.9e+01, 4.4e+00, NA, 2.6e+01, 2…\n$ `2008 [YR2008]` &lt;dbl&gt; 6.4e+01, 7.3e+00, NA, 9.5e+01, 4.7e+00, NA, 2.2e+01, 2…\n$ `2009 [YR2009]` &lt;dbl&gt; 6.4e+01, 5.3e+00, NA, 9.8e+01, 4.4e+00, NA, 2.6e+01, 2…\n$ `2010 [YR2010]` &lt;dbl&gt; 6.4e+01, 7.1e+00, NA, 9.5e+01, 4.6e+00, NA, 2.5e+01, 2…\n$ `2011 [YR2011]` &lt;dbl&gt; 6.4e+01, 7.0e+00, NA, 9.7e+01, 4.6e+00, NA, 2.6e+01, 2…\n$ `2012 [YR2012]` &lt;dbl&gt; 6.4e+01, 5.8e+00, 1.3e+01, 9.8e+01, 4.6e+00, 5.5e+01, …\n$ `2013 [YR2013]` &lt;dbl&gt; 6.4e+01, 6.1e+00, NA, 9.7e+01, 4.5e+00, 8.1e+01, 3.3e+…\n$ `2014 [YR2014]` &lt;dbl&gt; 6.4e+01, 6.7e+00, 1.3e+01, 1.0e+02, 4.7e+00, NA, 3.4e+…\n$ `2015 [YR2015]` &lt;dbl&gt; 6.3e+01, 5.2e+00, NA, 1.0e+02, NA, NA, 4.0e+01, NA, NA…\n$ `2016 [YR2016]` &lt;dbl&gt; 6.3e+01, 6.4e+00, NA, NA, NA, NA, 3.8e+01, NA, NA, 1.3…\n$ `2017 [YR2017]` &lt;dbl&gt; NA, 5.6e+00, NA, NA, NA, NA, 3.9e+01, NA, NA, 1.1e+01,…\n$ `2018 [YR2018]` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nWorld Development Indicators (WDI), sourced from the World Bank Group (2019)\n\n\n\nWhat are the data types?\nWhat are the variables?\nWhat are the observations?\nIs the data in tidy form?"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-27",
    "href": "week3/slides.html#case-study-world-development-indicators-27",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (2/7)",
    "text": "Case study: World development indicators (2/7)\n\n\n\ncountry_code_df &lt;- raw_dat  |&gt;\n  distinct(`Country Name`, `Country Code`)  |&gt;\n  rename_all(janitor::make_clean_names)  |&gt;\n  left_join(\n    countrycode::codelist |&gt; select(iso3c, region, continent),\n    by = c(\"country_code\" = \"iso3c\")\n  )  |&gt;\n  arrange(continent, region) \n\n\n\n\nRows: 217\nColumns: 4\n$ country_name &lt;chr&gt; \"Algeria\", \"Djibouti\", \"Egypt, Arab Rep.\", \"Libya\", \"Moro…\n$ country_code &lt;chr&gt; \"DZA\", \"DJI\", \"EGY\", \"LBY\", \"MAR\", \"TUN\", \"AGO\", \"BEN\", \"…\n$ region       &lt;chr&gt; \"Middle East & North Africa\", \"Middle East & North Africa…\n$ continent    &lt;chr&gt; \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa…\n\n\n# A tibble: 6 × 2\n  continent     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Africa       54\n2 Americas     46\n3 Asia         50\n4 Europe       46\n5 Oceania      19\n6 &lt;NA&gt;          2\n\n\n# A tibble: 8 × 2\n  region                         n\n  &lt;chr&gt;                      &lt;int&gt;\n1 East Asia & Pacific           37\n2 Europe & Central Asia         56\n3 Latin America & Caribbean     42\n4 Middle East & North Africa    21\n5 North America                  3\n6 South Asia                     8\n7 Sub-Saharan Africa            48\n8 &lt;NA&gt;                           2\n\n\n\n\n\n\nHow many countries are included\nHow many continents, regions?\nWhy are there NAs here?\n\n\n\ncountry_code_df |&gt; filter(is.na(continent))\n\n# A tibble: 2 × 4\n  country_name    country_code region continent\n  &lt;chr&gt;           &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;    \n1 Channel Islands CHI          &lt;NA&gt;   &lt;NA&gt;     \n2 Kosovo          XKX          &lt;NA&gt;   &lt;NA&gt;"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-37",
    "href": "week3/slides.html#case-study-world-development-indicators-37",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (3/7)",
    "text": "Case study: World development indicators (3/7)\n\n\n\nwdi_vars &lt;- raw_dat  |&gt;\n  select(`Series Name`, `Series Code`) |&gt;\n  distinct() |&gt;\n  rename_all(janitor::make_clean_names) \n\n\n\n\n\n\n\n\n\n\n\nseries_name\nseries_code\n\n\n\n\nAdolescent fertility rate (births per 1,000 women ages 15-19)\nSP.ADO.TFRT\n\n\nAgriculture, forestry, and fishing, value added (% of GDP)\nNV.AGR.TOTL.ZS\n\n\nAnnual freshwater withdrawals, total (% of internal resources)\nER.H2O.FWTL.ZS\n\n\nBirths attended by skilled health staff (% of total)\nSH.STA.BRTC.ZS\n\n\nCO2 emissions (metric tons per capita)\nEN.ATM.CO2E.PC\n\n\nContraceptive prevalence, any methods (% of women ages 15-49)\nSP.DYN.CONU.ZS\n\n\nDomestic credit provided by financial sector (% of GDP)\nFS.AST.DOMS.GD.ZS\n\n\nElectric power consumption (kWh per capita)\nEG.USE.ELEC.KH.PC\n\n\nEnergy use (kg of oil equivalent per capita)\nEG.USE.PCAP.KG.OE\n\n\nExports of goods and services (% of GDP)\nNE.EXP.GNFS.ZS\n\n\nExternal debt stocks, total (DOD, current US$)\nDT.DOD.DECT.CD\n\n\nFertility rate, total (births per woman)\nSP.DYN.TFRT.IN\n\n\nForeign direct investment, net inflows (BoP, current US$)\nBX.KLT.DINV.CD.WD\n\n\nForest area (sq. km)\nAG.LND.FRST.K2\n\n\nGDP (current US$)\nNY.GDP.MKTP.CD\n\n\nGDP growth (annual %)\nNY.GDP.MKTP.KD.ZG\n\n\nGNI per capita, Atlas method (current US$)\nNY.GNP.PCAP.CD\n\n\nGNI per capita, PPP (current international $)\nNY.GNP.PCAP.PP.CD\n\n\nGNI, Atlas method (current US$)\nNY.GNP.ATLS.CD\n\n\nGNI, PPP (current international $)\nNY.GNP.MKTP.PP.CD\n\n\nGross capital formation (% of GDP)\nNE.GDI.TOTL.ZS\n\n\nHigh-technology exports (% of manufactured exports)\nTX.VAL.TECH.MF.ZS\n\n\nImmunization, measles (% of children ages 12-23 months)\nSH.IMM.MEAS\n\n\nImports of goods and services (% of GDP)\nNE.IMP.GNFS.ZS\n\n\nIncome share held by lowest 20%\nSI.DST.FRST.20\n\n\nIndustry (including construction), value added (% of GDP)\nNV.IND.TOTL.ZS\n\n\nInflation, GDP deflator (annual %)\nNY.GDP.DEFL.KD.ZG\n\n\nLife expectancy at birth, total (years)\nSP.DYN.LE00.IN\n\n\nMerchandise trade (% of GDP)\nTG.VAL.TOTL.GD.ZS\n\n\nMilitary expenditure (% of GDP)\nMS.MIL.XPND.GD.ZS\n\n\nMobile cellular subscriptions (per 100 people)\nIT.CEL.SETS.P2\n\n\nMortality rate, under-5 (per 1,000 live births)\nSH.DYN.MORT\n\n\nNet barter terms of trade index (2000 = 100)\nTT.PRI.MRCH.XD.WD\n\n\nNet migration\nSM.POP.NETM\n\n\nNet official development assistance and official aid received (current US$)\nDT.ODA.ALLD.CD\n\n\nPersonal remittances, received (current US$)\nBX.TRF.PWKR.CD.DT\n\n\nPopulation density (people per sq. km of land area)\nEN.POP.DNST\n\n\nPopulation growth (annual %)\nSP.POP.GROW\n\n\nPopulation, total\nSP.POP.TOTL\n\n\nPoverty headcount ratio at $1.90 a day (2011 PPP) (% of population)\nSI.POV.DDAY\n\n\nPoverty headcount ratio at national poverty lines (% of population)\nSI.POV.NAHC\n\n\nPrevalence of HIV, total (% of population ages 15-49)\nSH.DYN.AIDS.ZS\n\n\nPrevalence of underweight, weight for age (% of children under 5)\nSH.STA.MALN.ZS\n\n\nPrimary completion rate, total (% of relevant age group)\nSE.PRM.CMPT.ZS\n\n\nRevenue, excluding grants (% of GDP)\nGC.REV.XGRT.GD.ZS\n\n\nSchool enrollment, primary (% gross)\nSE.PRM.ENRR\n\n\nSchool enrollment, primary and secondary (gross), gender parity index (GPI)\nSE.ENR.PRSC.FM.ZS\n\n\nSchool enrollment, secondary (% gross)\nSE.SEC.ENRR\n\n\nStatistical Capacity score (Overall average)\nIQ.SCI.OVRL\n\n\nSurface area (sq. km)\nAG.SRF.TOTL.K2\n\n\nTax revenue (% of GDP)\nGC.TAX.TOTL.GD.ZS\n\n\nTerrestrial and marine protected areas (% of total territorial area)\nER.PTD.TOTL.ZS\n\n\nTime required to start a business (days)\nIC.REG.DURS\n\n\nTotal debt service (% of exports of goods, services and primary income)\nDT.TDS.DECT.EX.ZS\n\n\nUrban population growth (annual %)\nSP.URB.GROW\n\n\n\n\n\n\n\n\n\n\nAnalysis will use the short name (series_code) for variables.\nStore full variable name (series_name) and short name (series_code) in a separate table.\nThe series_code will be used as the key whenever the full name is needed."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-47",
    "href": "week3/slides.html#case-study-world-development-indicators-47",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (4/7)",
    "text": "Case study: World development indicators (4/7)\n\n\n\nwdi &lt;- raw_dat  |&gt;\n  select(`Country Code`, `Series Code`, `1969 [YR1969]`:`2018 [YR2018]`) |&gt;\n  rename_all(janitor::make_clean_names) |&gt;\n  pivot_longer(x1969_yr1969:x2018_yr2018,\n               names_to = \"year\", \n               values_to = \"value\") |&gt;\n  mutate(year = as.numeric(str_sub(year, 2, 5)) ) |&gt;\n  pivot_wider(names_from = series_code,\n              values_from = value)\n\nwdi2017 &lt;- wdi  |&gt; filter(year == 2017)\n\n\nOrganise data into tidy form\nCheck missing value distribution"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-57",
    "href": "week3/slides.html#case-study-world-development-indicators-57",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (5/7)",
    "text": "Case study: World development indicators (5/7)\n\n\nCheck missings by\n\nvariable\ncountry"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-67",
    "href": "week3/slides.html#case-study-world-development-indicators-67",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (6/7)",
    "text": "Case study: World development indicators (6/7)\n\n\nLook at Costa Rica (CRI), most complete country\n\n\n\n\n\n\n\n\n\n\nTo illustrate imputation, we’ll show one of the variables, that is relatively complete.\n\n\n\n\n\n\n\n\n\nImpute a few temporal missings using nearest neighbours."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-67-1",
    "href": "week3/slides.html#case-study-world-development-indicators-67-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (6/7)",
    "text": "Case study: World development indicators (6/7)\n\n\nMissings imputed using imputeTS using the moving average method.\n\n\n\n\n\n\n\n\n\n\n\nDon’t have to impute before scrutinizing data\nWhat are these numbers supposed to be?\n\nSE.PRM.CMPT.ZS is “Primary completion rate, total (% of relevant age group)”\nDo we have any problems?\n\nYes. The explanation of the variable suggests the numbers should range between 0-100."
  },
  {
    "objectID": "week3/slides.html#summary-of-the-process",
    "href": "week3/slides.html#summary-of-the-process",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "📋 Summary of the process",
    "text": "📋 Summary of the process\nThe steps we took roughly followed these:\n\n\n\n\n\n\n\n\nAt the end of this stage we would have:\n\n3 tables of data: country name/code, variables name/key, time series of multiple variables for many countries\nWhat would you like to learn from this data? What sort of models might be fitted? What types of hypotheses might be tested?\nHave we done anything that might have compromised the later analysis?"
  },
  {
    "objectID": "week3/slides.html#data-collection",
    "href": "week3/slides.html#data-collection",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data collection",
    "text": "Data collection"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-13",
    "href": "week3/slides.html#case-study-employment-data-in-australia-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (1/3)",
    "text": "Case study: Employment Data in Australia (1/3)\nBelow is the data from ABS that shows the total number of people employed in a given month from February 1976 to December 2019 using the original time series.\n\n\nload(here(\"data/employed.rda\"))\nglimpse(employed)\n\nRows: 557\nColumns: 4\n$ date  &lt;date&gt; 1978-02-01, 1978-03-01, 1978-04-01, 1978-05-01, 1978-06-01, 197…\n$ month &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1…\n$ year  &lt;dbl&gt; 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978…\n$ value &lt;dbl&gt; 5986, 6041, 6054, 6038, 6031, 6036, 6005, 6024, 6046, 6034, 6125…\n\n\nAustralian Bureau of Statistics, Labour force, Australia, Table 01. Labour force status by Sex, Australia - Trend, Seasonally adjusted and Original"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-23",
    "href": "week3/slides.html#case-study-employment-data-in-australia-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (2/3)",
    "text": "Case study: Employment Data in Australia (2/3)\n\n\nDo you notice anything?\n\n\n\n\n\n\n\n\n\n\n\nWhy do you think the number of people employed is going up each year?\n\n\n\nAustralian population is 25.39 million in 2019\n1.5% annual increase in population\nVic population is 6.681 million (Sep 2020) - 26%\nNSW population is 8.166 (Sep 2020) - 32%"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-33",
    "href": "week3/slides.html#case-study-employment-data-in-australia-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (3/3)",
    "text": "Case study: Employment Data in Australia (3/3)\n\n\n\nThere’s a suspicious change in August numbers from 2014.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA potential explanation for this is that there was a change in the survey from 2014.\n\nSee discussion on this at Hyndsight blog (10 October 2014)."
  },
  {
    "objectID": "week3/slides.html#case-study-2014-data-mining-cup-winners",
    "href": "week3/slides.html#case-study-2014-data-mining-cup-winners",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: 2014 Data Mining Cup winners",
    "text": "Case study: 2014 Data Mining Cup winners\n\n\n\nUgly plot of all observations provided in training sample, with response variable in colour, and test sample to predict.\nWhat does this tell you about the test sample?"
  },
  {
    "objectID": "week3/slides.html#case-study-french-frieshot-chips-12",
    "href": "week3/slides.html#case-study-french-frieshot-chips-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: french fries/hot chips (1/2)",
    "text": "Case study: french fries/hot chips (1/2)\n\n\n\n\nRows: 696\nColumns: 9\n$ time      &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ treatment &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ subject   &lt;fct&gt; 3, 3, 10, 10, 15, 15, 16, 16, …\n$ rep       &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, …\n$ potato    &lt;dbl&gt; 2.9, 14.0, 11.0, 9.9, 1.2, 8.8…\n$ buttery   &lt;dbl&gt; 0.0, 0.0, 6.4, 5.9, 0.1, 3.0, …\n$ grassy    &lt;dbl&gt; 0.0, 0.0, 0.0, 2.9, 0.0, 3.6, …\n$ rancid    &lt;dbl&gt; 0.0, 1.1, 0.0, 2.2, 1.1, 1.5, …\n$ painty    &lt;dbl&gt; 5.5, 0.0, 0.0, 0.0, 5.1, 2.3, …\n\n\n\n10 week sensory experiment, 12 individuals assessed taste of french fries on several scales (how potato-y, buttery, grassy, rancid, paint-y do they taste?), fried in one of 3 different oils, replicated twice.\n\nIs the design complete?\nAre replicates like each other?\nHow do the ratings on the different scales differ?\nAre raters giving different scores on average?\nDo ratings change over the weeks?"
  },
  {
    "objectID": "week3/slides.html#case-study-french-frieshot-chips-22",
    "href": "week3/slides.html#case-study-french-frieshot-chips-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: french fries/hot chips (2/2)",
    "text": "Case study: french fries/hot chips (2/2)\n\n\n\nIs the design complete?\n\n\nfrench_fries |&gt; count(subject)\n\n# A tibble: 12 × 2\n   subject     n\n   &lt;fct&gt;   &lt;int&gt;\n 1 3          54\n 2 10         60\n 3 15         60\n 4 16         60\n 5 19         60\n 6 31         54\n 7 51         60\n 8 52         60\n 9 63         60\n10 78         60\n11 79         54\n12 86         54\n\n\n\n\n\nfrench_fries |&gt; count(time)\n\n# A tibble: 10 × 2\n   time      n\n   &lt;fct&gt; &lt;int&gt;\n 1 1        72\n 2 2        72\n 3 3        72\n 4 4        72\n 5 5        72\n 6 6        72\n 7 7        72\n 8 8        72\n 9 9        60\n10 10       60\n\nfrench_fries |&gt; count(treatment)\n\n# A tibble: 3 × 2\n  treatment     n\n  &lt;fct&gt;     &lt;int&gt;\n1 1           232\n2 2           232\n3 3           232\n\nfrench_fries |&gt; count(rep)\n\n# A tibble: 2 × 2\n    rep     n\n  &lt;dbl&gt; &lt;int&gt;\n1     1   348\n2     2   348"
  },
  {
    "objectID": "week3/slides.html#case-study-warranty-claims",
    "href": "week3/slides.html#case-study-warranty-claims",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Warranty claims",
    "text": "Case study: Warranty claims\n\n\n\n\nRows: 4,561\nColumns: 14\n$ Region           &lt;chr&gt; \"East\", \"West\", \"North …\n$ State            &lt;chr&gt; \"Delhi\", \"Gujarat\", \"We…\n$ Area             &lt;chr&gt; \"Urban\", \"Rural\", \"Urba…\n$ City             &lt;chr&gt; \"New Delhi\", \"Ahmedabad…\n$ Consumer_profile &lt;chr&gt; \"Personal\", \"Personal\",…\n$ TV_2001_Issue    &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 1, 1,…\n$ TV_2002_Issue    &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1,…\n$ TV_2003_Issue    &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 0,…\n$ Claim_Value      &lt;dbl&gt; 25000, 4216, 4000, 5000…\n$ Service_Centre   &lt;dbl&gt; 13, 10, 10, 12, 10, 10,…\n$ Product_Age      &lt;dbl&gt; 60, 672, 275, 10, 4, 34…\n$ Purchased_from   &lt;chr&gt; \"Dealer\", \"Dealer\", \"De…\n$ Call_details     &lt;dbl&gt; 1.3, 25.0, 11.0, 1.6, 0…\n$ Purpose          &lt;chr&gt; \"Complaint\", \"Other\", \"…\n\n\n\nTV_2001_Issue: failure of power supply\nTV_2002_Issue: failure of inverter\nTV_2003_Issue: failure of motherboard\n\n\n\n\nWhat is the population that this data is measuring?\nWhat is not measured?\n\n\n\n\n# A tibble: 2 × 2\n  City          n\n  &lt;chr&gt;     &lt;int&gt;\n1 Delhi       106\n2 Bangalore   320\n\n\nCan we say that Delhi has fewer problems with TVs than Bangalore?\n\n\nSource: ExcelR Projects. (2019). Warranty Claims. Kaggle."
  },
  {
    "objectID": "week3/slides.html#summary-of-checks-for-data-collection",
    "href": "week3/slides.html#summary-of-checks-for-data-collection",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "📋 Summary of checks for data collection",
    "text": "📋 Summary of checks for data collection\n✅ Has the collection process been consistent?\n✅ Does the set to be predicted match the training set?\n✅ Is the experimental design correctly applied?\n✅ Have treatments been appropriately randomised or assigned comprehensively across subjects?\n✅ What is the population that the collected data describes?\n✅ If the data is observational, can you group them into comparison sets?"
  },
  {
    "objectID": "week3/slides.html#imputing-missing-values",
    "href": "week3/slides.html#imputing-missing-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Imputing missing values",
    "text": "Imputing missing values"
  },
  {
    "objectID": "week3/slides.html#example-1-olympic-medals",
    "href": "week3/slides.html#example-1-olympic-medals",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 1: Olympic medals",
    "text": "Example 1: Olympic medals\n\n\n\n\n             country totalmedal\n1       UnitedStates        104\n2              China         88\n3             Russia         82\n4       GreatBritain         65\n5            Germany         44\n6              Japan         38\n7          Australia         35\n8             France         34\n9         SouthKorea         28\n10             Italy         28\n11       Netherlands         20\n12           Ukraine         20\n13            Canada         18\n14           Hungary         17\n15             Spain         17\n16            Brazil         17\n17              Cuba         14\n18        Kazakhstan         13\n19        NewZealand         13\n20              Iran         12\n21           Jamaica         12\n22           Belarus         12\n23             Kenya         11\n24     CzechRepublic         10\n25            Poland         10\n26        Azerbaijan         10\n27           Romania          9\n28           Denmark          9\n29            Sweden          8\n30          Colombia          8\n31          Ethiopia          7\n32            Mexico          7\n33           Georgia          7\n34        NorthKorea          6\n35       SouthAfrica          6\n36           Croatia          6\n37             India          6\n38            Turkey          5\n39         Lithuania          5\n40           Ireland          5\n41          Mongolia          5\n42       Switzerland          4\n43            Norway          4\n44          Slovenia          4\n45            Serbia          4\n46         Argentina          4\n47        Uzbekistan          4\n48 TrinidadandTobago          4\n49          Slovakia          4\n50           Tunisia          3\n51          Thailand          3\n52           Finland          3\n53           Belgium          3\n54           Armenia          3\n55 DominicanRepublic          2\n56            Latvia          2\n57             Egypt          2\n58        PuertoRico          2\n59          Malaysia          2\n60         Indonesia          2\n61           Estonia          2\n62            Taiwan          2\n63          Bulgaria          2\n64         Singapore          2\n65             Qatar          2\n66           Moldova          2\n67            Greece          2\n68         Venezuela          1\n69            Uganda          1\n70           Grenada          1\n71           Bahamas          1\n72           Algeria          1\n73          Portugal          1\n74        Montenegro          1\n75         Guatemala          1\n76             Gabon          1\n77            Cyprus          1\n78          Botswana          1\n79        Tajikistan          1\n80       SaudiArabia          1\n81           Morocco          1\n82            Kuwait          1\n83          HongKong          1\n84           Bahrain          1\n85       Afghanistan          1\n\n\n Is the average number of medals equal to 962/85 = 11.32?\n\n\nWhat is missing?\n\n\nWhat is the correct average number of medals?\n\n\n962/204 = 4.72\n Working out what is missing can be hard!"
  },
  {
    "objectID": "week3/slides.html#example-2-el-nino",
    "href": "week3/slides.html#example-2-el-nino",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 2: El Nino",
    "text": "Example 2: El Nino\n\n\nExplore missings\n\nplotting on edge of plots, or\nusing simple imputation like mean\n\n\noceanbuoys |&gt;\n  ggplot(aes(x=air_temp_c, y=humidity)) +\n  geom_miss_point()\n\n\n\n\n\n\n\n\n\nImpute and check\n\nImpute using regression or simulation\nCheck distribution relative to complete cases\n\n\n\n\nCode\nlibrary(simputation)\nocean_imp_yr &lt;- oceanbuoys %&gt;%\n  bind_shadow() %&gt;%\n  impute_lm(air_temp_c ~ wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  impute_lm(humidity ~  wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  impute_lm(sea_temp_c ~  wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  add_label_shadow()\n\nggplot(ocean_imp_yr,\n       aes(x = air_temp_c,\n           y = humidity,\n           color = any_missing)) + \n  geom_point() +\n  theme(legend.title = element_blank())"
  },
  {
    "objectID": "week3/slides.html#validators",
    "href": "week3/slides.html#validators",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Validators",
    "text": "Validators\nAutomating some checks"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-13",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (1/3)",
    "text": "Case study: Dutch supermarket revenue and cost (1/3)\n\nData contains the revenue and cost (in Euros) for 60 supermarkets\nData has been anonymised and distorted\n\n\nlibrary(validate)\ndata(\"SBS2000\", package = \"validate\")\ndplyr::glimpse(SBS2000)\n\nRows: 60\nColumns: 11\n$ id          &lt;fct&gt; RET01, RET02, RET03, RET04, …\n$ size        &lt;fct&gt; sc0, sc3, sc3, sc3, sc3, sc0…\n$ incl.prob   &lt;dbl&gt; 0.02, 0.14, 0.14, 0.14, 0.14…\n$ staff       &lt;int&gt; 75, 9, NA, NA, NA, 1, 5, 3, …\n$ turnover    &lt;int&gt; NA, 1607, 6886, 3861, NA, 25…\n$ other.rev   &lt;int&gt; NA, NA, -33, 13, 37, NA, NA,…\n$ total.rev   &lt;int&gt; 1130, 1607, 6919, 3874, 5602…\n$ staff.costs &lt;int&gt; NA, 131, 324, 290, 314, NA, …\n$ total.costs &lt;int&gt; 18915, 1544, 6493, 3600, 553…\n$ profit      &lt;int&gt; 20045, 63, 426, 274, 72, 3, …\n$ vat         &lt;int&gt; NA, NA, NA, NA, NA, NA, 1346…"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-23",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (2/3)",
    "text": "Case study: Dutch supermarket revenue and cost (2/3)\n\nChecking for completeness of records\n\n\nrules &lt;- validator(\n          is_complete(id),\n          is_complete(id, turnover),\n          is_complete(id, turnover, profit))\nout &lt;- confront(SBS2000, rules)\nsummary(out)\n\n  name items passes fails nNA error warning\n1   V1    60     60     0   0 FALSE   FALSE\n2   V2    60     56     4   0 FALSE   FALSE\n3   V3    60     52     8   0 FALSE   FALSE\n                         expression\n1                   is_complete(id)\n2         is_complete(id, turnover)\n3 is_complete(id, turnover, profit)"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-33",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (3/3)",
    "text": "Case study: Dutch supermarket revenue and cost (3/3)\n\nSanity check derived variables\n\n\nrules &lt;- validator(\n    total.rev - profit == total.costs,\n    turnover + other.rev == total.rev,\n    profit &lt;= 0.6 * total.rev\n)\nout &lt;- confront(SBS2000, rules)\nsummary(out)\n\n  name items passes fails nNA error warning\n1   V1    60     39    14   7 FALSE   FALSE\n2   V2    60     19     4  37 FALSE   FALSE\n3   V3    60     49     6   5 FALSE   FALSE\n                                      expression\n1 abs(total.rev - profit - total.costs) &lt;= 1e-08\n2 abs(turnover + other.rev - total.rev) &lt;= 1e-08\n3              profit - 0.6 * total.rev &lt;= 1e-08"
  },
  {
    "objectID": "week3/slides.html#ida-for-hypothesis-testing",
    "href": "week3/slides.html#ida-for-hypothesis-testing",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA for hypothesis testing",
    "text": "IDA for hypothesis testing"
  },
  {
    "objectID": "week3/slides.html#hypothesis-testing-13",
    "href": "week3/slides.html#hypothesis-testing-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hypothesis testing (1/3)",
    "text": "Hypothesis testing (1/3)\n\nState the hypothesis (pair), e.g. \\(H_o: \\mu_1 = \\mu_2\\) vs \\(H_a: \\mu_1 &lt; \\mu_2\\).\nTest statistic depends on assumption about the distribution, e.g. \n\n\\(t\\)-test will assume that distributions are normal, or small departures from if we have a large sample.\ntwo-sample might assume both groups have the same variance\n\n\n\n\nSteps to complete:\n\nCompute the test statistic\nMeasure it against a standard distribution\nIf it is extreme, \\(p\\)-value is small, decision is to reject \\(H_o\\)\n\\(p\\)-value is the probability of observing a value as large as this, or large, assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week3/slides.html#example-1-checking-variance-and-distribution-23",
    "href": "week3/slides.html#example-1-checking-variance-and-distribution-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 1: Checking variance and distribution (2/3)",
    "text": "Example 1: Checking variance and distribution (2/3)\n\n\n\n\n\nCode\ndata(sleep)\nggplot(sleep, aes(x=group, y=extra)) + \n  geom_boxplot() +\n  geom_point(colour=\"#D55E00\")\n\n\n\n\n\n\n\n\n\n\nCushny, A. R. and Peebles, A. R. (1905) The action of optical isomers: II hyoscines. The Journal of Physiology 32, 501–510.\n\n\nFew observations. Nothing strongly suggests violation of normality and spread of points is similar for each group.\n\ntt &lt;- with(sleep,\n     t.test(extra[group == 1],\n            extra[group == 2], \n            paired = TRUE))\ntt\n\n\n    Paired t-test\n\ndata:  extra[group == 1] and extra[group == 2]\nt = -4, df = 9, p-value = 0.003\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.5 -0.7\nsample estimates:\nmean difference \n           -1.6"
  },
  {
    "objectID": "week3/slides.html#example-2-checking-distribution-and-variance-33",
    "href": "week3/slides.html#example-2-checking-distribution-and-variance-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 2: Checking distribution and variance (3/3)",
    "text": "Example 2: Checking distribution and variance (3/3)\n\n\n\n\n\nCode\nInsectSprays  |&gt; \n  ggplot(aes(x=fct_reorder(spray, count), \n             y=count)) + \n  geom_jitter(width=0.1, height=0, colour=\"#D55E00\", size=3, alpha=0.8) +\n  xlab(\"\") \n\n\n\n\n\n\n\n\n\n\nIs it plausible that the samples are from a normal population? Do they have equal variance?\n\n\n\nfm1 &lt;- aov(count ~ spray, data = InsectSprays)\nsummary(fm1)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nspray        5   2669     534    34.7 &lt;2e-16 ***\nResiduals   66   1015      15                   \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhat hypothesis being tested? What would the decision be?\n\n\nWhy does equal variance matter in this test?"
  },
  {
    "objectID": "week3/slides.html#ida-for-inferential-modeling",
    "href": "week3/slides.html#ida-for-inferential-modeling",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA for inferential modeling",
    "text": "IDA for inferential modeling"
  },
  {
    "objectID": "week3/slides.html#linear-models-13",
    "href": "week3/slides.html#linear-models-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (1/3)",
    "text": "Linear models (1/3)\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nggplot(cars, aes(speed, dist)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + e_i\\]\nAssumptions:\n\nForm is linear\nError is normally distributed around 0\n\n\n\nCheck using residual plots\n\n\n\nCode\ncars_model &lt;- lm(dist ~ speed, data = cars)\ncars_fit &lt;- augment(cars_model)\n\ncars_p1 &lt;- ggplot(cars_fit, aes(x=.fitted, \n                                y=.resid)) + \n  geom_hline(yintercept = 0, colour=\"grey70\") +\n  geom_point() \ncars_p2 &lt;- ggplot(cars_fit, aes(x=.resid)) +\n  geom_density()\ncars_p1 + cars_p2 + plot_layout(ncol=2)"
  },
  {
    "objectID": "week3/slides.html#linear-models-23",
    "href": "week3/slides.html#linear-models-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (2/3)",
    "text": "Linear models (2/3)\n\n\n\nData and loess smoother\n\n\nCode\nggplot(diamonds, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(se=F)\n\n\n\n\n\n\n\n\n\n\nForm is not linear!\nAlso, insufficient data on large diamonds.\n\n\n\nFix 1: fit polynomial form\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + e_i.\\]\n\n\nCode\ndiamonds_sub &lt;- diamonds |&gt;\n  filter(carat &lt; 3)\n\ndiamonds_model &lt;- lm(price ~ poly(carat, 2),\n                     data=diamonds_sub)\ndiamonds_fit &lt;- diamonds_sub |&gt;\n  mutate(.fitted = diamonds_model$fitted.values, \n         .resid = diamonds_model$residuals)\n\ndiamonds_p1 &lt;- ggplot(diamonds_fit) +\n  geom_point(aes(x=carat, y=price)) +\n  geom_point(aes(x=carat, y=.fitted),\n             colour=\"#D55E00\")\ndiamonds_p2 &lt;- ggplot(diamonds_fit, \n                      aes(x=.fitted, \n                          y=.resid)) + \n  geom_hline(yintercept = 0, colour=\"grey70\") +\n  geom_point() \ndiamonds_p1 + diamonds_p2 + plot_layout(ncol=2)\n\n\n\n\n\n\n\n\n\n\nForm is not quadratic, continue to explore additional polynomial terms."
  },
  {
    "objectID": "week3/slides.html#linear-models-33",
    "href": "week3/slides.html#linear-models-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (3/3)",
    "text": "Linear models (3/3)\n\n\n\nData and loess smoother\n\n\nCode\nggplot(diamonds, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(se=F)\n\n\n\n\n\n\n\n\n\n\nForm is not linear!\nAlso, insufficient data on large diamonds.\n\n\n\nFix 2: linearise\n\nThe log transformation of both variables linearises the relationship, so that a simple linear model can be used, and can correct heteroskedasticity.\n\n\nCode\nggplot(diamonds_sub, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(method = lm) +\n  scale_x_log10() +\n  scale_y_log10()"
  },
  {
    "objectID": "week3/slides.html#cautions",
    "href": "week3/slides.html#cautions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Cautions",
    "text": "Cautions\n\nNotice that there was no formal statistical inference when trying to determine an appropriate model form.\n\n\nDiscarded models are hardly ever reported. Consequently, majority of reported statistics give a distorted view and it’s important to remind yourself what might not be reported."
  },
  {
    "objectID": "week3/slides.html#summary",
    "href": "week3/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\n\n\nIDA is a model-focused exploration to support a CDA with:\n\ndata description and collection\ndata quality checking, and\nchecking assumptions\nmodel fit without any formal statistical inference.\n\nIDA is part of EDA, even when no CDA is planned.\n\nIDA may never see the limelight BUT it forms the foundation that the main analysis is built upon. Document it! Do it well!\n\n\n\n\nThe Census Bureau tabulates same-sex couples in both the American Community Survey (ACS) and the Decennial Census. Two questions are used to identify same-sex couples: relationship and sex. The agency follows edit rules that are used to change data values for seemingly contradictory answers. The edit rules for combining information from relationship and sex have evolved since the category of unmarried partner was added in 1990. In that census, if a household consisted of a married couple and both spouses reported the same sex, the relationship category remained husband or wife, but the sex of the partner who reported being a spouse to the householder was changed. Humans all the way down\n\n\n\n\nHuman actions are ubiquitous in every part of data analysis! The most objective methods often have had subjective actions before and after."
  },
  {
    "objectID": "week3/slides.html#further-reading",
    "href": "week3/slides.html#further-reading",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Further reading",
    "text": "Further reading\n\nHuebner et al (2018) A Contemporary Conceptual Framework for Initial Data Analysis\nHuebner et al (2020) Hidden analyses\nChatfield (1985) The Initial Examination of Data. Journal of the Royal Statistical Society. Series A (General) 148 \nCox & Snell (1981) Applied Statistics. London: Chapman and Hall.\nvan der Loo and de Jonge (2018). Statistical Data Cleaning with Applications in R. John Wiley and Sons Ltd.\nHyndman (2014) Explaining the ABS unemployment fluctuations"
  },
  {
    "objectID": "week2/worksheet.html",
    "href": "week2/worksheet.html",
    "title": "ETC5521 Worksheet Week 2",
    "section": "",
    "text": "The goal of this worksheet is to search for R packages that can be used for exploring data, and to understand their capacity.\nThe paper The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week2/worksheet.html#objectives",
    "href": "week2/worksheet.html#objectives",
    "title": "ETC5521 Worksheet Week 2",
    "section": "",
    "text": "The goal of this worksheet is to search for R packages that can be used for exploring data, and to understand their capacity.\nThe paper The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week2/worksheet.html#tasks",
    "href": "week2/worksheet.html#tasks",
    "title": "ETC5521 Worksheet Week 2",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. Use AI to obtain a list of packages that might be used for exploring data in R.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nHere is my list of prompts:\n\nDo you know the paper “The Landscape of R Packages for Automated Exploratory Data Analysis” ?\nWhat would you recommend for R packages for exploratory data analysis today? I’m not sure that the package list in that paper are reasonable 5 years into the future of today.\nWhat about databot? (Notice the botching of author: “Jennifer (Yihui) Cheng and others”)\nAre there any packages that are true to Tukey’s original book Exploratory Data Analysis from 1977?\nWhat about packages that can explore high-dimensional data?\n\n\n\n\n\n\n\n2. Pick one of the packages mentioned and give it a spin\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Use AI to learn about some of Tukey’s most famous quotes, and also what methods he developed are still in common use today\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nMy prompts:\n\nWhat are some of Tukey’s most famous quotes?\nWhat are some methods that Tukey developed that are commonly used today?\nWhat are the R packages that have these methods?\nWhat about the letter value plot?\nI would love a “A one-page “Tukey contributions” teaching sheet for a lecture?” Can you provide it as a quarto document?\n\n\n\n\n\n\n\n4. For the gardenR example from week 1, ask for some Tukey-style summaries of this data\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nPrompt:\n\nFor the gardenR package data, could you make some code to do Tukey-like summaries of the data?"
  },
  {
    "objectID": "week2/worksheetsol.html",
    "href": "week2/worksheetsol.html",
    "title": "ETC5521 Worksheet Week 2",
    "section": "",
    "text": "The goal of this worksheet is to search for R packages that can be used for exploring data, and to understand their capacity.\nThe paper The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week2/worksheetsol.html#objectives",
    "href": "week2/worksheetsol.html#objectives",
    "title": "ETC5521 Worksheet Week 2",
    "section": "",
    "text": "The goal of this worksheet is to search for R packages that can be used for exploring data, and to understand their capacity.\nThe paper The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week2/worksheetsol.html#tasks",
    "href": "week2/worksheetsol.html#tasks",
    "title": "ETC5521 Worksheet Week 2",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. Use AI to obtain a list of packages that might be used for exploring data in R.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nHere is my list of prompts:\n\nDo you know the paper “The Landscape of R Packages for Automated Exploratory Data Analysis” ?\nWhat would you recommend for R packages for exploratory data analysis today? I’m not sure that the package list in that paper are reasonable 5 years into the future of today.\nWhat about databot? (Notice the botching of author: “Jennifer (Yihui) Cheng and others”)\nAre there any packages that are true to Tukey’s original book Exploratory Data Analysis from 1977?\nWhat about packages that can explore high-dimensional data?\n\n\n\n\n\n\n\n2. Pick one of the packages mentioned and give it a spin\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Use AI to learn about some of Tukey’s most famous quotes, and also what methods he developed are still in common use today\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nMy prompts:\n\nWhat are some of Tukey’s most famous quotes?\nWhat are some methods that Tukey developed that are commonly used today?\nWhat are the R packages that have these methods?\nWhat about the letter value plot?\nI would love a “A one-page “Tukey contributions” teaching sheet for a lecture?” Can you provide it as a quarto document?\n\n\n\n\n\n\n\n4. For the gardenR example from week 1, ask for some Tukey-style summaries of this data\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nPrompt:\n\nFor the gardenR package data, could you make some code to do Tukey-like summaries of the data?"
  },
  {
    "objectID": "week2/index.html",
    "href": "week2/index.html",
    "title": "Week 2: Learning from history",
    "section": "",
    "text": "EDA Case Study: Bay area blues"
  },
  {
    "objectID": "week2/index.html#main-reference",
    "href": "week2/index.html#main-reference",
    "title": "Week 2: Learning from history",
    "section": "",
    "text": "EDA Case Study: Bay area blues"
  },
  {
    "objectID": "week2/index.html#what-you-will-learn-this-week",
    "href": "week2/index.html#what-you-will-learn-this-week",
    "title": "Week 2: Learning from history",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nThe historical origins of EDA\nPencil and paper methods like stem-and-leaf plots\nHow to symmetrise and linearise your data\nWhere EDA is relevant today"
  },
  {
    "objectID": "week2/index.html#lecture-slides",
    "href": "week2/index.html#lecture-slides",
    "title": "Week 2: Learning from history",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week2/index.html#worksheet",
    "href": "week2/index.html#worksheet",
    "title": "Week 2: Learning from history",
    "section": "Worksheet",
    "text": "Worksheet\n\nqmd\nhtml"
  },
  {
    "objectID": "week2/index.html#tutorial-instructions",
    "href": "week2/index.html#tutorial-instructions",
    "title": "Week 2: Learning from history",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week2/index.html#assignments",
    "href": "week2/index.html#assignments",
    "title": "Week 2: Learning from history",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 1 is due on Thursday 07 August.\nQuiz 2 is due on Thursday 14 August."
  },
  {
    "objectID": "week11/index.html",
    "href": "week11/index.html",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "",
    "text": "How to use a tour to check if your model suffers from multicollinearity"
  },
  {
    "objectID": "week11/index.html#main-reference",
    "href": "week11/index.html#main-reference",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "",
    "text": "How to use a tour to check if your model suffers from multicollinearity"
  },
  {
    "objectID": "week11/index.html#what-you-will-learn-this-week",
    "href": "week11/index.html#what-you-will-learn-this-week",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nDifferent types of model fitting\nDecomposing data from model\n\nfitted\nresidual\n\nDiagnostic calculations\n\nanomalies\nleverage\ninfluence"
  },
  {
    "objectID": "week11/index.html#lecture-slides",
    "href": "week11/index.html#lecture-slides",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week11/index.html#tutorial-instructions",
    "href": "week11/index.html#tutorial-instructions",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week11/index.html#assignments",
    "href": "week11/index.html#assignments",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Assignments",
    "text": "Assignments\n\nProject Part 1 is due on Monday 13 October.\nQuiz 9 is due on Thursday 09 October.\nQuiz 10 is due on Thursday 16 October."
  },
  {
    "objectID": "week1/worksheet.html",
    "href": "week1/worksheet.html",
    "title": "ETC5521 Worksheet 1",
    "section": "",
    "text": "The goal of this worksheet is to tackle a data analysis together, by\n\nmapping out an analysis, with class input and help of AI\nwork on cleaning a data set\nmaking some plots\ndiscussing what is surprising, and what is expected"
  },
  {
    "objectID": "week1/worksheet.html#objectives",
    "href": "week1/worksheet.html#objectives",
    "title": "ETC5521 Worksheet 1",
    "section": "",
    "text": "The goal of this worksheet is to tackle a data analysis together, by\n\nmapping out an analysis, with class input and help of AI\nwork on cleaning a data set\nmaking some plots\ndiscussing what is surprising, and what is expected"
  },
  {
    "objectID": "week1/worksheet.html#about-the-data",
    "href": "week1/worksheet.html#about-the-data",
    "title": "ETC5521 Worksheet 1",
    "section": "📋 About the data",
    "text": "📋 About the data\nThe data to use is available from Tidy Tuesday 28 May 2024 page. Download the data from here, ideally using the tidytuesdayR package. You should only download the data from the Tidy Tuesday once, and save a copy locally on your computer.\nIn addition the gardenR package, available from remotes::install_github(\"llendway/gardenR\") has extra details about the garden.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThis is the code to download the data, and load relevant libraries.\n\n\nCode\n# remotes::install_github(\"llendway/gardenR\")\n# install.packages(\"tidytuesdayR\")\nlibrary(tidytuesdayR)\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-28')\nsave(tuesdata, file=\"tuesdata.rda\")\n\n\n\n\nCode\nlibrary(gardenR)\nlibrary(tidyverse)\nlibrary(ggbeeswarm)\n\nload(\"tuesdata.rda\")\nharvest_2020 &lt;- tuesdata$harvest_2020\nharvest_2021 &lt;- tuesdata$harvest_2021\nplanting_2020 &lt;- tuesdata$planting_2020\nplanting_2021 &lt;- tuesdata$planting_2021\nspending_2020 &lt;- tuesdata$spending_2020\nspending_2021 &lt;- tuesdata$spending_2021\n\ndata(\"garden_coords\")"
  },
  {
    "objectID": "week1/worksheet.html#tasks",
    "href": "week1/worksheet.html#tasks",
    "title": "ETC5521 Worksheet 1",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. What are the variable types, how and when was the data collected?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThis is collected in 2020 and 2021, and has a variety of numeric and categorical and time variables.\n\n\n\n\n\n\n2. What would some possible questions be to ask about this data?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe link to the ChatGPT conversation is here. It was prompted by\n\nDo you know the gardenR package by Lisa Lendway?\nWhat might be some questions that we could answer with this data?\n\nWe chose to tackle one of the Economics questions: “How much produce (by weight or value) was harvested per dollar spent?” But realised that it was not possible answer this particular question with this data. It was refined to be:\nCompare the ROI for varieties of beans.\nThe next step was to filter the data to focus on one vegetable, beans, and one year to get started.\n\n\nCode\nbeans_planting_2020 &lt;- planting_2020 |&gt;\n  filter(vegetable == \"beans\")\nbeans_harvest_2020 &lt;- harvest_2020 |&gt;\n  filter(vegetable == \"beans\")\nbeans_spending_2020 &lt;- spending_2020 |&gt;\n  filter(vegetable == \"beans\")\n\n\n\n\n\n\n\n\n3. What do you expect to find?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nWe might expect some difference in ROI between varieties.\n\n\n\n\n\n\n4. Pick one of the questions, and let’s try to answer it.\n\nWhat summaries should we make?\nAre we likely going to need to pre-process the data in any way?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nWe need to decide on a common scale. Steps are:\n\nexamine the price for each variety\nsummarise the planting data by variety and join to spending data.\nsummarise the harvest data by variety and join to spending.\n\n\n\nCode\nbeans_spending_2020 |&gt;\n  select(variety, brand, price_with_tax)\n\n\n# A tibble: 3 × 3\n  variety             brand          price_with_tax\n  &lt;chr&gt;               &lt;chr&gt;                   &lt;dbl&gt;\n1 Bush Bush Slender   Renee's Garden           3.01\n2 Chinese Red Noodle  Baker Creek              3.24\n3 Classic Slenderette Renee's Garden           3.23\n\n\nCode\nbeans_planting_2020_smry &lt;- beans_planting_2020 |&gt;\n  group_by(variety) |&gt;\n  summarise(number_seeds_planted = sum(number_seeds_planted))\n\nbeans_2020_smry &lt;- beans_spending_2020 |&gt;\n  select(variety, brand, price_with_tax) |&gt;\n  left_join(beans_planting_2020_smry)\n\nggplot(beans_2020_smry, \n       aes(price_with_tax, \n           number_seeds_planted)) + geom_point()\n\n\n\n\n\n\n\n\n\nCode\nbeans_2020_smry |&gt; \n  mutate(pr_per_seed = price_with_tax/number_seeds_planted) |&gt;\n  select(variety, pr_per_seed)\n\n\n# A tibble: 3 × 2\n  variety             pr_per_seed\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 Bush Bush Slender        0.0752\n2 Chinese Red Noodle       0.324 \n3 Classic Slenderette      0.111 \n\n\nCode\n# Do all the seed packs have the same number of seeds, \n# or did Lisa plant every seed in every pack?\n\n# Harvest\nbeans_harvest_2020_smry &lt;- beans_harvest_2020 |&gt;\n  group_by(variety) |&gt;\n  summarise(weight = sum(weight))\nbeans_harvest_2020_smry\n\n\n# A tibble: 3 × 2\n  variety             weight\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Bush Bush Slender    10038\n2 Chinese Red Noodle     356\n3 Classic Slenderette   1635\n\n\nCode\nbeans_2020_smry &lt;- beans_2020_smry |&gt;\n  left_join(beans_harvest_2020_smry)\n\nbeans_2020_smry &lt;- beans_2020_smry |&gt;\n  mutate(psy = weight/number_seeds_planted)\nbeans_2020_smry\n\n\n# A tibble: 3 × 6\n  variety             brand     price_with_tax number_seeds_planted weight   psy\n  &lt;chr&gt;               &lt;chr&gt;              &lt;dbl&gt;                &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Bush Bush Slender   Renee's …           3.01                   40  10038 251. \n2 Chinese Red Noodle  Baker Cr…           3.24                   10    356  35.6\n3 Classic Slenderette Renee's …           3.23                   29   1635  56.4\n\n\nProblems encountered.\n\nSpending didn’t specify what the cost involved, whether it was one packet of seeds,\nNot clear whether all seeds in each packet were planted. Numbers suggest, no, because some varieties had many sends and others very few.\n\nAdjustments:\n\nweight was adjusted by number of seeds planted\n\nConclusion:\nBush Bush Slender outperforms the other two, by a lot! This should also indicate that this variety is a better ROI.\nCaveats: Need to check that\n\nthe plots where each was planted was a reasonable chance that they had same access to sunlight and nutrients.\nAssuming that seeds were planted at same distance from each other.\n\n\nNext steps\nExamine the 2021 data. If same varieties grown do the same results happen.\nProblems discovered in doing this: Name of variety in 2021 might have changed to be just “Bush”, and the other two were not used. Could compare against the one new variety."
  },
  {
    "objectID": "week1/worksheetsol.html",
    "href": "week1/worksheetsol.html",
    "title": "ETC5521 Worksheet 1",
    "section": "",
    "text": "The goal of this worksheet is to tackle a data analysis together, by\n\nmapping out an analysis, with class input and help of AI\nwork on cleaning a data set\nmaking some plots\ndiscussing what is surprising, and what is expected"
  },
  {
    "objectID": "week1/worksheetsol.html#objectives",
    "href": "week1/worksheetsol.html#objectives",
    "title": "ETC5521 Worksheet 1",
    "section": "",
    "text": "The goal of this worksheet is to tackle a data analysis together, by\n\nmapping out an analysis, with class input and help of AI\nwork on cleaning a data set\nmaking some plots\ndiscussing what is surprising, and what is expected"
  },
  {
    "objectID": "week1/worksheetsol.html#about-the-data",
    "href": "week1/worksheetsol.html#about-the-data",
    "title": "ETC5521 Worksheet 1",
    "section": "📋 About the data",
    "text": "📋 About the data\nThe data to use is available from Tidy Tuesday 28 May 2024 page. Download the data from here, ideally using the tidytuesdayR package. You should only download the data from the Tidy Tuesday once, and save a copy locally on your computer.\nIn addition the gardenR package, available from remotes::install_github(\"llendway/gardenR\") has extra details about the garden.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThis is the code to download the data, and load relevant libraries.\n\n\nCode\n# remotes::install_github(\"llendway/gardenR\")\n# install.packages(\"tidytuesdayR\")\nlibrary(tidytuesdayR)\ntuesdata &lt;- tidytuesdayR::tt_load('2024-05-28')\nsave(tuesdata, file=\"tuesdata.rda\")\n\n\n\n\nCode\nlibrary(gardenR)\nlibrary(tidyverse)\nlibrary(ggbeeswarm)\n\nload(\"tuesdata.rda\")\nharvest_2020 &lt;- tuesdata$harvest_2020\nharvest_2021 &lt;- tuesdata$harvest_2021\nplanting_2020 &lt;- tuesdata$planting_2020\nplanting_2021 &lt;- tuesdata$planting_2021\nspending_2020 &lt;- tuesdata$spending_2020\nspending_2021 &lt;- tuesdata$spending_2021\n\ndata(\"garden_coords\")"
  },
  {
    "objectID": "week1/worksheetsol.html#tasks",
    "href": "week1/worksheetsol.html#tasks",
    "title": "ETC5521 Worksheet 1",
    "section": "🧩 Tasks",
    "text": "🧩 Tasks\n\n1. What are the variable types, how and when was the data collected?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThis is collected in 2020 and 2021, and has a variety of numeric and categorical and time variables.\n\n\n\n\n\n\n2. What would some possible questions be to ask about this data?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe link to the ChatGPT conversation is here. It was prompted by\n\nDo you know the gardenR package by Lisa Lendway?\nWhat might be some questions that we could answer with this data?\n\nWe chose to tackle one of the Economics questions: “How much produce (by weight or value) was harvested per dollar spent?” But realised that it was not possible answer this particular question with this data. It was refined to be:\nCompare the ROI for varieties of beans.\nThe next step was to filter the data to focus on one vegetable, beans, and one year to get started.\n\n\nCode\nbeans_planting_2020 &lt;- planting_2020 |&gt;\n  filter(vegetable == \"beans\")\nbeans_harvest_2020 &lt;- harvest_2020 |&gt;\n  filter(vegetable == \"beans\")\nbeans_spending_2020 &lt;- spending_2020 |&gt;\n  filter(vegetable == \"beans\")\n\n\n\n\n\n\n\n\n3. What do you expect to find?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nWe might expect some difference in ROI between varieties.\n\n\n\n\n\n\n4. Pick one of the questions, and let’s try to answer it.\n\nWhat summaries should we make?\nAre we likely going to need to pre-process the data in any way?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nWe need to decide on a common scale. Steps are:\n\nexamine the price for each variety\nsummarise the planting data by variety and join to spending data.\nsummarise the harvest data by variety and join to spending.\n\n\n\nCode\nbeans_spending_2020 |&gt;\n  select(variety, brand, price_with_tax)\n\n\n# A tibble: 3 × 3\n  variety             brand          price_with_tax\n  &lt;chr&gt;               &lt;chr&gt;                   &lt;dbl&gt;\n1 Bush Bush Slender   Renee's Garden           3.01\n2 Chinese Red Noodle  Baker Creek              3.24\n3 Classic Slenderette Renee's Garden           3.23\n\n\nCode\nbeans_planting_2020_smry &lt;- beans_planting_2020 |&gt;\n  group_by(variety) |&gt;\n  summarise(number_seeds_planted = sum(number_seeds_planted))\n\nbeans_2020_smry &lt;- beans_spending_2020 |&gt;\n  select(variety, brand, price_with_tax) |&gt;\n  left_join(beans_planting_2020_smry)\n\nggplot(beans_2020_smry, \n       aes(price_with_tax, \n           number_seeds_planted)) + geom_point()\n\n\n\n\n\n\n\n\n\nCode\nbeans_2020_smry |&gt; \n  mutate(pr_per_seed = price_with_tax/number_seeds_planted) |&gt;\n  select(variety, pr_per_seed)\n\n\n# A tibble: 3 × 2\n  variety             pr_per_seed\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 Bush Bush Slender        0.0752\n2 Chinese Red Noodle       0.324 \n3 Classic Slenderette      0.111 \n\n\nCode\n# Do all the seed packs have the same number of seeds, \n# or did Lisa plant every seed in every pack?\n\n# Harvest\nbeans_harvest_2020_smry &lt;- beans_harvest_2020 |&gt;\n  group_by(variety) |&gt;\n  summarise(weight = sum(weight))\nbeans_harvest_2020_smry\n\n\n# A tibble: 3 × 2\n  variety             weight\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Bush Bush Slender    10038\n2 Chinese Red Noodle     356\n3 Classic Slenderette   1635\n\n\nCode\nbeans_2020_smry &lt;- beans_2020_smry |&gt;\n  left_join(beans_harvest_2020_smry)\n\nbeans_2020_smry &lt;- beans_2020_smry |&gt;\n  mutate(psy = weight/number_seeds_planted)\nbeans_2020_smry\n\n\n# A tibble: 3 × 6\n  variety             brand     price_with_tax number_seeds_planted weight   psy\n  &lt;chr&gt;               &lt;chr&gt;              &lt;dbl&gt;                &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Bush Bush Slender   Renee's …           3.01                   40  10038 251. \n2 Chinese Red Noodle  Baker Cr…           3.24                   10    356  35.6\n3 Classic Slenderette Renee's …           3.23                   29   1635  56.4\n\n\nProblems encountered.\n\nSpending didn’t specify what the cost involved, whether it was one packet of seeds,\nNot clear whether all seeds in each packet were planted. Numbers suggest, no, because some varieties had many sends and others very few.\n\nAdjustments:\n\nweight was adjusted by number of seeds planted\n\nConclusion:\nBush Bush Slender outperforms the other two, by a lot! This should also indicate that this variety is a better ROI.\nCaveats: Need to check that\n\nthe plots where each was planted was a reasonable chance that they had same access to sunlight and nutrients.\nAssuming that seeds were planted at same distance from each other.\n\n\nNext steps\nExamine the 2021 data. If same varieties grown do the same results happen.\nProblems discovered in doing this: Name of variety in 2021 might have changed to be just “Bush”, and the other two were not used. Could compare against the one new variety."
  },
  {
    "objectID": "week1/slides.html#about-this-unit",
    "href": "week1/slides.html#about-this-unit",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "About this unit",
    "text": "About this unit"
  },
  {
    "objectID": "week1/slides.html#teaching-team-12",
    "href": "week1/slides.html#teaching-team-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Teaching team 1/2",
    "text": "Teaching team 1/2\n\n\n\nDi Cook  Distinguished Professor  Monash University \n\n🌐 https://dicook.org/\n✉️ ETC5521.Clayton-x@monash.edu\n🦣 @visnut@aus.social  @visnut.bsky.social\n\n\nI have a PhD from Rutgers University, NJ, and a Bachelor of Science from University of New England\nI am a Fellow of the American Statistical Association, elected member of the the R Foundation and International Statistical Institute, Past-Editor of the Journal of Computational and Graphical Statistics, and the R Journal.\nMy research is in data visualisation, statistical graphics and computing, with application to sports, ecology and bioinformatics. I likes to develop new methodology and software.\nMy students work on methods and software that is generally useful for the world. They have been responsible for bringing you the tidyverse suite, knitr, plotly, and many other R packages we regularly use."
  },
  {
    "objectID": "week1/slides.html#teaching-team-22",
    "href": "week1/slides.html#teaching-team-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Teaching team 2/2",
    "text": "Teaching team 2/2\n\n\n\nKrisanat Anukarnsakulchularp  Master of Business Analytics  Monash University \n\n🌐 https://github.com/KrisanatA\n✉️ ETC5521.Clayton-x@monash.edu\n\n\nHe has a Bachelor of Actuarial Science, Monash University, 2018 - 2021\nand a Master of Business Analytics, Monash University | 2022 - 2023.\nHe has published the R package animbook\nand is a first year PhD student at Monash, working on data structures, visualisation and models for spatiotemporal networks.\nThis is his fourth semester tutoring at Monash, and the only unit working on this semester."
  },
  {
    "objectID": "week1/slides.html#got-a-question-or-a-comment",
    "href": "week1/slides.html#got-a-question-or-a-comment",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Got a question, or a comment?",
    "text": "Got a question, or a comment?\n\n✋ 🔡 You can ask directly by unmuting yourself, or typing in the chat, of the live lecture.\n\n💻 If watching the recording, please post questions in the discussion (ED) forum.\n\nI hope you have many questions! 🙋🏻👣"
  },
  {
    "objectID": "week1/slides.html#welcome",
    "href": "week1/slides.html#welcome",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Welcome!",
    "text": "Welcome!\n\n\nSynopsis\nBeyond modelling and prediction, data might have many more stories to tell. Exploring data to uncover patterns and structures, involves both numerical and visual techniques designed to reveal interesting information that may be unexpected. However, an analyst must be cautious not to over-interpret apparent patterns, and to use randomisation tools to assess whether the patterns are real or spurious.\n\n\n\nLearning objectives\n\nlearn to use modern data exploration tools with many different types of contemporary data to uncover interesting structures, unusual relationships and anomalies.\nunderstand how to map out appropriate analyses for a given data set and description, define what we would expect to see in the data, and whether what we see is contrary to expectations.\nbe able to compute null samples in order to test apparent patterns, and to interpret the results using computational methods for statistical inference.\ncritically assess the strength and adequacy of data analysis."
  },
  {
    "objectID": "week1/slides.html#unit-structure",
    "href": "week1/slides.html#unit-structure",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "📅 Unit Structure",
    "text": "📅 Unit Structure\n\n\n2 hour lecture 👩‍🏫 Tue 2.00 - 4:00pm, on zoom (see moodle for the link) Class is more fun if you can attend live!\n1 hour workshop Tue 4:00 - 5:00pm, on same zoom link. This is based on material during lecture.\n1 hour on-campus tutorial 🛠️ Thu 9:00-10:00am, 10:00-11am and 3:00-4:00pm CL_Anc-19.LTB_188 Attendance is expected - this is the chance to practice and get help with assignments from your tutor’s."
  },
  {
    "objectID": "week1/slides.html#resources",
    "href": "week1/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "📚 Resources",
    "text": "📚 Resources\n🏡 Course homepage: this is where you find the course materials  (lecture slides, tutorials and tutorial solutions) https://ddde.numbat.space/\n\n🈴 Moodle: this is where you find discussion forum, zoom links, and marks https://learning.monash.edu/course/view.php?id=34784\n\n🧰 GitHub classroom: this is where you will find assignments, but links to each will be available in moodle. https://classroom.github.com/"
  },
  {
    "objectID": "week1/slides.html#assessment-part-12",
    "href": "week1/slides.html#assessment-part-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "💯 Assessment Part 1/2",
    "text": "💯 Assessment Part 1/2\n\nWeekly quizzes (5%) There will be a weekly quiz starting week 2 provided through Moodle. These are a great chance to check your knowledge, and help you prepare for the tutorial and to keep up to date with the weekly course material. Your best 10 scores will be used for your final quiz total. \nExercises 1 (15%), through GitHub classroom, Due: Aug 18, 11:55pm. This is an individual assessment. \nExercises 2 (20%), through GitHub classroom, Due: Sep 1, 11:55pm. This is an individual assessment. \nExercises 3 (20%): through GitHub classroom, Due: Sep 22, 11:55pm. This is an individual assessment. \nProject, parts 1 and 2 (20% each), through GitHub classroom, Due: Oct 13, 11:55pm and Nov 3, 11:55pm."
  },
  {
    "objectID": "week1/slides.html#github-classroom",
    "href": "week1/slides.html#github-classroom",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nWe are going to use GitHub Classroom to distribute assignment templates and keep track of your assignment progress.\n\n\nClone the first assignment by clicking on the link given in Moodle.\nOnce you have accepted it, you will get a cloned copy on your own GitHub account. It is a private repo, which means you and the teaching staff will be the only people with access.\nIf you need some help getting started, check this information.\nThe week 1 tutorial will help you get started."
  },
  {
    "objectID": "week1/slides.html#what-does-it-mean-to-explore-data",
    "href": "week1/slides.html#what-does-it-mean-to-explore-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What does it mean to explore data?",
    "text": "What does it mean to explore data?\n\n\n\n\nhttps://www.gocomics.com/calvinandhobbes/2015/08/26"
  },
  {
    "objectID": "week1/slides.html#a-simple-example-to-illustrate-exploratory-data-analysis-contrasted-with-a-confirmatory-data-analysis",
    "href": "week1/slides.html#a-simple-example-to-illustrate-exploratory-data-analysis-contrasted-with-a-confirmatory-data-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "A simple example to illustrate “exploratory data analysis” contrasted with a “confirmatory data analysis”",
    "text": "A simple example to illustrate “exploratory data analysis” contrasted with a “confirmatory data analysis”"
  },
  {
    "objectID": "week1/slides.html#what-are-the-factors-that-affect-tipping-behaviour",
    "href": "week1/slides.html#what-are-the-factors-that-affect-tipping-behaviour",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What are the factors that affect tipping behaviour?",
    "text": "What are the factors that affect tipping behaviour?\n\n\nIn one restaurant, a food server recorded the following data on all customers they served during an interval of two and a half months in early 1990.\nFood servers’ tips in restaurants may be influenced by many factors, including the nature of the restaurant, size of the party, and table locations in the restaurant. Restaurant managers need to know which factors matter when they assign tables to food servers.\n\n\n\nlibrary(tidyverse)\ntips &lt;- read_csv(\"http://ggobi.org/book/data/tips.csv\")"
  },
  {
    "objectID": "week1/slides.html#what-is-tipping",
    "href": "week1/slides.html#what-is-tipping",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is tipping?",
    "text": "What is tipping?\n\nWhen you’re dining at a full-service restaurant\n\nTip 20 percent of your full bill.\n\nWhen you grab a cup of coffee\n\nRound up or add a dollar if you’re a regular or ordered a complicated drink.\n\nWhen you have lunch at a food truck\n\nDrop a few dollars into the tip jar, but a little less than you would at a dine-in spot.\n\nWhen you use a gift card\n\nTip on the total value of the meal, not just what you paid out of pocket.\n\n\n\n\nThe basic rules of tipping that everyone should know about"
  },
  {
    "objectID": "week1/slides.html#recommended-procedure-in-the-book",
    "href": "week1/slides.html#recommended-procedure-in-the-book",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Recommended procedure in the book",
    "text": "Recommended procedure in the book\n\nStep 1: Develop a model\n\nShould the response be tip alone and use the total bill as a predictor?\nShould you create a new variable tip rate and use this as the response?\n\nStep 2: Fit the full model with sex, smoker, day, time and size as predictors\nStep 3: Refine model: Should some variables should be dropped?\nStep 4: Check distribution of residuals\nStep 5: Summarise the model, if X=something, what would be the expected tip"
  },
  {
    "objectID": "week1/slides.html#step-1",
    "href": "week1/slides.html#step-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 1",
    "text": "Step 1\nCalculate tip % as tip/total bill \\(\\times\\) 100\n  \n\ntips &lt;- tips %&gt;%\n  mutate(tip_pct = tip/totbill * 100) \n\n\n\nNote: Creating new variables (sometimes called feature engineering), is a common step in any data analysis."
  },
  {
    "objectID": "week1/slides.html#step-2-fit",
    "href": "week1/slides.html#step-2-fit",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 2 Fit",
    "text": "Step 2 Fit\nFit the full model with all variables\n \n\ntips_lm &lt;- tips %&gt;%\n  select(tip_pct, sex, smoker, day, time, size) %&gt;%\n  lm(tip_pct ~ ., data=.)"
  },
  {
    "objectID": "week1/slides.html#step-2-model-summary",
    "href": "week1/slides.html#step-2-model-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 2 Model summary",
    "text": "Step 2 Model summary\n\n\n\nlibrary(broom)\nlibrary(kableExtra)\ntidy(tips_lm) %&gt;% \n  kable(digits=2) %&gt;% \n  kable_styling() \n\n\nglance(tips_lm) %&gt;% \n  select(r.squared, statistic, \n         p.value) %&gt;% \n  kable(digits=3)\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n20.66\n2.49\n8.29\n0.00\n\n\nsexM\n-0.85\n0.83\n-1.02\n0.31\n\n\nsmokerYes\n0.36\n0.85\n0.43\n0.67\n\n\ndaySat\n-0.18\n1.83\n-0.10\n0.92\n\n\ndaySun\n1.67\n1.90\n0.88\n0.38\n\n\ndayThu\n-1.82\n2.32\n-0.78\n0.43\n\n\ntimeNight\n-2.34\n2.61\n-0.89\n0.37\n\n\nsize\n-0.96\n0.42\n-2.28\n0.02\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nstatistic\np.value\n\n\n\n\n0.042\n1.5\n0.17\n\n\n\n\n\n🤔 Which variable(s) would be considered important for predicting tip %?"
  },
  {
    "objectID": "week1/slides.html#step-3-refine-model",
    "href": "week1/slides.html#step-3-refine-model",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 3: Refine model",
    "text": "Step 3: Refine model\n\n\n\ntips_lm &lt;- tips %&gt;%\n  select(tip_pct, size) %&gt;% \n  lm(tip_pct ~ ., data=.) \ntidy(tips_lm) %&gt;% \n  kable(digits=2) %&gt;% \n  kable_styling() \n\n\nglance(tips_lm) %&gt;% \n  select(r.squared, statistic, p.value) %&gt;% \n  kable(digits=3)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n18.44\n1.12\n16.5\n0.00\n\n\nsize\n-0.92\n0.41\n-2.2\n0.03\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nstatistic\np.value\n\n\n\n\n0.02\n5\n0.026"
  },
  {
    "objectID": "week1/slides.html#model-summary",
    "href": "week1/slides.html#model-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model summary",
    "text": "Model summary\n \n\\[\\widehat{tip %} = 18.44 - 0.92 \\times size\\]\n\n \nAs the size of the dining party increases by one person the tip decreases by approximately 1%."
  },
  {
    "objectID": "week1/slides.html#model-assessment",
    "href": "week1/slides.html#model-assessment",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model assessment",
    "text": "Model assessment\n   \\(R^2 = 0.02\\).\n\n  This dropped by half from the full model, even though no other variables contributed significantly to the model. It might be a good step to examine interaction terms.\nWhat does \\(R^2 = 0.02\\) mean?"
  },
  {
    "objectID": "week1/slides.html#model-assessment-1",
    "href": "week1/slides.html#model-assessment-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model assessment",
    "text": "Model assessment\n\\(R^2 = 0.02\\) means that size explains just 2% of the variance in tip %. This is a very weak model.\n\nAnd \\(R^2 = 0.04\\) is also a very weak model.\nWhat do the \\(F\\) statistic and \\(p\\)-value mean?\nWhat do the \\(t\\) statistics and \\(p\\)-value associated with model coefficients mean?"
  },
  {
    "objectID": "week1/slides.html#overall-model-significance",
    "href": "week1/slides.html#overall-model-significance",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Overall model significance",
    "text": "Overall model significance\nAssume that we have a random sample from a population. Assume that the model for the population is\n\\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed\n\\[ \\widehat{tip %} = b_0 + b_1  sexM + ... + b_7 size \\] The \\(F\\) statistic refers to\n\\[ H_o: \\beta_1 = ... = \\beta_7 = 0 ~~ vs ~~ H_a: \\text{at least one is not 0}\\] The \\(p\\)-value is the probability that we observe the given \\(F\\) value or larger, computed assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week1/slides.html#term-significance",
    "href": "week1/slides.html#term-significance",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Term significance",
    "text": "Term significance\nAssume that we have a random sample from a population. Assume that the model for the population is\n\\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed\n\\[ \\widehat{tip %} = b_0 + b_1  sexM + ... + b_7 size \\]\nThe \\(t\\) statistics in the coefficient summary refer to\n\\[ H_o: \\beta_k = 0 ~~ vs ~~ H_a: \\beta_k \\neq 0 \\] The \\(p\\)-value is the probability that we observe the given \\(t\\) value or more extreme, computed assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week1/slides.html#model-diagnostics-md",
    "href": "week1/slides.html#model-diagnostics-md",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model diagnostics (MD)",
    "text": "Model diagnostics (MD)\nNormally, the final model summary would be accompanied diagnostic plots\n\nobserved vs fitted values to check strength and appropriateness of the fit\nunivariate plot, and normal probability plot, of residuals to check for normality\nin the simple final model like this, the observed vs predictor, with model overlaid would be advised to assess the model relative to the variability around the model\nwhen the final model has more terms, using a partial dependence plot to check the relative relationship between the response and predictors would be recommended."
  },
  {
    "objectID": "week1/slides.html#residual-plots",
    "href": "week1/slides.html#residual-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plots",
    "text": "Residual plots\n\n\n\ntips_aug &lt;- augment(tips_lm)\nggplot(tips_aug, \n    aes(x=.resid)) + \n  geom_histogram() +\n  xlab(\"residuals\")"
  },
  {
    "objectID": "week1/slides.html#residual-normal-probability-plots",
    "href": "week1/slides.html#residual-normal-probability-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual normal probability plots",
    "text": "Residual normal probability plots\n\n\n\nggplot(tips_aug, \n    aes(sample=.resid)) + \n  stat_qq() +\n  stat_qq_line() +\n  xlab(\"residuals\") +\n  theme(aspect.ratio=1)"
  },
  {
    "objectID": "week1/slides.html#fitted-vs-observed",
    "href": "week1/slides.html#fitted-vs-observed",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Fitted vs observed",
    "text": "Fitted vs observed\n\n\n\nggplot(tips_aug, \n    aes(x=.fitted, y=tip_pct)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  xlab(\"observed\") +\n  ylab(\"fitted\")"
  },
  {
    "objectID": "week1/slides.html#model-in-the-data-space",
    "href": "week1/slides.html#model-in-the-data-space",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "“Model-in-the-data-space”",
    "text": "“Model-in-the-data-space”\n\n\n\nggplot(tips_aug, \n    aes(x=size, y=tip_pct)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  ylab(\"tip %\")\n\n\nThe fitted model is overlaid on a plot of the data. This is called “model-in-the-data-space” (Wickham et al, 2015).\n\nAll the plots on the previous three slides: histogram of residuals, normal probability plot, fitted vs residuals are considered to be “data-in-the-model-space”. Stay tuned for more discussion on this later."
  },
  {
    "objectID": "week1/slides.html#section",
    "href": "week1/slides.html#section",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The result of this work might leave us with\n\na model that could be used to impose a dining/tipping policy in restaurants (see here)\n\n\nbut it should also leave us with an unease that this policy is based on weak support."
  },
  {
    "objectID": "week1/slides.html#summary",
    "href": "week1/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\n\n\nPlots as we have just seen, associated with pursuit of an answer to a specific question may be best grouped into the category of “model diagnostics (MD)”.\n\nThere are additional categories of plots for data analysis that include initial data analysis (IDA), descriptive statistics. Stay tuned for more on these.\n\n\nA separate and big area for plots of data is for communication, where we already know what is in the data and we want to communicate the information as best possible.\n\nWhen exploring data, we are using data plots to discover things we didn’t already know."
  },
  {
    "objectID": "week1/slides.html#what-did-this-analysis-miss",
    "href": "week1/slides.html#what-did-this-analysis-miss",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What did this analysis miss?",
    "text": "What did this analysis miss?"
  },
  {
    "objectID": "week1/slides.html#general-strategy-for-exploring-data",
    "href": "week1/slides.html#general-strategy-for-exploring-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "General strategy for EXPLORING DATA",
    "text": "General strategy for EXPLORING DATA\n\n\nIt’s a good idea to examine the data description, the explanation of the variables, and how the data was collected.\nYou need to know what type of variables are in the data in order to decide appropriate choice of plots, and calculations to make.\nData description should have information about data collection methods, so that the extent of what we learn from the data might apply to new data.\n\n\nWhat does that look like here?\n\nglimpse(tips)\n\nRows: 244\nColumns: 9\n$ obs     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1…\n$ totbill &lt;dbl&gt; 17.0, 10.3, 21.0, 23.7, 24.6, 25…\n$ tip     &lt;dbl&gt; 1.0, 1.7, 3.5, 3.3, 3.6, 4.7, 2.…\n$ sex     &lt;chr&gt; \"F\", \"M\", \"M\", \"M\", \"F\", \"M\", \"M…\n$ smoker  &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ day     &lt;chr&gt; \"Sun\", \"Sun\", \"Sun\", \"Sun\", \"Sun…\n$ time    &lt;chr&gt; \"Night\", \"Night\", \"Night\", \"Nigh…\n$ size    &lt;dbl&gt; 2, 3, 3, 2, 4, 4, 2, 4, 2, 2, 2,…\n$ tip_pct &lt;dbl&gt; 5.9, 16.1, 16.7, 14.0, 14.7, 18.…\n\n\n\nLook at the distribution of quantitative variables tips, total bill.\n\n\n\nExamine the distributions across categorical variables.\n\n\nExamine quantitative variables relative to categorical variables"
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips",
    "href": "week1/slides.html#distributions-of-tips",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) + \n  geom_histogram(\n    colour=\"white\")"
  },
  {
    "objectID": "week1/slides.html#because-one-binwidth-is-never-enough",
    "href": "week1/slides.html#because-one-binwidth-is-never-enough",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Because, one binwidth is never enough …",
    "text": "Because, one binwidth is never enough …"
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips-1",
    "href": "week1/slides.html#distributions-of-tips-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) +\n  geom_histogram(\n    breaks=seq(0.5,10.5,1),  \n    colour=\"white\") + \n  scale_x_continuous(\n    breaks=seq(0,11,1))\n\nBig fat bins. Tips are skewed, which means most tips are relatively small."
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips-2",
    "href": "week1/slides.html#distributions-of-tips-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) + \n  geom_histogram(\n    breaks=seq(0.5,10.5,0.1), \n    colour=\"white\") +\n  scale_x_continuous(\n    breaks=seq(0,11,1))\n\nSkinny bins. Tips are multimodal, and occurring at the full dollar and 50c amounts."
  },
  {
    "objectID": "week1/slides.html#we-could-also-look-at-total-bill-this-way",
    "href": "week1/slides.html#we-could-also-look-at-total-bill-this-way",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "We could also look at total bill this way",
    "text": "We could also look at total bill this way\nbut I’ve already done this, and we don’t learn anything more about the multiple peaks than waht is learned by plotting tips."
  },
  {
    "objectID": "week1/slides.html#relationship-between-tip-and-total",
    "href": "week1/slides.html#relationship-between-tip-and-total",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Relationship between tip and total",
    "text": "Relationship between tip and total\n\n\n\np &lt;- ggplot(tips, \n    aes(x= totbill, y=tip)) + \n  geom_point() + \n  scale_y_continuous(\n    breaks=seq(0,11,1))\np\n\nWhy is total on the x axis? \nShould we add a guideline?"
  },
  {
    "objectID": "week1/slides.html#add-a-guideline-indicating-common-practice",
    "href": "week1/slides.html#add-a-guideline-indicating-common-practice",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Add a guideline indicating common practice",
    "text": "Add a guideline indicating common practice\n\n\n\np &lt;- p + geom_abline(intercept=0, \n              slope=0.2) + \n  annotate(\"text\", x=45, y=10, \n           label=\"20% tip\") \np\n\n\n\n\nMost tips less than 20%: Skin flints vs generous diners\nA couple of big tips\nBanding horizontally is the rounding seen previously"
  },
  {
    "objectID": "week1/slides.html#we-should-examine-bar-charts-and-mosaic-plots-of-the-categorical-variables-next",
    "href": "week1/slides.html#we-should-examine-bar-charts-and-mosaic-plots-of-the-categorical-variables-next",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "We should examine bar charts and mosaic plots of the categorical variables next",
    "text": "We should examine bar charts and mosaic plots of the categorical variables next\nbut I’ve already done that, and there’s not too much of interest there."
  },
  {
    "objectID": "week1/slides.html#relative-to-categorical-variables",
    "href": "week1/slides.html#relative-to-categorical-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Relative to categorical variables",
    "text": "Relative to categorical variables\n\n\n\np + facet_grid(smoker~sex) \n\n\n\n\n\n\n\n\n\n\n\n\nThe bigger bills tend to be paid by men (and females that smoke).\nExcept for three diners, female non-smokers are very consistent tippers, probably around 15-18% though.\nThe variability in the smokers is much higher than for the non-smokers."
  },
  {
    "objectID": "week1/slides.html#isnt-this-interesting",
    "href": "week1/slides.html#isnt-this-interesting",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Isn’t this interesting?",
    "text": "Isn’t this interesting?"
  },
  {
    "objectID": "week1/slides.html#procedure-of-eda",
    "href": "week1/slides.html#procedure-of-eda",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Procedure of EDA",
    "text": "Procedure of EDA\n\nWe gained a wealth of insight in a short time.\nUsing nothing but graphical methods we investigated univariate, bivariate, and multivariate relationships.\nWe found both global features and local detail. We saw that\n\ntips were rounded; then we saw the obvious\n\ncorrelation between the tip and the size of the bill, noting the scarcity of generous tippers; finally we\ndiscovered differences in the tipping behavior of male and female smokers and non-smokers.\n\n\nThese are unexpected insights were missed from the analysis that focused solely on the primary question."
  },
  {
    "objectID": "week1/slides.html#what-can-go-wrong",
    "href": "week1/slides.html#what-can-go-wrong",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What can go wrong?",
    "text": "What can go wrong?"
  },
  {
    "objectID": "week1/slides.html#how-was-data-collected",
    "href": "week1/slides.html#how-was-data-collected",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How was data collected?",
    "text": "How was data collected?\n\n\nIn one restaurant, a food server recorded the following data on all customers they served during an interval of two and a half months in early 1990.\n\nHow much can you infer about tipping more broadly?\n\n\nTip has a weak but significant relationship with total bill?\nTips have a skewed distribution? (More small tips and fewer large tips?)\nTips tend to be made in nice round numbers.\nPeople generally under-tip?\nSmokers are less reliable tippers."
  },
  {
    "objectID": "week1/slides.html#ways-to-verify-support-or-refute-generalisations",
    "href": "week1/slides.html#ways-to-verify-support-or-refute-generalisations",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Ways to verify, support or refute generalisations",
    "text": "Ways to verify, support or refute generalisations\n\n\n\nexternal information\nother studies/samples\ngood choice of calculations and plots\nall the permutations and subsets of measured variables\ncomputational re-sampling methods (we’ll see these soon)\n\n\n\nPoor data collection methods affects every analysis, including statistical or computational modeling.\n\n\nFor this waiter and the restaurant manager, there is some useful information. Like what?\n\n\nService fee for smokers to ensure consistency?\nAssign waiter to variety of party sizes and composition.\nShifts on different days or time of day (not shown)."
  },
  {
    "objectID": "week1/slides.html#words-of-wisdom",
    "href": "week1/slides.html#words-of-wisdom",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Words of wisdom",
    "text": "Words of wisdom\nFalse discovery is the lesser danger when compared to non-discovery. Non-discovery is the failure to identify meaningful structure, and it may result in false or incomplete modeling. In a healthy scientific enterprise, the fear of non-discovery should be at least as great as the fear of false discovery."
  },
  {
    "objectID": "week1/slides.html#guide",
    "href": "week1/slides.html#guide",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Guide",
    "text": "Guide\n\n\n\nRead the data description to understand the context, and the extent of the data collection.\nUnderstand the types of variables that have been measured.\nBrainstorm a set of questions that might be interesting to answer with this data.\nFor each of your question, write down what you EXPECT to find.\nMap out the possible plots and numerical summaries to make, that could be made, but particularly, what needs to be computed in order to answer the questions.\n\n\n\nWhat potential errors might be in the data? Missing values, not recorded data, errors in coding, and think about the strategy to deal with them.\nStart on your analysis. Think about the results suggested and whether they match or contradict what you expected.\nUse randomisation methods to learn whether what you have observed is possibly spurious.\nCommunicate findings."
  },
  {
    "objectID": "week1/slides.html#topics-covered",
    "href": "week1/slides.html#topics-covered",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Topics covered",
    "text": "Topics covered\n\nMethods for single, bivariate, multivariate\n\nnumerical variables\ncategorical variables\n\nMethods to accommodate temporal and spatial (maybe also networks) context\nHow to make effective comparisons\nUtilising computational methods to assess what you see is “real”"
  },
  {
    "objectID": "week1/slides.html#resources-1",
    "href": "week1/slides.html#resources-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nCook and Swayne (2007) Interactive and Dynamic Graphics for Data Analysis, Introduction\nDonoho (2017) 50 Years of Data Science\nStaniak and Biecek (2019) The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "ETC5521 Resources",
    "section": "",
    "text": "Books\n\nUnwin (2015) Graphical Data Analysis with R\nWilke (2019) Fundamentals of Data Visualization\nCook and Swayne (2007) Interactive and Dynamic Graphics for Data Analysis, Introduction\nvan der Loo and de Jonge (2018). Statistical Data Cleaning with Applications in R, John Wiley and Sons Ltd.\nCleveland (1993) Visualizing Data, Hobart Press.\nCox & Snell (1981) Applied Statistics, London: Chapman and Hall.\nMoraga (2019) Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny\nSievert (2019) Interactive web-based data visualization with R, plotly, and shiny\n\n\n\nWebsites\n\nJosse et al (2022) R-miss-tastic\nFriendly and Denis Milestones in History of Thematic Cartography, Statistical Graphics and Data Visualisation available at http://www.datavis.ca/milestones/\nWang, Cook, Hyndman, O’Hara-Wild (2019) tsibble\ncubble: A Vector Spatio-Temporal Data Structure for Data Analysis\nTierney, Cook, Prvan (2020) Browse Over Longitudinal Data Graphically and Analytically in R\nsf: Simple Features for R\nVisualising spatial data using R\nCook and Laa (2023) Interactively exploring high-dimensional data and models in R\nMason, Lee, Laa, and Cook (2022). cassowaryr: Compute Scagnostics on Pairs of Numeric Variables in a Data Set\n\n\n\nArticles\n\nDonoho (2017) 50 Years of Data Science\nStaniak and Biecek (2019) The Landscape of R Packages for Automated Exploratory Data Analysis\nHuebner et al (2018) A Contemporary Conceptual Framework for Initial Data Analysis\nHuebner et al (2020) Hidden analyses\nChatfield (1985) The Initial Examination of Data, Journal of the Royal Statistical Society, Series A (General) 148(3):214–231\nHyndman (2014) Explaining the ABS unemployment fluctuations\nBuja et al. (2009). Statistical Inference for Exploratory Data Analysis and Model Diagnostics. Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences 367 (1906): 4361–83.\nWickham et al (2010) Graphical Inference for Infovis. IEEE Transactions on Visualization and Computer Graphics 16 (6): 973–79.\nHofmann et al (2012) Graphical Tests for Power Comparison of Competing Designs. IEEE Transactions on Visualization and Computer Graphics 18 (12): 2441–48.\nMajumder et al (2013) Validation of Visual Statistical Inference, Applied to Linear Models. Journal of the American Statistical Association 108 (503): 942–56.\nTierney et al (2023) Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.\nBecker, R. A., Cleveland, W. S., & Shyu, M. J. (1996). “The Visual Design and Control of Trellis Display.” Journal of Computational and Graphical Statistics, 5(2), 123-155.\nWang, Cook, Hyndman (2019) A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data\nKobakian et al Hexagon tile map\nWickham et al (2011). tourr: An R Package for Exploring Multivariate Data with Projections"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Professor Di Cook\n\nEmail: etc5521.clayton-x@monash.edu\nConsultation: Fridays 11-1 Clayton: Education Blg, its blg 6, 29 Ancora Imparo way, Room 352 and on zoom (see link in moodle)"
  },
  {
    "objectID": "index.html#lecturerchief-examiner",
    "href": "index.html#lecturerchief-examiner",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Professor Di Cook\n\nEmail: etc5521.clayton-x@monash.edu\nConsultation: Fridays 11-1 Clayton: Education Blg, its blg 6, 29 Ancora Imparo way, Room 352 and on zoom (see link in moodle)"
  },
  {
    "objectID": "index.html#tutors",
    "href": "index.html#tutors",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Tutors",
    "text": "Tutors\n\nKrisanat Anukarnsakulchularp\n\nTutorials: Thu 9am (CL_LTB_188), 10am (CL_LTB_188), 3pm (CL_LTB_387)\nConsultation: Mondays 9-10am, Thursdays 4-5pm Clayton: Education Blg, its blg 6, 29 Ancora Imparo way, Room 232A"
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Weekly schedule",
    "text": "Weekly schedule\n\nLecture+workshop: Tues 2-5pm on zoom (link in Moodle)\nTutorial: 1 hour\nWeekly learning quizzes due each Thursday 9am, from week 2\n\n\n\n\nWeek\nTopic\nReference\nAssessments\n\n\n\n\n29 Jul\nOverview. Why this course? What is EDA?\nThe Landscape of R Packages for Automated Exploratory Data Analysis\n\n\n\n05 Aug\nLearning from history\nEDA Case Study: Bay area blues\nQuiz 1\n\n\n12 Aug\nInitial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA\nThe initial examination of data\nQuiz 2\n\n\n19 Aug\nUsing computational tools to determine whether what is seen in the data can be assumed to apply more broadly\nWickham et al. (2010) Graphical inference for Infovis\nExercises 1,Quiz 3\n\n\n26 Aug\nWorking with a single variable, making transformations, detecting outliers, using robust statistics\nWilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions\nQuiz 4\n\n\n02 Sep\nBivariate dependencies and relationships, transformations to linearise\nWilke (2019) Ch 12 Visualising associations\nExercises 2,Quiz 5\n\n\n09 Sep\nMaking comparisons between groups and strata\nWilke (2019) Ch 9, 10.2-4, 11.2\nQuiz 6\n\n\n16 Sep\nGoing beyond two variables, exploring high dimensions\nCook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1\nQuiz 7\n\n\n23 Sep\nExploring data having a space and time context Part I\nbrolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R\nExercises 3,Quiz 8\n\n\n30 Sep\nMid-semester break\n\n\n\n\n07 Oct\nExploring data having a space and time context Part II\ncubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data\nQuiz 9\n\n\n14 Oct\nSculpting data using models, checking assumptions, co-dependency and performing diagnostics\nHow to use a tour to check if your model suffers from multicollinearity\nProject Part 1,Quiz 10\n\n\n21 Oct\nHelp session\n\nQuiz 11\n\n\n04 Nov\n\n\nProject Part 2"
  },
  {
    "objectID": "index.html#assessments",
    "href": "index.html#assessments",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Assessments",
    "text": "Assessments\n\nWeekly learning quizzes: 5% (Due each week by Thu 9am, weeks 2-12)\nExercises 1: Instructions (15%) (Due Monday 11:55pm)\nExercises 2: Instructions (20%) (Due Monday 11:55pm)\nExercises 3: Instructions (20%) (Due Monday 11:55pm)\nProject part 1: Instructions (20%) (Due Monday 11:55pm)\nProject part 2: Instructions (20%) (Due Monday 11:55pm)"
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Software",
    "text": "Software\nWe will be using the latest versions of R and RStudio.\nHere is the code to install (most of) the R packages we will be using in this unit.\ninstall.packages(c(\"tidyr\", \"dplyr\", \"readr\", \"readxl\", \"readabs\", \"forcats\", \"tsibble\", \"cubble\", \"lubridate\", \"ggplot2\", \"GGally\", \"ggthemes\", \"sugrrants\", \"ggbeeswarm\", \"plotly\", \"gganimate\", \"tourr\", \"sugarbag\", \"tsibbletalk\", \"visdat\", \"inspectdf\", \"naniar\", \"validate\", \"vcd\", \"mvtnorm\", \"nullabor\", \"visage\", \"forecast\", \"cassowaryr\", \"brolgar\", \"palmerpenguins\", \"housingData\",  \"broom\", \"kableExtra\", \"lvplot\", \"colorspace\", \"patchwork\"), dependencies=TRUE)\nFrom GitHub, install\nremotes::install_github(\"casperhart/detourr\")\nIf you are relatively new to R, working through the materials at https://startr.numbat.space is an excellent way to up-skill. You are epsecially encouraged to work through Chapter 3, on Troubleshooting and asking for help, because at some point you will need help with your coding, and how you go about this matters and impacts the ability of others to help you.\nThese materials are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "week1/index.html",
    "href": "week1/index.html",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "",
    "text": "The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week1/index.html#reading",
    "href": "week1/index.html#reading",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "",
    "text": "The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week1/index.html#what-you-will-learn-this-week",
    "href": "week1/index.html#what-you-will-learn-this-week",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nHow exploring data is different from a confirmatory analysis\nGet up and running with GitHub Classroom"
  },
  {
    "objectID": "week1/index.html#lecture-slides",
    "href": "week1/index.html#lecture-slides",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week1/index.html#worksheet",
    "href": "week1/index.html#worksheet",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Worksheet",
    "text": "Worksheet\n\nqmd\nhtml"
  },
  {
    "objectID": "week1/index.html#tutorial-instructions",
    "href": "week1/index.html#tutorial-instructions",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week1/index.html#assignments",
    "href": "week1/index.html#assignments",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 1 is due on Thursday 07 August."
  },
  {
    "objectID": "week1/tutorialsol.html",
    "href": "week1/tutorialsol.html",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you’ve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorialsol.html#objectives",
    "href": "week1/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you’ve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorialsol.html#preparation",
    "href": "week1/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 1",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nHave git installed on your laptop so that you can access the GitHub classroom.\nHave the latest versions of RStudio and R installed on your laptop.\nInstall this list of R packages:\n\n\nCreate an RStudio Project for this unit, called ETC5521. All your work in the tutorials should be conducted in this project. Ideally, your project is organised into folders, one for data, one for tutorial_XX, … Each week when you begin your tutorial, open the project."
  },
  {
    "objectID": "week1/tutorialsol.html#exercises",
    "href": "week1/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 1",
    "section": "📥 Exercises",
    "text": "📥 Exercises"
  },
  {
    "objectID": "week1/tutorialsol.html#how-good-are-your-detective-skills",
    "href": "week1/tutorialsol.html#how-good-are-your-detective-skills",
    "title": "ETC5521 Tutorial 1",
    "section": "1. How good are your detective skills?",
    "text": "1. How good are your detective skills?\nBeing good at noticing something unexpected or unusual is an important skills for exploratory data analysis. This exercise is designed to practice your detective skills.\nPlay the game alzheimer_test from the fun package by running this code:\nYou will be given 6 tasks to complete. Each one is to find a specific letter hidden among a \\(10\\times 30\\) grid of letters. When you are finished, answer these questions:\n\nWhich task did you THINK was the most difficult?\nWhich task does the DATA say was most difficult based, based on the time taken to answer, tm1.1.j. in your results data?\nSave the dataset to an .rda file.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\n           char1.1.j. char2.1.j.  tm1.1.j.\nans.user.2          M          N 30.839718\nans.user.3          I          T 19.695932\nans.user.5          D          O 17.189302\nans.user.1          O          C 16.534676\nans.user.4          F          E  4.424869\nans.user            9          6  3.812386"
  },
  {
    "objectID": "week1/tutorialsol.html#get-started-using-github-classroom",
    "href": "week1/tutorialsol.html#get-started-using-github-classroom",
    "title": "ETC5521 Tutorial 1",
    "section": "3. Get started using GitHub Classroom",
    "text": "3. Get started using GitHub Classroom\n\nIn Moodle go to the Assignment 1 instructions to find the invitation to a GitHub Classroom. Accept this invitation.\nClone the assignment repo to your computer.\nOpen the assign01.html instructions.\nMake a start on loading the data into R."
  },
  {
    "objectID": "week1/tutorialsol.html#finishing-up",
    "href": "week1/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 1",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week1/tutorial.html",
    "href": "week1/tutorial.html",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you’ve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorial.html#objectives",
    "href": "week1/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you’ve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorial.html#preparation",
    "href": "week1/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 1",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nHave git installed on your laptop so that you can access the GitHub classroom.\nHave the latest versions of RStudio and R installed on your laptop.\nInstall this list of R packages:\n\n\nCreate an RStudio Project for this unit, called ETC5521. All your work in the tutorials should be conducted in this project. Ideally, your project is organised into folders, one for data, one for tutorial_XX, … Each week when you begin your tutorial, open the project."
  },
  {
    "objectID": "week1/tutorial.html#exercises",
    "href": "week1/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 1",
    "section": "📥 Exercises",
    "text": "📥 Exercises"
  },
  {
    "objectID": "week1/tutorial.html#how-good-are-your-detective-skills",
    "href": "week1/tutorial.html#how-good-are-your-detective-skills",
    "title": "ETC5521 Tutorial 1",
    "section": "1. How good are your detective skills?",
    "text": "1. How good are your detective skills?\nBeing good at noticing something unexpected or unusual is an important skills for exploratory data analysis. This exercise is designed to practice your detective skills.\nPlay the game alzheimer_test from the fun package by running this code:\nYou will be given 6 tasks to complete. Each one is to find a specific letter hidden among a \\(10\\times 30\\) grid of letters. When you are finished, answer these questions:\n\nWhich task did you THINK was the most difficult?\nWhich task does the DATA say was most difficult based, based on the time taken to answer, tm1.1.j. in your results data?\nSave the dataset to an .rda file.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\n           char1.1.j. char2.1.j.  tm1.1.j.\nans.user.2          M          N 30.839718\nans.user.3          I          T 19.695932\nans.user.5          D          O 17.189302\nans.user.1          O          C 16.534676\nans.user.4          F          E  4.424869\nans.user            9          6  3.812386"
  },
  {
    "objectID": "week1/tutorial.html#get-started-using-github-classroom",
    "href": "week1/tutorial.html#get-started-using-github-classroom",
    "title": "ETC5521 Tutorial 1",
    "section": "3. Get started using GitHub Classroom",
    "text": "3. Get started using GitHub Classroom\n\nIn Moodle go to the Assignment 1 instructions to find the invitation to a GitHub Classroom. Accept this invitation.\nClone the assignment repo to your computer.\nOpen the assign01.html instructions.\nMake a start on loading the data into R."
  },
  {
    "objectID": "week1/tutorial.html#finishing-up",
    "href": "week1/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 1",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week10/index.html",
    "href": "week10/index.html",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "",
    "text": "cubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data\nAlso see the Moraga, Paula. (2019). Geospatial Health Data."
  },
  {
    "objectID": "week10/index.html#main-reference",
    "href": "week10/index.html#main-reference",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "",
    "text": "cubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data\nAlso see the Moraga, Paula. (2019). Geospatial Health Data."
  },
  {
    "objectID": "week10/index.html#what-you-will-learn-this-week",
    "href": "week10/index.html#what-you-will-learn-this-week",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nBreaking up data by time, and by space\nChanging focus:\n\nMaps of space over time\nExploring time over space with glyph maps\n\nInference for spatial trends\nA flash back to the 1970s: Tukey’s median polish\nWorking with spatial polygon data\n\nMaking a choropleth map\nBending the choropleth into a cartogram\nTiling spatial regions"
  },
  {
    "objectID": "week10/index.html#lecture-slides",
    "href": "week10/index.html#lecture-slides",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week10/index.html#tutorial-instructions",
    "href": "week10/index.html#tutorial-instructions",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week10/index.html#assignments",
    "href": "week10/index.html#assignments",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 9 is due on Thursday 09 October."
  },
  {
    "objectID": "week12/index.html#assignments",
    "href": "week12/index.html#assignments",
    "title": "Week 12: Long help session",
    "section": "Assignments",
    "text": "Assignments\n\nProject Part 1 is due on Monday 13 October.\nQuiz 10 is due on Thursday 16 October.\nQuiz 11 is due on Thursday 23 October."
  },
  {
    "objectID": "week2/slides.html#birth-of-eda",
    "href": "week2/slides.html#birth-of-eda",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Birth of EDA",
    "text": "Birth of EDA\n\nThe field of exploratory data analysis came of age when this book appeared in 1977.\n\nTukey held that too much emphasis in statistics was placed on statistical hypothesis testing (confirmatory data analysis); more emphasis needed to be placed on using data to suggest hypotheses to test."
  },
  {
    "objectID": "week2/slides.html#john-w.-tukey",
    "href": "week2/slides.html#john-w.-tukey",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "John W. Tukey",
    "text": "John W. Tukey\n\n\n\n\n\n\n Image source: wikimedia.org\n\n\nBorn in 1915, in New Bedford, Massachusetts.\nMum was a private tutor who home-schooled John. Dad was a Latin teacher.\nBA and MSc in Chemistry, and PhD in Mathematics\nAwarded the National Medal of Science in 1973, by President Nixon\nBy some reports, his home-schooling was unorthodox and contributed to his thinking and working differently."
  },
  {
    "objectID": "week2/slides.html#taking-a-glimpse-back-in-time",
    "href": "week2/slides.html#taking-a-glimpse-back-in-time",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Taking a glimpse back in time",
    "text": "Taking a glimpse back in time\nis possible with the American Statistical Association video lending library.\n We’re going to watch John Tukey talking about exploring high-dimensional data with an amazing new computer in 1973, four years before the EDA book.\n\nLook out for these things:\nTukey’s expertise is described as for trial and error learning and the computing equipment.\n\n\nFirst 4.25 minutes"
  },
  {
    "objectID": "week2/slides.html#setting-the-frame-of-mind",
    "href": "week2/slides.html#setting-the-frame-of-mind",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Setting the frame of mind",
    "text": "Setting the frame of mind\nExcerpt from the introduction\n\nThis book is based on an important principle.\n It is important to understand what you CAN DO before you learn to measure how WELL you seem to have DONE it.\n Learning first what you can do will help you to work more easily and effectively.\n This book is about exploratory data analysis, about looking at data to see what it seems to say. It concentrates on simple arithmetic and easy-to-draw pictures. It regards whatever appearances we have recognized as partial descriptions, and tries to look beneath them for new insights. Its concern is with appearance, not with confirmation.\n Examples, NOT case histories\n The book does not exist to make the case that exploratory data analysis is useful. Rather it exists to expose its readers and users to a considerable variety of techniques for looking more effectively at one’s data. The examples are not intended to be complete case histories. Rather they should isolated techniques in action on real data. The emphasis is on general techniques, rather than specific problems. \nA basic problem about any body of data is to make it more easily and effectively handleable by minds – our minds, her mind, his mind. To this general end:\n\nanything that make a simpler description possible makes the description more easily handleable.\nanything that looks below the previously described surface makes the description more effective.\n\n\nSo we shall always be glad (a) to simplify description and (b) to describe one layer deeper. In particular,\n\nto be able to say that we looked one layer deeper, and found nothing, is a definite step forward – though not as far as to be able to say that we looked deeper and found thus-and-such.\nto be able to say that “if we change our point of view in the following way … things are simpler” is always a gain–though not quite so much as to be able to say “if we don’t bother to change out point of view (some other) things are equally simple.”\n\n …\n Consistent with this view, we believe, is a clear demand that pictures based on exploration of data should force their messages upon us. Pictures that emphasize what we already know–“security blankets” to reassure us–are frequently not worth the space they take. Pictures that have to be gone over with a reading glass to see the main point are wasteful of time and inadequate of effect. The greatest value of a picture is when it forces us to notice what we never expected to see.\n\n\nConfirmation\n\n\nThe principles and procedures of what we call confirmatory data analysis are both widely used and one of the great intellectual products of our century. In their simplest form, these principles and procedures look at a sample–and at what that sample has told us about the population from which it came–and assess the precision with which our inference from sample to population is made. We can no longer get along without confirmatory data analysis. But we need not start with it.\n\nThe best way to understand what CAN be done is not longer–if it ever was–to ask what things could, in the current state of our skill techniques, be confirmed (positively or negatively). Even more understanding is lost if we consider each thing we can do to data only in terms of some set of very restrictive assumptions under which that thing is best possible–assumptions we know we CANNOT check in practice.\n\nExploration AND confirmation\n\nOnce upon a time, statisticians only explored. Then they learned to confirm exactly–to confirm a few things exactly, each under very specific circumstances. As they emphasized exact confirmation, their techniques inevitably became less flexible. The connection of the most used techniques with past insights was weakened. Anything to which confirmatory procedure was not explicitly attached was decried as “mere descriptive statistics”, no matter how much we learned from it.\n\nToday, the flexibility of (approximate) confirmation by the jacknife makes it relatively easy to ask, for almost any clearly specified exploration, “How far is it confirmed?”\n\nToday, exploratory and confirmatory can–and should–proceed side by side. This book, of course, considers only exploratory techniques, leaving confirmatory techniques to other accounts.\n\n\n About the problems \n\n The teacher needs to be careful about assigning problems. Not too many, please. They are likely to take longer than you think. The number supplied is to accommodate diversity of interest, not to keep everybody busy.\n Besides the length of our problems, both teacher and student need to realise that many problems do not have a single “right answer”. There can be many ways to approach a body of data. Not all are equally good. For some bodies of data this may be clear, but for others we may not be able to tell from a single body of data which approach is preferred. Even several bodies of data about very similar situations may not be enough to show which approach should be preferred. Accordingly, it will often be quite reasonable for different analysts to reach somewhat different analyses.\n Yet more–to unlock the analysis of a body of day, to find the good way to approach it, may require a key, whose finding is a creative act. Not everyone can be expected to create the key to any one situation. And to continue to paraphrase Barnum, no one can be expected to create a key to each situation he or she meets.\n To learn about data analysis, it is right that each of us try many things that do not work–that we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed–at least until we were told about it–an opportunity to learn more. Each teacher needs to recognize this in grading and commenting on problems.\n\n\n Precision\n\nThe teacher who heeds these words and admits that there need be no one correct approach may, I regret to contemplate, still want whatever is done to be digit perfect. (Under such a requirement, the write should still be able to pass the course, but it is not clear whether she would get an “A”.) One does, from time to time, have to produce digit-perfect, carefully checked results, but forgiving techniques that are not too distributed by unusual data are also, usually, little disturbed by SMALL arithmetic errors. The techniques we discuss here have been chosen to be forgiving. It is hoped, then, that small arithmetic errors will take little off the problem’s grades, leaving severe penalties for larger errors, either of arithmetic or concept."
  },
  {
    "objectID": "week2/slides.html#outline",
    "href": "week2/slides.html#outline",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Outline",
    "text": "Outline\n\n\n\nScratching down numbers\nSchematic summary\nEasy re-expression\nEffective comparison\nPlots of relationship\nStraightening out plots (using three points)\nSmoothing sequences\nParallel and wandering schematic plots\nDelineations of batches of points\nUsing two-way analyses\n\n\n\n\n\nMaking two-way analyses\nAdvanced fits\nThree way fits\nLooking in two or more ways at batched of points\nCounted fractions\nBetter smoothing\nCounts in bin after bin\nProduct-ratio plots\nShapes of distributions\nMathematical distributions"
  },
  {
    "objectID": "week2/slides.html#looking-at-numbers-with-tukey",
    "href": "week2/slides.html#looking-at-numbers-with-tukey",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Looking at numbers with Tukey",
    "text": "Looking at numbers with Tukey"
  },
  {
    "objectID": "week2/slides.html#scratching-down-numbers",
    "href": "week2/slides.html#scratching-down-numbers",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scratching down numbers",
    "text": "Scratching down numbers\n\n\nPrices of Chevrolet in the local used car newspaper ads of 1968.\n\noptions(width=20)\nchevrolets &lt;- tibble(\n  prices = c(250, 150, 795, 895, 695, \n               1699, 1499, 1099, 1693, 1166,\n               688, 1333, 895, 1775, 895,\n               1895, 795))\n#chevrolets$prices\n\n\nStem-and-leaf plot: still seen in introductory statistics texts"
  },
  {
    "objectID": "week2/slides.html#section-1",
    "href": "week2/slides.html#section-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "First stem-and-leaf, first digit on stem, second digit on leaf\n\n\nOrder any leaves which need it, eg stem 6\n\n\n\nA benefit is that the numbers can be read off the plot, but the focus is still on the pattern. Also quantiles like the median, can be computed easily."
  },
  {
    "objectID": "week2/slides.html#section-2",
    "href": "week2/slides.html#section-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Shrink the stem\n\n\nShrink the stem more"
  },
  {
    "objectID": "week2/slides.html#and-in-r",
    "href": "week2/slides.html#and-in-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "And, in R …",
    "text": "And, in R …\n\nchevrolets$prices\n\n [1]  250  150  795\n [4]  895  695 1699\n [7] 1499 1099 1693\n[10] 1166  688 1333\n[13]  895 1775  895\n[16] 1895  795\n\nstem(chevrolets$prices)\n\n\n  The decimal point is 3 digit(s) to the right of the |\n\n  0 | 23\n  0 | 7788999\n  1 | 123\n  1 | 57789"
  },
  {
    "objectID": "week2/slides.html#remember-the-tips-data",
    "href": "week2/slides.html#remember-the-tips-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "🔖 Remember the tips data",
    "text": "🔖 Remember the tips data\n\n\n [1] 1.01 1.66 3.50 3.31 3.61 4.71 2.00 3.12 1.96 3.23 1.71 5.00 1.57 3.00 3.02\n[16] 3.92 1.67 3.71 3.50 3.35 4.08 2.75 2.23 7.58 3.18 2.34 2.00 2.00 4.30 3.00\n[31] 1.45 2.50 3.00 2.45 3.27 3.60 2.00 3.07 2.31 5.00 2.24 2.54 3.06 1.32 5.60\n[46] 3.00 5.00 6.00 2.05 3.00\n\n\n\nstem(tips$tip, scale=0.5, width=120)\n\n\n  The decimal point is at the |\n\n   1 | 000001233334445555555555556666667777788889\n   2 | 000000000000000000000000000000000000000001122222223333555555555555556666677788899\n   3 | 00000000000000000000000011111112222222333344445555555555555666778889\n   4 | 0000000000001112233335777\n   5 | 00000000001122226799\n   6 | 05577\n   7 | 6\n   8 | \n   9 | 0\n  10 | 0"
  },
  {
    "objectID": "week2/slides.html#refining-the-size",
    "href": "week2/slides.html#refining-the-size",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Refining the size",
    "text": "Refining the size\n\n\nFive digits per stem\n\n\n\nWhat is the number in parentheses? And why might this be useful?\n\n\n\nTwo digits per stem\n\n\n\n\n\nstem(tips$tip, scale=2)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n   10 | 0000107\n   12 | 55526\n   14 | 44578000000000678\n   16 | 1346781356\n   18 | 032678\n   20 | 00000000000000000000000000000000011233598\n   22 | 0033440114\n   24 | 5700000000002456\n   26 | 01412455\n   28 | 382\n   30 | 00000000000000000000000267891245688\n   32 | 133557159\n   34 | 0188800000000015\n   36 | 0181566\n   38 | 2\n   40 | 0000000000006889\n   42 | 09004\n   44 | 0\n   46 | 713\n   48 | \n   50 | 000000000074567\n   52 | 0\n   54 | \n   56 | 05\n   58 | 52\n   60 | 0\n   62 | \n   64 | 00\n   66 | 03\n   68 | \n   70 | \n   72 | \n   74 | 8\n   76 | \n   78 | \n   80 | \n   82 | \n   84 | \n   86 | \n   88 | \n   90 | 0\n   92 | \n   94 | \n   96 | \n   98 | \n  100 | 0\n\n\n\n\nWhy no number in parentheses?\n\n\n\nmedian(tips$tip)\n\n[1] 2.9"
  },
  {
    "objectID": "week2/slides.html#summary",
    "href": "week2/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\nStem-and-leaf plots are similar information to the histogram.\nGenerally it is possible to also read off the numbers, and to then easily calculate median or Q1 or Q3.\nIt’s great for small data sets, when you only have pencil and paper.\nAlternatives are a histogram, (jittered) dotplot, density plot, box plot, violin plot, letter value plot."
  },
  {
    "objectID": "week2/slides.html#a-different-style-of-number-scratching",
    "href": "week2/slides.html#a-different-style-of-number-scratching",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "a different style of number scratching",
    "text": "a different style of number scratching\nfor categorical variables\n\n\nWe know about\n\nbut its too easy to\n\nmake a mistake\n\nIs this easier?\n\n\nor harder"
  },
  {
    "objectID": "week2/slides.html#count-this-data-using-the-squares-approach.",
    "href": "week2/slides.html#count-this-data-using-the-squares-approach.",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Count this data using the squares approach.",
    "text": "Count this data using the squares approach.\n\n\n\n\n [1] \"F\" \"M\" \"M\" \"M\" \"F\" \"M\"\n [7] \"M\" \"M\" \"M\" \"M\" \"M\" \"F\"\n[13] \"M\" \"M\" \"F\" \"M\" \"F\" \"M\"\n[19] \"F\" \"M\" \"M\" \"F\" \"F\" \"M\"\n[25] \"M\" \"M\" \"M\" \"M\" \"M\" \"F\"\n[31] \"M\" \"M\" \"F\" \"F\" \"M\" \"M\"\n[37] \"M\" \"F\" \"M\" \"M\" \"M\" \"M\"\n[43] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n[49] \"M\" \"M\" \"M\" \"F\" \"F\" \"M\"\n[55] \"M\" \"M\" \"M\" \"F\" \"M\" \"M\"\n[61] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n[67] \"F\" \"F\" \"M\" \"M\" \"M\" \"F\""
  },
  {
    "objectID": "week2/slides.html#what-does-it-mean-to-feel-what-the-data-are-like",
    "href": "week2/slides.html#what-does-it-mean-to-feel-what-the-data-are-like",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What does it mean to “feel what the data are like?”",
    "text": "What does it mean to “feel what the data are like?”"
  },
  {
    "objectID": "week2/slides.html#section-3",
    "href": "week2/slides.html#section-3",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "This is a stem and leaf of the height of the highest peak in each of the 50 US states.\n\nThe states roughly fall into three groups.\n\nIt’s not really surprising, but we can imagine this grouping. Alaska is in a group of its own, with a much higher high peak. Then the Rocky Mountain states, California, Washington and Hawaii also have high peaks, and the rest of the states lump together."
  },
  {
    "objectID": "week2/slides.html#section-4",
    "href": "week2/slides.html#section-4",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "More summaries of numerical values"
  },
  {
    "objectID": "week2/slides.html#hinges-and-5-number-summaries",
    "href": "week2/slides.html#hinges-and-5-number-summaries",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hinges and 5-number summaries",
    "text": "Hinges and 5-number summaries\n\n\n\n\n [1] -3.2 -1.7 -0.4  0.1\n [5]  0.3  1.2  1.5  1.8\n [9]  2.4  3.0  4.3  6.4\n[13]  9.8\n\n\nYou know the median is the middle number. What’s a hinge?\nThere are 13 data values here, provided already sorted. We are going to write them into a Tukey named down-up-down-up pattern, evenly.\nMedian will be 7th, hinge will be 4th from each end."
  },
  {
    "objectID": "week2/slides.html#hinges-and-5-number-summary",
    "href": "week2/slides.html#hinges-and-5-number-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hinges and 5-number summary",
    "text": "Hinges and 5-number summary\n\n\n\n\n\nHinges are almost always the same as Q1 and Q3"
  },
  {
    "objectID": "week2/slides.html#box-and-whisker-display",
    "href": "week2/slides.html#box-and-whisker-display",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "box-and-whisker display",
    "text": "box-and-whisker display\n\n\n\n\nStarting with a 5-number summary"
  },
  {
    "objectID": "week2/slides.html#box-and-whisker-display-1",
    "href": "week2/slides.html#box-and-whisker-display-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "box-and-whisker display",
    "text": "box-and-whisker display\n\n\nStarting with a 5-number summary"
  },
  {
    "objectID": "week2/slides.html#identified-end-values",
    "href": "week2/slides.html#identified-end-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Identified end values",
    "text": "Identified end values\n\n\n\nWhy are some individual points singled out?\n\n\nRules for this one may be clearer?"
  },
  {
    "objectID": "week2/slides.html#section-5",
    "href": "week2/slides.html#section-5",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Isn’t this imposing a belief?"
  },
  {
    "objectID": "week2/slides.html#section-6",
    "href": "week2/slides.html#section-6",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "There is no excuse for failing to plot and look\nAnother Tukey wisdom drop"
  },
  {
    "objectID": "week2/slides.html#fences-and-outside-values",
    "href": "week2/slides.html#fences-and-outside-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Fences and outside values",
    "text": "Fences and outside values\n\n\nH-spread: difference between the hinges (we would call this Inter-Quartile Range)\nstep: 1.5 times H-spread\ninner fences: 1 step outside the hinges\nouter fences: 2 steps outside the hinges\nthe value at each end closest to, but still inside the inner fence are “adjacent”\nvalues between an inner fence and its neighbouring outer fence are “outside”\nvalues beyond outer fences are “far out”\nthese rules produce a SCHEMATIC PLOT"
  },
  {
    "objectID": "week2/slides.html#new-statistics-trimeans",
    "href": "week2/slides.html#new-statistics-trimeans",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "New statistics: trimeans",
    "text": "New statistics: trimeans\nThe number that comes closest to\n\\[\\frac{\\text{lower hinge} + 2\\times \\text{median} + \\text{upper hinge}}{4}\\] is the trimean.\n \nThink about trimmed means, where we might drop the highest and lowest 5% of observations."
  },
  {
    "objectID": "week2/slides.html#letter-value-plots-todays-solution",
    "href": "week2/slides.html#letter-value-plots-todays-solution",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Letter value plots: today’s solution",
    "text": "Letter value plots: today’s solution\n\n\nWhy break the data into quarters? Why not eighths, sixteenths? k-number summaries?\nWhat does a 7-number summary look like?\n\nHow would you make an 11-number summary?\n\n\nlibrary(lvplot)\np &lt;- ggplot(mpg, \n            aes(class, hwy))\np + geom_lv(aes(fill=..LV..)) + \n  scale_fill_brewer() + \n  coord_flip() + \n  xlab(\"\")"
  },
  {
    "objectID": "week2/slides.html#box-plots-are-ubiquitous-in-use-today.",
    "href": "week2/slides.html#box-plots-are-ubiquitous-in-use-today.",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Box plots are ubiquitous in use today.",
    "text": "Box plots are ubiquitous in use today.\n - 🐈🐩 Mostly used to compare distributions, multiple subsets of the data.\n\nPuts the emphasis on the middle 50% of observations, although variations can put emphasis on other aspects."
  },
  {
    "objectID": "week2/slides.html#easy-re-expression",
    "href": "week2/slides.html#easy-re-expression",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Easy re-expression",
    "text": "Easy re-expression"
  },
  {
    "objectID": "week2/slides.html#logs-square-roots-reciprocals",
    "href": "week2/slides.html#logs-square-roots-reciprocals",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Logs, square roots, reciprocals",
    "text": "Logs, square roots, reciprocals\n\n\nWhat you need to know about logs?\n\nhow to find good enough logs fast and easily\nthat equal differences in logs correspond to equal ratios of raw values.\n\n(This means that wherever you find people using products or ratios– even in such things as price indexes–using logs–thus converting producers to sums and ratios to differences–is likely to help.)\n\n\nThe most common transformations are logs, sqrt root, reciprocals, reciprocals of square roots\n\n-1, -1/2, +1/2, +1\n\nWhat happened to ZERO?\n\n\nIt turns out that the role of a zero power, is for the purposes of re-expression, neatly solved by the logarithm."
  },
  {
    "objectID": "week2/slides.html#re-express-to-symmetrize-the-distribution",
    "href": "week2/slides.html#re-express-to-symmetrize-the-distribution",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Re-express to symmetrize the distribution",
    "text": "Re-express to symmetrize the distribution"
  },
  {
    "objectID": "week2/slides.html#power-ladder",
    "href": "week2/slides.html#power-ladder",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Power ladder",
    "text": "Power ladder\n \n⬅️ fix RIGHT-skewed values  \n-2, -1, -1/2, 0 (log), 1/3, 1/2, 1, 2, 3, 4\n\nfix LEFT-skewed values ➡️"
  },
  {
    "objectID": "week2/slides.html#section-7",
    "href": "week2/slides.html#section-7",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "We now regard re-expression as a tool, something to let us do a better job of grasping. The grasping is done with the eye and the better job is through a more symmetric appearance.\nAnother Tukey wisdom drop"
  },
  {
    "objectID": "week2/slides.html#linearising-bivariate-relationships",
    "href": "week2/slides.html#linearising-bivariate-relationships",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linearising bivariate relationships",
    "text": "Linearising bivariate relationships\n  \n\nSurprising observation: The small fluctuations in later years.\nWhat might be possible reasons?"
  },
  {
    "objectID": "week2/slides.html#linearising-bivariate-relationships-1",
    "href": "week2/slides.html#linearising-bivariate-relationships-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linearising bivariate relationships",
    "text": "Linearising bivariate relationships\n  \n\nSee some fluctuations in the early years, too. Note that the log transformation couldn’t linearise."
  },
  {
    "objectID": "week2/slides.html#rules-and-advice",
    "href": "week2/slides.html#rules-and-advice",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Rules and advice",
    "text": "Rules and advice\n\n\n\nGraphics are friendly.\nArithmetic often exists to make graphs possible.\nGraphs force us to notice the unexpected; nothing could be more important.\nDifferent graphs show us quite different aspects of the same data.\nThere is no more reason to expect one graph to “tell all” than to expect one number to do the same.\n“Plotting \\(y\\) against \\(x\\)” involves significant choices–how we express one or both variables can be crucial.\n\n\n\n\n\nThe first step in penetrating plotting is to straighten out the dependence or point scatter as much as reasonable.\nPlotting \\(y^2\\), \\(\\sqrt{y}\\), \\(log(y)\\), \\(-1/y\\) or the like instead of \\(y\\) is one plausible step to take in search of straightness.\nPlotting \\(x^2\\), \\(\\sqrt{x}\\), \\(log(x)\\), \\(-1/x\\) or the like instead of \\(x\\) is another.\nOnce the plot is straightened, we can usually gain much by flattening it, usually by plotting residuals.\nWhen plotting scatters, we may need to be careful about how we express \\(x\\) and \\(y\\) in order to avoid concealment by crowding."
  },
  {
    "objectID": "week2/slides.html#section-8",
    "href": "week2/slides.html#section-8",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The book is a digest of 🌟 tricks and treats 🌟 of massaging numbers and drafting displays.\nMany of the tools have made it into today’s analyses in various ways. Many have not.\nNotice the word developments too: froots, fences. Tukey brought you the word “software”\nThe temperament of the book is an inspiration for the mind-set for this unit. There is such delight in working with numbers!\n“We love data!”"
  },
  {
    "objectID": "week2/slides.html#take-aways",
    "href": "week2/slides.html#take-aways",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Take-aways",
    "text": "Take-aways\n\nTukey’s approach was a reaction to many years of formalising data analysis using statistical hypothesis testing.\nMethodology development in statistical testing was a reaction to the ad-hoc nature of data analysis.\nComplex machine learning models like neural networks are in reaction to the inability of statistical models to capture highly non-linear relationships, and depend heavily on the data provided.\nExploring data today is in reaction to the need to explain complex models, to support organisations against legal challenges to decisions made from the model\nIt is much easier to accomplish computers.\n“Exploratory data analysis” as commonly used today term is unfortunately synonymous with “descriptive statistics”, but it is truly much more. Understanding its history from Tukey’s advocation helps you see it is the tooling to discover what you don’t know."
  },
  {
    "objectID": "week2/slides.html#resources",
    "href": "week2/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nwikipedia\nJohn W. Tukey (1977) Exploratory data analysis\nData coding using tidyverse suite of R packages\nSketching canvases made using fabricerin"
  },
  {
    "objectID": "week2/tutorialsol.html",
    "href": "week2/tutorialsol.html",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week2/tutorialsol.html#objectives",
    "href": "week2/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week2/tutorialsol.html#preparation",
    "href": "week2/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 2",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is EDA Case Study: Bay area blues. It is authored by Hadley Wickham, Deborah F. Swayne, and David Poole. It appeared in the book “Beautiful Data” edited by Jeff Hammerbacher and Toby Segaran. Not all the chapters in the book are good examples of data analysis, though.\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"forcats\", \"patchwork\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week2/tutorialsol.html#exercises",
    "href": "week2/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 2",
    "section": "📥 Exercises",
    "text": "📥 Exercises\nPoint your web browser to the github site for the analysis, https://github.com/hadley/sfhousing. The main data file is house-sales.csv. Read this data into your R session. (🛑 ARE YOU USING A PROJECT FOR THIS UNIT? IF NOT, STOP and OPEN IT NOW.)\nYou can read the data in directly from the web site using this code:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(forcats)\nsales &lt;- read_csv(\"https://raw.githubusercontent.com/hadley/sfhousing/master/house-sales.csv\")\n\n\n1. What’s in the data?\n\nIs the data in tidy form?\nOf the variables in the data, which are\n\nnumeric?\ncategorical?\ntemporal?\n\nWhat would be an appropriate plot to make to examine the\n\nnumeric variables?\ncategorical variables?\na categorical and numeric variable?\na temporal variable and a numeric variable?\n\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nYes\n\nprice, br, lsqft, bsqft\ncounty, city, zip, street\nyear, date, datesold\n\n\nscatterplots\nbar charts, pie charts, mosaic\nfacet by the categorical variable. could be boxplots, or density plots, or facetted scatterplots to look at multiple numeric variables\n\ntime series plot, connect lines to indicate time, maybe need to aggregate over time to get one value per time point\n\n\n\n\n\n\n\n\n2. Time series plots\nReproduce the time series plots of weekly average price and volume of sales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nNote, stacking time series plots helps compare the series relative to the time point.\n\nsales_weekly &lt;- sales |&gt;\n  group_by(date) |&gt;\n  summarise(av_price = mean(price, na.rm=TRUE),\n            volume = n())\np1 &lt;- ggplot(sales_weekly, aes(x=date,\n                               y=av_price)) +\n  geom_line() +\n  scale_y_continuous(\"Average price (millions)\", \n              breaks = seq(500000, 800000, 50000), \n              labels = c(\"0.50\", \"0.55\", \"0.60\", \"0.65\",\n                         \"0.70\", \"0.75\", \"0.80\")) +\n  scale_x_date(\"\", date_breaks = \"1 years\", \n               minor_breaks = NULL, \n               date_labels = \"%Y\") +\n  theme(aspect.ratio = 0.5)\np2 &lt;- ggplot(sales_weekly, aes(x=date, y=volume)) + geom_line() +\n  scale_y_continuous(\"Number of sales\", \n              breaks = seq(500,3000,500), \n              labels = c(\"500\", \"1,000\", \"1,500\", \"2,000\",\n                         \"2,500\", \"3,000\")) +\n  scale_x_date(\"\", date_breaks = \"1 years\", \n               minor_breaks = NULL, \n               date_labels = \"%Y\") +\n  theme(aspect.ratio = 0.5)\np1 + p2 + plot_layout(ncol=1)\n\n\n\n\n\n\n\n3. Correlation between series\nIt looks like volume goes down as price goes up. There is a better plot to make to examine this. What is it? Make the plot. After making the plot, report what you learn about the apparent correlation.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nggplot(sales_weekly, aes(x=av_price, y=volume)) +\n  geom_point() +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\nAny correlation is very weak, and negative.\n\n\n\n\n\n\n4. Geographic differences\nThink about potential plots you might make for examining differences by geographic region (as measured by zip, county or city). Make a plot, and report what you learn.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nggplot(sales, \n       aes(x = fct_reorder(county, \n                  price, na.rm=TRUE), \n           y = price)) +\n         geom_boxplot() + \n  scale_y_log10() +\n  xlab(\"\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nMarin County has the highest prices on average, and San Joaquin the lowest. The lowest priced house was sold in Sonoma County. The highest priced properties and lowest priced are pretty similar from one county to another - that is, the variability within county is large.\n\n\n\n\n\n\n5. The Rich Get Richer and the Poor Get Poorer\nIn the section “The Rich Get Richer and the Poor Get Poorer” there are some interesting transformations of the data, and unusual types of plots. Explain why looking at proportional change in value refines the view of price movement in different higher vs lower priced properties.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe transformation makes changes relative to the initial average price at the start of the time period. All curves produced will start from the same point. This means that we only need to compare the end points of each line, saving us from calculating differences between lines relative to the difference at the beginning.\n\n\n\n\n\n\n6. Anything surprising?\nWere there any findings that surprised the authors? Or would surprise you?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nI found it interesting that Mountain View had no decline in housing prices. This city has the headquarters of many of the world’s largest technology companies are in the city, including Google, Mozilla Foundation, Symantec, and Intuit.\n\n\n\n\n\n\n7. Additional resources\nSome of the findings were compared against information gathered from external sources. Can you point to an example of this, and how the other information was used to support or question the finding?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nAll of this is consistent with what we have learned about subprime mortgages since the housing bust hit the headlines.\nSubprime mortgages were offered on little collateral which meant they were quite risky, and they tended to be on the lower end of the housing market. This information was in all the news headlines at the time, and the analysis that these authors have done was checked against the common reporting at the time. The data was consistent with these reports."
  },
  {
    "objectID": "week2/tutorialsol.html#finishing-up",
    "href": "week2/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 2",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week2/tutorial.html",
    "href": "week2/tutorial.html",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week2/tutorial.html#objectives",
    "href": "week2/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week2/tutorial.html#preparation",
    "href": "week2/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 2",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is EDA Case Study: Bay area blues. It is authored by Hadley Wickham, Deborah F. Swayne, and David Poole. It appeared in the book “Beautiful Data” edited by Jeff Hammerbacher and Toby Segaran. Not all the chapters in the book are good examples of data analysis, though.\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"forcats\", \"patchwork\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week2/tutorial.html#exercises",
    "href": "week2/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 2",
    "section": "📥 Exercises",
    "text": "📥 Exercises\nPoint your web browser to the github site for the analysis, https://github.com/hadley/sfhousing. The main data file is house-sales.csv. Read this data into your R session. (🛑 ARE YOU USING A PROJECT FOR THIS UNIT? IF NOT, STOP and OPEN IT NOW.)\nYou can read the data in directly from the web site using this code:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(forcats)\nsales &lt;- read_csv(\"https://raw.githubusercontent.com/hadley/sfhousing/master/house-sales.csv\")\n\n\n1. What’s in the data?\n\nIs the data in tidy form?\nOf the variables in the data, which are\n\nnumeric?\ncategorical?\ntemporal?\n\nWhat would be an appropriate plot to make to examine the\n\nnumeric variables?\ncategorical variables?\na categorical and numeric variable?\na temporal variable and a numeric variable?\n\n\n\n\n2. Time series plots\nReproduce the time series plots of weekly average price and volume of sales.\n\n\n\n\n\n\n\n\n\n\n\n3. Correlation between series\nIt looks like volume goes down as price goes up. There is a better plot to make to examine this. What is it? Make the plot. After making the plot, report what you learn about the apparent correlation.\n\n\n4. Geographic differences\nThink about potential plots you might make for examining differences by geographic region (as measured by zip, county or city). Make a plot, and report what you learn.\n\n\n5. The Rich Get Richer and the Poor Get Poorer\nIn the section “The Rich Get Richer and the Poor Get Poorer” there are some interesting transformations of the data, and unusual types of plots. Explain why looking at proportional change in value refines the view of price movement in different higher vs lower priced properties.\n\n\n6. Anything surprising?\nWere there any findings that surprised the authors? Or would surprise you?\n\n\n7. Additional resources\nSome of the findings were compared against information gathered from external sources. Can you point to an example of this, and how the other information was used to support or question the finding?"
  },
  {
    "objectID": "week2/tutorial.html#finishing-up",
    "href": "week2/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 2",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/index.html",
    "href": "week3/index.html",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "",
    "text": "The initial examination of data"
  },
  {
    "objectID": "week3/index.html#main-reference",
    "href": "week3/index.html#main-reference",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "",
    "text": "The initial examination of data"
  },
  {
    "objectID": "week3/index.html#what-you-will-learn-this-week",
    "href": "week3/index.html#what-you-will-learn-this-week",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nRole of IDA\nTechniques for\n\ndata screening\ndata cleaning\nimputation\nvalidation\n\nChecking assumptions for hypothesis testing and fitting linear models"
  },
  {
    "objectID": "week3/index.html#lecture-slides",
    "href": "week3/index.html#lecture-slides",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week3/index.html#worksheet",
    "href": "week3/index.html#worksheet",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Worksheet",
    "text": "Worksheet\n\nqmd\nhtml"
  },
  {
    "objectID": "week3/index.html#tutorial-instructions",
    "href": "week3/index.html#tutorial-instructions",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week3/index.html#assignments",
    "href": "week3/index.html#assignments",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Assignments",
    "text": "Assignments\n\nExercises 1 is due on Monday 18 August.\nQuiz 2 is due on Thursday 14 August.\nQuiz 3 is due on Thursday 21 August."
  },
  {
    "objectID": "week3/tutorialsol.html",
    "href": "week3/tutorialsol.html",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week3/tutorialsol.html#objectives",
    "href": "week3/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week3/tutorialsol.html#preparation",
    "href": "week3/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 3",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is The initial examination of data. It is authored by Chris Chatfield, and is a classic paper explaining the role of initial data analysis.\n\nComplete the weekly quiz, before the deadline!\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week3/tutorialsol.html#exercises",
    "href": "week3/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 3",
    "section": "📥 Exercises",
    "text": "📥 Exercises\nToday, you have sprint competition to discover as many problems as possible in this data, cafe.rda.\nA small cafe in the city of Melbourne is interested in determining whether the daily earnings depend on the weather. They compiled data for a period over 2000-2001 to study this question. The data has the following variables:\n\n\n\n \n  \n    var \n    description \n  \n \n\n  \n    dt \n    Date \n  \n  \n    wday \n    Day of the week \n  \n  \n    revenue \n    Daily revenue in hundreds, 11=1100 \n  \n  \n    expend \n    Daily expenses in hundreds \n  \n  \n    precip \n    Precipitation in mm \n  \n  \n    mint \n    Minimum temperature, Celsius \n  \n  \n    maxt \n    Maximum temperature, Celsius \n  \n  \n    source \n    Source of the weather data \n  \n\n\n\n\n\nGuidelines\n\nUse whatever R package or software or tool you’d like, and report how you found the error.\nFeel free to buddy up, and work with another student in your tutorial session.\nNo cheating! This time do the assignment without AI help, or searching for answers on the web.\nPlease don’t ruin the later tutorials by sharing your work!\n\nThe most complete list with best process for findings, as decided by your tutor, wins the prize! Your tutor’s decision is final!\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nYour tutor has the solution!"
  },
  {
    "objectID": "week3/tutorialsol.html#finishing-up",
    "href": "week3/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 3",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/tutorial.html",
    "href": "week3/tutorial.html",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week3/tutorial.html#objectives",
    "href": "week3/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week3/tutorial.html#preparation",
    "href": "week3/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 3",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is The initial examination of data. It is authored by Chris Chatfield, and is a classic paper explaining the role of initial data analysis.\n\nComplete the weekly quiz, before the deadline!\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week3/tutorial.html#exercises",
    "href": "week3/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 3",
    "section": "📥 Exercises",
    "text": "📥 Exercises\nToday, you have sprint competition to discover as many problems as possible in this data, cafe.rda.\nA small cafe in the city of Melbourne is interested in determining whether the daily earnings depend on the weather. They compiled data for a period over 2000-2001 to study this question. The data has the following variables:\n\n\n\n \n  \n    var \n    description \n  \n \n\n  \n    dt \n    Date \n  \n  \n    wday \n    Day of the week \n  \n  \n    revenue \n    Daily revenue in hundreds, 11=1100 \n  \n  \n    expend \n    Daily expenses in hundreds \n  \n  \n    precip \n    Precipitation in mm \n  \n  \n    mint \n    Minimum temperature, Celsius \n  \n  \n    maxt \n    Maximum temperature, Celsius \n  \n  \n    source \n    Source of the weather data \n  \n\n\n\n\n\nGuidelines\n\nUse whatever R package or software or tool you’d like, and report how you found the error.\nFeel free to buddy up, and work with another student in your tutorial session.\nNo cheating! This time do the assignment without AI help, or searching for answers on the web.\nPlease don’t ruin the later tutorials by sharing your work!\n\nThe most complete list with best process for findings, as decided by your tutor, wins the prize! Your tutor’s decision is final!"
  },
  {
    "objectID": "week3/tutorial.html#finishing-up",
    "href": "week3/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 3",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week4/index.html",
    "href": "week4/index.html",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "",
    "text": "Wickham et al. (2010) Graphical inference for Infovis"
  },
  {
    "objectID": "week4/index.html#main-reference",
    "href": "week4/index.html#main-reference",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "",
    "text": "Wickham et al. (2010) Graphical inference for Infovis"
  },
  {
    "objectID": "week4/index.html#what-you-will-learn-this-week",
    "href": "week4/index.html#what-you-will-learn-this-week",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nrevision of statistical inference\nusing re-sampling methods to calibrate reading patterns\ngenerating lineups of plots\nhow to specify the null hypothesis\ncalculating p-value and power"
  },
  {
    "objectID": "week4/index.html#lecture-slides",
    "href": "week4/index.html#lecture-slides",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week4/index.html#worksheet",
    "href": "week4/index.html#worksheet",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Worksheet",
    "text": "Worksheet\n\nqmd\nhtml"
  },
  {
    "objectID": "week4/index.html#tutorial-instructions",
    "href": "week4/index.html#tutorial-instructions",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week4/index.html#assignments",
    "href": "week4/index.html#assignments",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Assignments",
    "text": "Assignments\n\nExercises 1 is due on Monday 18 August.\nQuiz 3 is due on Thursday 21 August.\nQuiz 4 is due on Thursday 28 August."
  },
  {
    "objectID": "week4/tutorialsol.html",
    "href": "week4/tutorialsol.html",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorialsol.html#objectives",
    "href": "week4/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorialsol.html#preparation",
    "href": "week4/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 4",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is Wickham et al. (2010) Graphical inference for Infovis.\n- Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\ninstall.packages(c(\"tidyverse\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week4/tutorialsol.html#exercises",
    "href": "week4/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 4",
    "section": "📥 Exercises",
    "text": "📥 Exercises\nThis tutorial focuses on IDA for the gardenR data, with the goal to answer this question:\nWhich variety of tomato produces the most return on investment, as measured by weight?\n\nExercise 1\n\nHow many types of vegetables were grown in each year?\nHow many vegetables were grown in 2020 that were not grown in 2021?\nWhat are some of the data recording errors that can be seen by comparing vegetables grown in each year?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nlibrary(gardenR)\nlibrary(tidyverse)\nlibrary(ggbeeswarm)\n\ndata(\"garden_coords\")\ndata(\"garden_harvest\")\ndata(\"garden_planting\")\ndata(\"garden_spending\")\ndata(\"harvest_2021\")\ndata(\"planting_2021\")\ndata(\"spending_2021\")\n\n31 grown in 2020, and 33 grown in 2021.\n\nTo work out which differ between years, use the anti_join() function. Because there are multiple plots of vegetables use distinct() to remove duplicates, and then arrange alphabetically for ease of comparison.\n\n\nin20butnot21 &lt;- anti_join(select(garden_harvest, vegetable), select(harvest_2021, vegetable), by=\"vegetable\") |&gt; \n  distinct() |&gt; \n  arrange(vegetable)\nin21butnot20 &lt;- anti_join(select(harvest_2021, vegetable), select(garden_harvest, vegetable), by=\"vegetable\") |&gt; \n  distinct() |&gt; \n  arrange(vegetable)\nin20butnot21$vegetable\n\n[1] \"Swiss chard\" \"apple\"       \"broccoli\"    \"chives\"      \"hot peppers\"\n[6] \"jalapeño\"    \"kohlrabi\"    \"onions\"      \"pumpkins\"   \n\nin21butnot20$vegetable\n\n [1] \"apples\"       \"cabbage\"      \"dill\"         \"garlic\"       \"mint\"        \n [6] \"oregano\"      \"pumpkin\"      \"sweet potato\" \"swiss chard\"  \"tomatillos\"  \n[11] \"watermelon\"  \n\n\n\n\n\n\nswiss chard was grown in each year but capital “S” was used in 2020.\napples were grown in each year, but singular name was used in 2021.\nsimilarly for pumpkins but in reverse, singular used in 2020!\n\n\nharvest_2020 &lt;- garden_harvest |&gt;\n  mutate(vegetable = tolower(vegetable),\n         variety = tolower(variety)) |&gt;\n  mutate(vegetable = case_match(\n    vegetable,\n    \"apple\" ~ \"apples\",\n    .default = vegetable)\n  )\nharvest_2021 &lt;- harvest_2021 |&gt;\n  mutate(vegetable = case_match(\n    vegetable,\n    \"pumpkin\" ~ \"pumpkins\",\n    .default = vegetable)\n  )\n\nSo final answer for (b) is here are the vegetables not grown both years:\n\n\n[1] \"Swiss chard\" \"apple\"       \"broccoli\"    \"chives\"      \"hot peppers\"\n[6] \"jalapeño\"    \"kohlrabi\"    \"onions\"     \n\n\n [1] \"apples\"       \"cabbage\"      \"dill\"         \"garlic\"       \"mint\"        \n [6] \"oregano\"      \"sweet potato\" \"swiss chard\"  \"tomatillos\"   \"watermelon\"  \n\n\n\n\n\n\n\n\nExercise 2\n\nJoin the harvest, spending and planting data for the two years, after adding a new variable each, called year. Show your code.\nMake a subset containing just the tomatoes, for each set.\nAre the varieties of tomatoes grown each year the same?\nAre the tomato varieties grown in the same plots each year?\nWhen are tomatoes planted and harvested, in Lisa’s garden?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\n\n\nharvest_2020 &lt;- garden_harvest |&gt;\n  mutate(year = \"2020\")\nharvest_2021 &lt;- harvest_2021 |&gt;\n  mutate(year = \"2021\")\nharvest &lt;- bind_rows(harvest_2020, harvest_2021)\n\nspending_2020 &lt;- garden_spending |&gt;\n  mutate(year = \"2020\")\nspending_2021 &lt;- spending_2021 |&gt;\n  mutate(year = \"2021\")\nspending &lt;- bind_rows(spending_2020, spending_2021)\n\nplanting_2020 &lt;- garden_planting |&gt;\n  mutate(year = \"2020\")\nplanting_2021 &lt;- planting_2021 |&gt;\n  mutate(year = \"2021\")\nplanting &lt;- bind_rows(planting_2020, planting_2021)\n\n\n\n\n\nharvest_toms &lt;- harvest |&gt; filter(vegetable == \"tomatoes\")\nspending_toms &lt;- spending |&gt; filter(vegetable == \"tomatoes\")\nplanting_toms &lt;- planting |&gt; filter(vegetable == \"tomatoes\")\n\n\n\n\n\ntom_smry &lt;- harvest_toms |&gt; \n  count(variety, year) |&gt; \n  pivot_wider(names_from = year, values_from = n)\ntom_smry\n\n# A tibble: 19 × 3\n   variety          `2020` `2021`\n   &lt;chr&gt;             &lt;int&gt;  &lt;int&gt;\n 1 Amish Paste          30     19\n 2 Better Boy           23     NA\n 3 Big Beef             21     24\n 4 Black Krim           12     12\n 5 Bonny Best           27     15\n 6 Brandywine           16     NA\n 7 Bush Goliath         NA      6\n 8 Cherokee Purple      14     10\n 9 Early Girl           NA     29\n10 Jet Star             13     NA\n11 Mortgage Lifter      18     22\n12 Old German           19      4\n13 San Marzano          NA     19\n14 Striped German       NA      8\n15 Sweet 100 Cherry     NA     28\n16 grape                39     NA\n17 volunteer            NA     16\n18 volunteers           31     NA\n19 yellow               NA      4\n\nn_complete &lt;- tom_smry |&gt; mutate(m=`2020`+`2021`) |&gt; filter(!is.na(m)) |&gt; nrow()\n\nThere are a lot of varieties of tomatoes grown, only 7 are grown in both years.\n\nUse only the varieties grown in both years.\n\n\n\nCode\ntom_both_yrs &lt;- tom_smry |&gt; mutate(m=`2020`+`2021`) |&gt; filter(!is.na(m)) \nplanting_toms |&gt; \n  filter(variety %in% tom_both_yrs$variety) |&gt;\n  count(variety, plot, year) |&gt;\n  pivot_wider(names_from = plot, values_from = n)\n\n\n# A tibble: 14 × 6\n   variety         year      D     J     N     O\n   &lt;chr&gt;           &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 Amish Paste     2021      1    NA     1    NA\n 2 Amish Paste     2020     NA     1     1    NA\n 3 Big Beef        2021      1    NA     1    NA\n 4 Big Beef        2020     NA    NA     1    NA\n 5 Black Krim      2020     NA    NA     1    NA\n 6 Black Krim      2021     NA    NA     1    NA\n 7 Bonny Best      2020     NA     1    NA    NA\n 8 Bonny Best      2021     NA    NA    NA     1\n 9 Cherokee Purple 2020     NA     1    NA    NA\n10 Cherokee Purple 2021     NA    NA     1     1\n11 Mortgage Lifter 2021      1    NA     1    NA\n12 Mortgage Lifter 2020     NA     1     1    NA\n13 Old German      2021      1    NA    NA    NA\n14 Old German      2020     NA     1    NA    NA\n\n\nNot a single variety is grown in the same plot each year. This might cause problems, if the plots are not equally good for growing tomatoes.\n\nAgain, just use the tomatoes that are grown in each year.\n\n\n\nCode\nplanting_toms_sub &lt;- planting_toms |&gt;\n  filter(variety %in% tom_both_yrs$variety) |&gt;\n  mutate(type = \"planting\") |&gt;\n  select(variety, date, type, year)\nharvest_toms_sub &lt;- harvest_toms |&gt;\n  filter(variety %in% tom_both_yrs$variety) |&gt;\n  mutate(type = \"harvest\") |&gt;\n  select(variety, date, type, year)\ntom_variety_order &lt;- harvest_toms_sub |&gt;\n  group_by(variety) |&gt;\n  summarise(date = min(date)) |&gt;\n  arrange(date)\np_h_tom &lt;- bind_rows(planting_toms_sub, harvest_toms_sub) |&gt;\n  mutate(day_yr = yday(date)) |&gt;\n  mutate(variety = factor(variety, tom_variety_order$variety))\nggplot(p_h_tom) +\n  geom_point(aes(x=day_yr, y=year, colour=type)) +\n  facet_wrap(~variety, ncol=4, scales=\"free_x\") +\n  scale_colour_brewer(\"\", palette=\"Dark2\") +\n  xlab(\"day of year\") + ylab(\"\") +\n  theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\n\nPlanting is usually in late May, and it is consistent for all the varieties.\nHarvesting starts around 50 days after planting. 2020 had an earlier harvest than 2021 for all varieties. Big Beef tends to be harvested first, and Black Kim later. Old German had a poor harvest in 2021 relative to 2022.\n\n\n\n\n\n\nExercise 3 Try to answer the original question.\n\nHow should you calibrate weight of harvest by amount of seeds planted?\nWhich variety produces the most return on investment?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nWe’ll divide the weight by number of seeds planted. An interesting observation is that so few tomato seeds were planted! Packets of tomato seeds have lots of seeds.\n\n\n\nCode\nplanting_smry &lt;- planting_toms |&gt;\n  filter(variety %in% tom_both_yrs$variety) |&gt;\n  select(variety, date, year, number_seeds_planted, number_seeds_exact) |&gt;\n  group_by(variety, year) |&gt;\n  summarise(nseeds = sum(number_seeds_planted))\nharvest_smry &lt;- harvest_toms |&gt;\n  filter(variety %in% tom_both_yrs$variety) |&gt;\n  group_by(variety, year) |&gt;\n  summarise(weight = sum(weight))\ntom_weight &lt;- left_join(planting_smry, harvest_smry) |&gt;\n  mutate(wgt_ps = weight/nseeds)\n\nggplot(tom_weight, aes(\n  x = fct_reorder(variety, wgt_ps),\n  y = wgt_ps,\n  colour = year)) +\n  geom_point() + \n  coord_flip() +\n  xlab(\"\") + ylab(\"weight per seed\") +\n  scale_colour_brewer(\"\", palette=\"Dark2\")\n\n\n\n\n\n\n\n\n\n\n\n\nBig Beef is a consistently high performer, over these two years. Old German is the most varied, top by weight in 2020 but failed in 2021."
  },
  {
    "objectID": "week4/tutorialsol.html#finishing-up",
    "href": "week4/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 4",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week4/tutorial.html",
    "href": "week4/tutorial.html",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorial.html#objectives",
    "href": "week4/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorial.html#preparation",
    "href": "week4/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 4",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is Wickham et al. (2010) Graphical inference for Infovis.\n- Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\ninstall.packages(c(\"tidyverse\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week4/tutorial.html#exercises",
    "href": "week4/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 4",
    "section": "📥 Exercises",
    "text": "📥 Exercises\nThis tutorial focuses on IDA for the gardenR data, with the goal to answer this question:\nWhich variety of tomato produces the most return on investment, as measured by weight?\n\nExercise 1\n\nHow many types of vegetables were grown in each year?\nHow many vegetables were grown in 2020 that were not grown in 2021?\nWhat are some of the data recording errors that can be seen by comparing vegetables grown in each year?\n\n\n\nExercise 2\n\nJoin the harvest, spending and planting data for the two years, after adding a new variable each, called year. Show your code.\nMake a subset containing just the tomatoes, for each set.\nAre the varieties of tomatoes grown each year the same?\nAre the tomato varieties grown in the same plots each year?\nWhen are tomatoes planted and harvested, in Lisa’s garden?\n\n\n\nExercise 3 Try to answer the original question.\n\nHow should you calibrate weight of harvest by amount of seeds planted?\nWhich variety produces the most return on investment?"
  },
  {
    "objectID": "week4/tutorial.html#finishing-up",
    "href": "week4/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 4",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week5/index.html",
    "href": "week5/index.html",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "",
    "text": "Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions"
  },
  {
    "objectID": "week5/index.html#main-reference",
    "href": "week5/index.html#main-reference",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "",
    "text": "Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions"
  },
  {
    "objectID": "week5/index.html#what-you-will-learn-this-week",
    "href": "week5/index.html#what-you-will-learn-this-week",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nNumeric and visual summaries for a single variable\nCommon features to discover\nTools for inference for a single variable\nImputing missings on a single variable"
  },
  {
    "objectID": "week5/index.html#lecture-slides",
    "href": "week5/index.html#lecture-slides",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week5/index.html#worksheet",
    "href": "week5/index.html#worksheet",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Worksheet",
    "text": "Worksheet\n\nqmd\nhtml"
  },
  {
    "objectID": "week5/index.html#tutorial-instructions",
    "href": "week5/index.html#tutorial-instructions",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week5/index.html#assignments",
    "href": "week5/index.html#assignments",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Assignments",
    "text": "Assignments\n\nExercises 2 is due on Monday 01 September.\nQuiz 4 is due on Thursday 28 August.\nQuiz 5 is due on Thursday 04 September."
  },
  {
    "objectID": "week5/tutorialsol.html",
    "href": "week5/tutorialsol.html",
    "title": "ETC5521 Tutorial 5",
    "section": "",
    "text": "These are exercises in making plots of one variable and what can be learned about the distributions, data patterns and apply randomisation methods to check what we see."
  },
  {
    "objectID": "week5/tutorialsol.html#objectives",
    "href": "week5/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 5",
    "section": "",
    "text": "These are exercises in making plots of one variable and what can be learned about the distributions, data patterns and apply randomisation methods to check what we see."
  },
  {
    "objectID": "week5/tutorialsol.html#preparation",
    "href": "week5/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 5",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\ninstall.packages(c(\"ggplot2movies\", \"bayesm\",  \"ggbeeswarm\", \"patchwork\", \"nullabor\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week5/tutorialsol.html#exercises",
    "href": "week5/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 5",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: What are the common lengths of movies?\nLoad the movies dataset in the ggplot2movies package and answer the following questions based on it.\n\nHow many observations are in the data?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThere are 58788 observations.\n\n\n\n\n\nDraw a histogram with an appropriate binwidth that shows the peaks at 7 minutes and 90 minutes. Draw another set of histograms to show whether these peaks existed both before and after 1980.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\ndata(movies)\nmovies |&gt;\n  mutate(\n    after1980 = ifelse(year &gt; 1980,\n      \"After 1980\",\n      \"1980 or before\"\n    ),\n    copy = FALSE\n  ) |&gt;\n  bind_rows(mutate(movies, copy = TRUE, after1980 = \"All\")) |&gt;\n  ggplot(aes(length)) +\n  geom_histogram(binwidth = 1, fill = \"yellow\", color = \"black\") +\n  scale_x_continuous(\n    breaks = c(0, 7, 30, 60, 90, 120, 150, 180),\n    limits = c(0, 180)\n  ) +\n  facet_grid(after1980 ~ .)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe variable Short indicates whether the film was classified as a short film (1) or not (0). Draw plots to investigate what rules was used to define a film as “short” and whether the films have been consistently classified.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nmovies |&gt;\n  group_by(Short) |&gt;\n  summarise(x = list(summary(length))) |&gt;\n  unnest_wider(x)\n\n# A tibble: 2 × 7\n  Short Min.        `1st Qu.`   Median      Mean        `3rd Qu.`   Max.       \n  &lt;int&gt; &lt;table[1d]&gt; &lt;table[1d]&gt; &lt;table[1d]&gt; &lt;table[1d]&gt; &lt;table[1d]&gt; &lt;table[1d]&gt;\n1     0 1           85          93          95          103         5220       \n2     1 1            7          10          14           19          240       \n\n\nThe maximum length for a film classified as short is 240 minutes.\n\nmovies |&gt;\n  mutate(short = factor(Short, labels = c(\"Long\", \"Short\"))) |&gt;\n  ggplot(aes(y=length, x=short)) +\n  geom_quasirandom() +\n  scale_y_log10(\n    limits = c(1, 240),\n    breaks = c(1, 7, 10, 15, 20, 30, 45, 50, 70, 90, 110, 240)\n  ) +\n  labs(y = \"\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nFrom the graph, majority of films classified as short are under 50 minutes while those classified as long tend to be longer than 50 minutes. There are clear cases of mislabelling, e.g. a one-minute long film classified as “not short”.\nOn further detective work, the original source of the data says “Any theatrical film or made-for-video title with a running time of less than 45 minutes, i.e., 44 minutes or less, or any TV series or TV movie with a running time of less than 22 minutes, i.e. 21 minutes or less. (A”half-hour” television program should not be listed as a Short.)” Given this is an objective measure based on the length of the film, we can see that that any films that are 45 minutes or longer should be classified as long and less than that as short.\n\n\n\n\n\nHow would you use the lineup protocol to determine if the periodic peaks could happen by chance? What would be the null hypothesis? Make your lineup. Does the data plot stand out? Compute the \\(p\\)-value, if 5 out of 12 people picked the data plot as the most different one in the lineup. Comment on the results. (Note: It might be most useful to assess this only for movies between 50-150 minutes long.)\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nmovies |&gt; \n  filter(between(length, 50, 150)) %&gt;%\n  select(length) %&gt;%\n  lineup(null_dist(\"length\", \"norm\"), true=., n=9) %&gt;%\n  ggplot(aes(x=length)) +\n    geom_histogram(binwidth = 1) +\n    scale_x_continuous(\n      breaks = c(0, 7, 30, 60, 90, 120, 150, 180),\n      limits = c(0, 180)\n    ) +\n  facet_wrap(~.sample, ncol=3, scales=\"free\") +\n  theme(axis.text = element_blank(),\n    axis.title = element_blank(),\n    panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\nSimulate samples from a normal distribution using the sample mean and standard deviation as the parameters.\n\\(H_0\\): Periodic peaks are not present by chance.\nI would expect the \\(p\\)-value to be 0 as the data plot is very clearly different from the nulls. This suggests that the multimodality is not possible to observe in samples from a normal distribution, and that the pattern is present because the movie lengths are commonly cut to specific numbers of minutes.\n\n\n\n\n\n\nExercise 2: What is the market for different brands of whisky?\nThe Scotch data set in bayesm package was collated from a survey on scotch drinkers, recording the brand they consumed. Take a quick look at the data, and rearrange it to look like:\n\n\n# A tibble: 21 × 2\n   brand                      count\n   &lt;chr&gt;                      &lt;int&gt;\n 1 Chivas Regal                 806\n 2 Dewar's White Label          517\n 3 Johnnie Walker Black Label   502\n 4 J&B                          458\n 5 Johnnie Walker Red Label     424\n 6 Other Brands                 414\n 7 Glenlivet                    354\n 8 Cutty Sark                   339\n 9 Glenfiddich                  334\n10 Pinch (Haig)                 117\n11 Clan MacGregor               103\n12 Ballantine                    99\n13 Macallan                      95\n14 Passport                      82\n15 Black & White                 81\n16 Scoresby Rare                 79\n17 Grant's                       74\n18 Ushers                        67\n19 White Horse                   62\n20 Knockando                     47\n21 Singleton                     31\n\n\n\nProduce a barplot of the number of respondents per brand. What ordering of the brands do you think is the best? What is interesting about the distribution of counts?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nscotch_consumption |&gt;\n  mutate(\n    brand = fct_reorder(brand, count)) |&gt;\n  ggplot(aes(x=count, y=brand)) +\n    geom_col() +\n    ylab(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are 20 named brands and one category that is labelled as Other Brands. Produce a barplot that reduces the number of categories by selecting a criteria to lump certain brands to the Other Brands category.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nscotch_consumption |&gt;\n  mutate(\n    brand = ifelse(count &gt; 200, brand, \"Other Brands\"),\n    brand = fct_reorder(brand, count),\n    brand = fct_relevel(brand, \"Other Brands\")\n  ) |&gt;\n  ggplot(aes(count, brand)) +\n  geom_col()\n\n\n\n\n\n\n\n\nI’ve chosen the cut-off to be 200 as there was a gap in frequency between brands that sold more than 200 and less than 200. This reduces the comparison to 8 named brands, which is more manageable for comparison.\n\n\n\n\n\nThink about what a not interesting pattern might be for this data, and formulate an appropriate null hypothesis.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe least interesting pattern is probably if all the bars are similar heights, meaning that all brands are consumed equally.\nThis leads to a null hypothesis of \\(H_o: p_k = 1/K\\) where all brands have the same proportion of consumers.\n\n\n\n\n\nIf you were to test whether this sample were consistent with a sample from a multinomial distribution, where all whiskeys were equally popular, how would to generate null samples? Make the lineup for testing this.\n\nThe following code might help:\n\n# Subset the data, and anonymmise brand name\nscotch_consumption_sub &lt;- scotch_consumption |&gt;\n    mutate(\n    brand = ifelse(count &gt; 200, brand, \"Other Brands\")\n  ) |&gt;\n  filter(brand != \"Other Brands\") |&gt;\n  mutate(brand = as.character(factor(brand, labels=c(1:8)))) \n\nset.seed(220)\nsim &lt;- rmultinom(n=9,\n   size=sum(scotch_consumption_sub$count),\n   prob=rep(1/8, 8))\nsim &lt;- t(sim)\ncolnames(sim) &lt;- as.character(c(1:8))\nsim &lt;- sim |&gt;\n  as_tibble() |&gt;\n  mutate(.sample=1:9)\nsim &lt;- sim |&gt;\n  pivot_longer(cols=`1`:`8`, names_to = \"brand\", values_to = \"count\")\nscotch_lineup &lt;- bind_rows(sim,\n  bind_cols(.sample=10, scotch_consumption_sub))\n\n# Randomise .sample  to hide data plot\nscotch_lineup$.sample &lt;- rep(sample(1:10, 10), rep(8, 10))\n  \n# Make the lineup\nggplot(scotch_lineup, aes(x=brand, y=count)) +\n  geom_col() +\n  facet_wrap(~.sample, scales=\"free\", ncol=5) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nWe need to simulate samples from a multinomial distribution for the null sets.\nIf the data was a sample from a uniform distribution, it would mean that all of these brands are typically consumed in equal quantity.\n\\(H_0:\\) The counts for the brands are consistent with a sample from a uniform distribution.\n\n\n\n\n\n\n\n\n\nIdeally the bars are sorted from highest to lowest in each plot. This is tricky to do with facetting. The code below will do the sorting and re-draw the lineup.\n\n# Order categories in all samples from highest to lowest: TRICKY\np &lt;- list(\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\", \"p9\", \"p10\")\nfor (i in unique(scotch_lineup$.sample)) {\n  d &lt;- scotch_lineup |&gt; \n    filter(.sample == i)\n  d &lt;- d |&gt;\n    mutate(brand = fct_reorder(brand, count))\n  p[[i]] &lt;- ggplot(d, aes(y=brand, x=count)) +\n    geom_col() +\n    ggtitle(i) +\n    theme(axis.text = element_blank(),\n        axis.title = element_blank())\n}\n\np[[1]] + p[[2]] + p[[3]] + p[[4]] + p[[5]] +\n  p[[6]] + p[[7]] + p[[8]] + p[[9]] + p[[10]] + \n  plot_layout(ncol=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose you show your lineup to five of people who have not seen the data, and three of them report the data plot as the most different plot. Compute the \\(p\\)-value. What would these results tell you about the typical consumption of the different whiskey brands?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\npvisual(3, 5, 10)\n\n     x simulated  binom\n[1,] 3     0.015 0.0086\n\n\nLikely people will report that the reason for choosing the plot is that one bar is much bigger than the other bars.\nIt tells us that the data is not a sample from multinomial with equal probabilities, at least in the sense that one brand is consumed more frequently than the others.\n\n\n\n\n\nThis analysis ignored structure in the data, that survey participants could report consuming more than one brand. Have a discussion about what complications this might introduce for the analysis that we have just done. What might be an alternative way to compute the “counts” that takes this multiple responses into account? What else might we want to learn about survey participant responses?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nBy ignoring the number of responses per participant we have made the assumption that all responses are independent of each other.\nOne alternative way to compute the counts would be convert each participants’ responses into a fraction of their responses, for example, participant 1,\n\nScotch[1,]\n\n  Chivas.Regal Dewar.s.White.Label Johnnie.Walker.Black.Label J...B\n1            1                   0                          0     0\n  Johnnie.Walker.Red.Label Other.Brands Glenlivet Cutty.Sark Glenfiddich\n1                        1            1         1          0           1\n  Pinch..Haig. Clan.MacGregor Ballantine Macallan Passport Black...White\n1            0              0          0        0        0             0\n  Scoresby.Rare Grants Ushers White.Horse Knockando the.Singleton\n1             0      0      0           0         0             0\n\n\nreports consuming Chivas.Regal, Johnnie.Walker.Red.Label, Other.Brands, Glenlivet, and Glenfiddich. Each 1 would then be converted to 1/5.\nThe reason for re-analysing this way is to equally weight participants’ responses, so a participant who responded a lot would contribute the same relative amount to the overall count as a participant who responded very little.\nOther questions that might be of interest are:\n\nDo some respondents drink a variety of brands and others only few?\nDo some brands get consumed together more often than others?\n\nYou might come up with some other ideas!"
  },
  {
    "objectID": "week5/tutorialsol.html#finishing-up",
    "href": "week5/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 5",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week5/tutorial.html",
    "href": "week5/tutorial.html",
    "title": "ETC5521 Tutorial 5",
    "section": "",
    "text": "These are exercises in making plots of one variable and what can be learned about the distributions, data patterns and apply randomisation methods to check what we see."
  },
  {
    "objectID": "week5/tutorial.html#objectives",
    "href": "week5/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 5",
    "section": "",
    "text": "These are exercises in making plots of one variable and what can be learned about the distributions, data patterns and apply randomisation methods to check what we see."
  },
  {
    "objectID": "week5/tutorial.html#preparation",
    "href": "week5/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 5",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\ninstall.packages(c(\"ggplot2movies\", \"bayesm\",  \"ggbeeswarm\", \"patchwork\", \"nullabor\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week5/tutorial.html#exercises",
    "href": "week5/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 5",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: What are the common lengths of movies?\nLoad the movies dataset in the ggplot2movies package and answer the following questions based on it.\n\nHow many observations are in the data?\n\n\nDraw a histogram with an appropriate binwidth that shows the peaks at 7 minutes and 90 minutes. Draw another set of histograms to show whether these peaks existed both before and after 1980.\n\n\nThe variable Short indicates whether the film was classified as a short film (1) or not (0). Draw plots to investigate what rules was used to define a film as “short” and whether the films have been consistently classified.\n\n\nHow would you use the lineup protocol to determine if the periodic peaks could happen by chance? What would be the null hypothesis? Make your lineup. Does the data plot stand out? Compute the \\(p\\)-value, if 5 out of 12 people picked the data plot as the most different one in the lineup. Comment on the results. (Note: It might be most useful to assess this only for movies between 50-150 minutes long.)\n\n\n\nExercise 2: What is the market for different brands of whisky?\nThe Scotch data set in bayesm package was collated from a survey on scotch drinkers, recording the brand they consumed. Take a quick look at the data, and rearrange it to look like:\n\n\n# A tibble: 21 × 2\n   brand                      count\n   &lt;chr&gt;                      &lt;int&gt;\n 1 Chivas Regal                 806\n 2 Dewar's White Label          517\n 3 Johnnie Walker Black Label   502\n 4 J&B                          458\n 5 Johnnie Walker Red Label     424\n 6 Other Brands                 414\n 7 Glenlivet                    354\n 8 Cutty Sark                   339\n 9 Glenfiddich                  334\n10 Pinch (Haig)                 117\n11 Clan MacGregor               103\n12 Ballantine                    99\n13 Macallan                      95\n14 Passport                      82\n15 Black & White                 81\n16 Scoresby Rare                 79\n17 Grant's                       74\n18 Ushers                        67\n19 White Horse                   62\n20 Knockando                     47\n21 Singleton                     31\n\n\n\nProduce a barplot of the number of respondents per brand. What ordering of the brands do you think is the best? What is interesting about the distribution of counts?\n\n\nThere are 20 named brands and one category that is labelled as Other Brands. Produce a barplot that reduces the number of categories by selecting a criteria to lump certain brands to the Other Brands category.\n\n\nThink about what a not interesting pattern might be for this data, and formulate an appropriate null hypothesis.\n\n\nIf you were to test whether this sample were consistent with a sample from a multinomial distribution, where all whiskeys were equally popular, how would to generate null samples? Make the lineup for testing this.\n\nThe following code might help:\n\n# Subset the data, and anonymmise brand name\nscotch_consumption_sub &lt;- scotch_consumption |&gt;\n    mutate(\n    brand = ifelse(count &gt; 200, brand, \"Other Brands\")\n  ) |&gt;\n  filter(brand != \"Other Brands\") |&gt;\n  mutate(brand = as.character(factor(brand, labels=c(1:8)))) \n\nset.seed(220)\nsim &lt;- rmultinom(n=9,\n   size=sum(scotch_consumption_sub$count),\n   prob=rep(1/8, 8))\nsim &lt;- t(sim)\ncolnames(sim) &lt;- as.character(c(1:8))\nsim &lt;- sim |&gt;\n  as_tibble() |&gt;\n  mutate(.sample=1:9)\nsim &lt;- sim |&gt;\n  pivot_longer(cols=`1`:`8`, names_to = \"brand\", values_to = \"count\")\nscotch_lineup &lt;- bind_rows(sim,\n  bind_cols(.sample=10, scotch_consumption_sub))\n\n# Randomise .sample  to hide data plot\nscotch_lineup$.sample &lt;- rep(sample(1:10, 10), rep(8, 10))\n  \n# Make the lineup\nggplot(scotch_lineup, aes(x=brand, y=count)) +\n  geom_col() +\n  facet_wrap(~.sample, scales=\"free\", ncol=5) +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\nSuppose you show your lineup to five of people who have not seen the data, and three of them report the data plot as the most different plot. Compute the \\(p\\)-value. What would these results tell you about the typical consumption of the different whiskey brands?\n\n\nThis analysis ignored structure in the data, that survey participants could report consuming more than one brand. Have a discussion about what complications this might introduce for the analysis that we have just done. What might be an alternative way to compute the “counts” that takes this multiple responses into account? What else might we want to learn about survey participant responses?"
  },
  {
    "objectID": "week5/tutorial.html#finishing-up",
    "href": "week5/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 5",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week6/index.html",
    "href": "week6/index.html",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "",
    "text": "Wilke (2019) Ch 12 Visualising associations"
  },
  {
    "objectID": "week6/index.html#main-reference",
    "href": "week6/index.html#main-reference",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "",
    "text": "Wilke (2019) Ch 12 Visualising associations"
  },
  {
    "objectID": "week6/index.html#what-you-will-learn-this-week",
    "href": "week6/index.html#what-you-will-learn-this-week",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nThe humble but powerful scatterplot\nAdditions and variations\nTransformations to linearity\n(Robust) numerical measures of association\nSimpson’s paradox\nMaking null samples to test for association\nImputing missing values\nAssociations as networks"
  },
  {
    "objectID": "week6/index.html#lecture-slides",
    "href": "week6/index.html#lecture-slides",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week6/index.html#worksheet",
    "href": "week6/index.html#worksheet",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Worksheet",
    "text": "Worksheet\n\nqmd\nhtml"
  },
  {
    "objectID": "week6/index.html#tutorial-instructions",
    "href": "week6/index.html#tutorial-instructions",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week6/index.html#assignments",
    "href": "week6/index.html#assignments",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Assignments",
    "text": "Assignments\n\nExercises 2 is due on Monday 01 September.\nQuiz 5 is due on Thursday 04 September.\nQuiz 6 is due on Thursday 11 September."
  },
  {
    "objectID": "week6/tutorialsol.html",
    "href": "week6/tutorialsol.html",
    "title": "ETC5521 Tutorial 6",
    "section": "",
    "text": "These are exercises in making scatterplots and variations to examine association between two variables, to explore association matrices and networks, and conduct inference on associations."
  },
  {
    "objectID": "week6/tutorialsol.html#objectives",
    "href": "week6/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 6",
    "section": "",
    "text": "These are exercises in making scatterplots and variations to examine association between two variables, to explore association matrices and networks, and conduct inference on associations."
  },
  {
    "objectID": "week6/tutorialsol.html#preparation",
    "href": "week6/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 6",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is Wilke (2019) Ch 12 Visualizing associations. - Complete the weekly quiz, before the deadline! - Install the following R-packages if you do not have them already:\n\ninstall.packages(c(\"nullabor\", \"tidygraph\", \"ggraph\", \"plotly\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week6/tutorialsol.html#exercises",
    "href": "week6/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 6",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: Olympics\nWe have seen from the lecture that the Athletics category has too many different types of athletics in it for it to be a useful group for studying height and weight. There is another variable called Event which contains more specific information.\n\n# Read-in data\ndata(oly12, package = \"VGAMdata\")\n\n\nTabulate Event for just the Sport category Athletics, and decide which new categories to create.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nWorld Athletics, the sport’s governing body, defines athletics in six disciplines: track and field, road running, race walking, cross country running, mountain running, and trail running wikipedia.\nThat’s not so helpful! Track and field should be two different groups. I suggest running (short, middle and long distance), throwing, jumping, walking, and Decathlon (men) or Heptathlon (women)\n\n\n\n\n\nCreate the new categories, in steps, creating a new binary variable for each. The function str_detect is useful for searching for text patterns in a string. It also helps to know about regular expressions to work with strings like this. And there are two sites, which are great for learning: Regex puzzles, Information and testing board\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n# Give each athlete an id as unique identifier to facilitate relational joins\noly12 &lt;- oly12 %&gt;%\n  mutate(id = row_number(), .before = Name)\n\n# For athletes with &gt; 1 event, separate each event into a row\noly12_ath &lt;- oly12 %&gt;%\n  filter(Sport == \"Athletics\") %&gt;%\n  separate_rows(Event, sep = \", \")\n\n# Determine athlete types into 7 categories\noly12_ath &lt;- oly12_ath %&gt;%\n  mutate(\n    Ath_type = case_when(\n      # 100m, 110m Hurdles, 200m, 400m, 400m hurdles, 800m, 4 x 100m relay, 4 x 400m relay,\n      str_detect(Event, \"[1248]00m|Hurdles\") ~ \"Short distance\",\n      # 1500m, 3000m Steeplechase, 5000m\n      str_detect(Event, \"1500m|5000m|Steeplechase\") ~ \"Middle distance\",\n      # 10,000m, Marathon\n      str_detect(Event, \",000m|Marathon\") ~ \"Long distance\",\n      # 20km Race walk, Men's 50km Race walk\n      str_detect(Event, \"Walk\") ~ \"Walking\",\n      # discus throw, hammer throw, javelin throw, shot put,\n      str_detect(Event, \"Throw|Put\") ~ \"Throwing\",\n      # high jump, long jump, triple jump, pole vault\n      str_detect(Event, \"Jump|Pole Vault\") ~ \"Jumping\",\n      # decathlon (men) or heptathlon (women)\n      str_detect(Event, \"Decathlon|Heptathlon\") ~ \"Decathlon/Heptathlon\"\n    )\n  )\n\n# Remove rows with &gt; 1 of the same athlete type\noly12_ath &lt;- oly12_ath %&gt;%\n  select(-Event) %&gt;%\n  distinct()\n\n# Add events back to each athlete\noly12_ath &lt;- oly12_ath %&gt;%\n  left_join(\n    select(.data = oly12, c(Event, id)),\n    by = \"id\"\n  )\n\n\n\n\n\n\nMake several plots to explore the association between height and weight for the different athletic categories, eg scatterplots faceted by sex and event type, with/without free scales, linear models for the different subsets, overlaid on the same plot, 2D density plots faceted by sex and event type, with free scales.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nlibrary(colorspace)\nggplot(data = oly12_ath, aes(x = Height, y = Weight)) +\n  geom_point(alpha = 0.4) +\n  facet_grid(Sex ~ Ath_type)\n\n\n\n\n\n\n\nggplot(data = oly12_ath, aes(x = Height, y = Weight)) +\n  geom_point(alpha = 0.4) +\n  facet_grid(Sex ~ Ath_type, scales=\"free\")\n\n\n\n\n\n\n\nggplot(oly12_ath, aes(x = Height, y = Weight, colour = Ath_type)) +\n  geom_smooth(method = \"lm\", se = F) +\n  scale_colour_discrete_qualitative() +\n  facet_wrap(~Sex)\n\n\n\n\n\n\n\nggplot(data = oly12_ath, aes(x = Height, y = Weight)) +\n  geom_density2d(alpha = 0.4) +\n  facet_grid(Sex ~ Ath_type, scales=\"free\")\n\n\n\n\n\n\n\n\nIt is important to separate the sexes. Faceting by sport and sex and examining the scatterplots of height and weight in each is appropriate. Making a plot of just the regression models, faceted by sex can be useful also. A density plot is not so useful here because there isn’t a lot of difference in variance between the groups.\n\n\n\n\n\nList what you learned about body types across the different athletics types and sexes.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nHere are some examples:\n\nFrom the scatter plots we learn that\n\nthere are some heavy runners, especially in the shorter distances\nthe throwers are generally taller and much heavier.\nfemale walkers tend to be pretty small.\nlong distance runners are light!\n\nDecathlon/Heptathlon athletes are usually quite heavy; which makes sense as they have to be all-rounded\n\nif they’re too light, they may not do well in throwing events\nif they’re too heavy, they may not do well in running or jump events\n\nThe comparisons between groups is easier from the models.\n\nthrowers are heavy!\nlong/middle distance runners and walkers are relatively light.\n\n\n\n\n\n\n\nIf one were use visual inference to check for a different relationship between height and weight across sports how would you generate null data? Do it, and test your lineup with others in the class.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nYou would permute the Ath_type variable, keeping sex fixed. You could still facet by sex for the lineup, and use a smaller number of nulls so it’s not too over-whelming to read. Or you could separately make a full lineup of 20 for males and females separately.\n\nlibrary(nullabor)\nset.seed(141)\nggplot(lineup(null_permute(\"Ath_type\"),\n              true=oly12_ath, n=6), \n       aes(x = Height, \n           y = Weight, \n           colour = Ath_type)) +\n  geom_smooth(method = \"lm\", se = F, \n              fullrange=TRUE) +\n  scale_colour_discrete_qualitative() +\n  facet_grid(Sex~.sample) +\n  theme(legend.position=\"none\",\n        axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\nIf there was no difference between the event types, the the lines would all be a bit varied in slope but intersect in a single point. (Why?) The actual data has several differences: some events have the same slope but are shifted vertically, some have different slopes and are shifted. You don’t see this in the null plots.\nShould you re-do the nulls by forcing the intercepts to be the same in the data sample? If so, how would you do this?\n\n\n\n\n\n\nExercise 2: Exploring associations\nDownload the bike.rda file from the geomnet software site. This network is a summary of the bike trips taken by customers of the bike sharing company Capital Bikeshare () during the second quarter of 2015. Only trips between stations in the vicinity of Rockville, MD, are included. The data is organized as a list of two datasets, vertices (stations) and edges (trips between stations), as follows:\nA list of two data frames:\n\ntrips: the trips data set consists of four variables of length 53:\n\nStart.station: Station where bike trip starts\nEnd.station: Station where bike trip ends\nn: Number of trips between the two stations\nminlength: Duration of shortest trip between the two stations (in seconds). Only those stations are included, if the shortest trip between them lasted not more than 15 minutes.\n\nstations: the vertices data set consists of five variables with information on 21 stations:\n\nid: Station ID number\nname: Station name\nlat: Latitude of station location\nlong: Longitude of station location\nnbDocks: Number of bike docks at the station\n\n\nImagine you are the bike company, and you are interested in learning how to manage keeping bikes in places where people will use them.\n\na. Make some summaries of the bike station data.\n\n\nb. Make some summary of the trips data.\n\n\nc. Make an interactive heatmap\nThis is to understand the number of trips from one station to another. This will be examining where bikes typically are rented from and where they are left. You need to make sure you have a complete association matrix in order to do this.\n\n\nd. Represent the association as an interactive network\nYou can generate a layout based on the number of trips, or you can use the geographic location of the bike stations.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\n\nCode\nload(here::here(\"data/bikes.rda\"))\n\nggplot(bikes$stations, \n       aes(x=long, y=lat)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nCode\nggplot(bikes$stations, \n       aes(x=fct_reorder(name, nbDocks), y=nbDocks)) +\n  geom_col() +\n  xlab(\"\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nCode\ntrips &lt;- bikes$trips |&gt; \n  tibble()\n\nfull_list &lt;- expand_grid(Start.station = unique(trips$Start.station), \n                         End.station = unique(trips$End.station)) |&gt; \n  left_join(trips,\n            by = c(\"Start.station\" = \"Start.station\",\n                   \"End.station\" = \"End.station\")) |&gt; \n  mutate(n = replace_na(n, 0),\n         minlength = replace_na(n, 0))\n\nbike_level &lt;- full_list |&gt; \n  group_by(Start.station) |&gt; \n  summarise(n = sum(n)) |&gt; \n  arrange(desc(n)) |&gt; \n  pull(Start.station)\n\np &lt;- full_list |&gt; \n  mutate(Start.station = factor(Start.station, levels = bike_level),\n         End.station = factor(End.station, levels = bike_level)) |&gt; \n  ggplot( \n    aes(x = Start.station,\n        y = End.station, fill = n)) +\n  geom_tile() +\n  scale_fill_continuous_sequential(palette = \"YlGnBu\") +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        legend.position = \"none\")\n\nggplotly(p, width = 600, height = 600)\n\n\n\n\n\n\nCode\nstations &lt;- bikes$stations |&gt; \n  tibble() |&gt; \n  select(-id)\n  \n# graph data structure\nbikes_graph &lt;- tbl_graph(nodes = stations, edges = trips)\n\n# kk layout\nbikes_graph |&gt; \n  ggraph(layout = \"kk\") +\n  geom_edge_link(aes(edge_alpha = n)) +\n  geom_node_point(aes(size = nbDocks)) +\n  geom_node_label(aes(label = name), size = 1.5, repel = TRUE,\n                  label.padding = 0.15) +\n  theme(aspect.ratio=1,\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        panel.background = element_rect(fill=NA, \n                                        colour=\"black\"),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# Spatial layout\nbikes_graph |&gt; \n  ggraph(x = stations$long, y = stations$lat) +\n  geom_edge_link(aes(edge_alpha = n)) +\n  geom_node_point(aes(size = nbDocks)) +\n  geom_node_label(aes(label = name), size = 1.5, repel = TRUE,\n                  label.padding = 0.15) +\n  theme(aspect.ratio=1,\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        panel.background = element_rect(fill=NA, \n                                        colour=\"black\"),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# interactive graph\ninteractive_graph &lt;- bikes_graph |&gt; \n  ggraph(x = stations$long, y = stations$lat) +\n  geom_edge_link(aes(edge_alpha = n)) +\n  geom_point_interactive(aes(x = x, y = y,\n                             size = nbDocks,\n                             tooltip = name,\n                             data_id = name)) +\n  theme(aspect.ratio=1,\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        panel.background = element_rect(fill=NA, \n                                        colour=\"black\"),\n        legend.position = \"none\")\n\ngirafe(ggobj = interactive_graph,\n       options = list(\n         opts_hover(css = \"fill:lightblue;stroke:grey;stroke-width:0.5px\"),\n         opts_zoom(min = 0.5, max = 3)\n       ))\n\n\n\n\n\n\nCode\n# p2 &lt;- ggplot() +\n#   geom_point(data=bikes_nodes, \n#              aes(x=x, y=y, label=name)) +\n#   geom_segment(data=bikes_edges, \n#                aes(x=x, y=y, xend=xend, yend=yend),\n#                linewidth = 0.3) +\n#   scale_color_viridis_c() +\n#   theme(aspect.ratio=1,\n#         axis.text = element_blank(),\n#         axis.title = element_blank(),\n#         axis.ticks = element_blank(),\n#         panel.background = element_rect(fill=NA, \n#                                         colour=\"black\"),\n#         legend.position = \"none\")\n# ggplotly(p2, tooltip = \"label\", width=900, height=600)"
  },
  {
    "objectID": "week6/tutorialsol.html#finishing-up",
    "href": "week6/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 6",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week6/tutorial.html",
    "href": "week6/tutorial.html",
    "title": "ETC5521 Tutorial 6",
    "section": "",
    "text": "These are exercises in making scatterplots and variations to examine association between two variables, to explore association matrices and networks, and conduct inference on associations."
  },
  {
    "objectID": "week6/tutorial.html#objectives",
    "href": "week6/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 6",
    "section": "",
    "text": "These are exercises in making scatterplots and variations to examine association between two variables, to explore association matrices and networks, and conduct inference on associations."
  },
  {
    "objectID": "week6/tutorial.html#preparation",
    "href": "week6/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 6",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\nThe reading for this week is Wilke (2019) Ch 12 Visualizing associations. - Complete the weekly quiz, before the deadline! - Install the following R-packages if you do not have them already:\n\ninstall.packages(c(\"nullabor\", \"tidygraph\", \"ggraph\", \"plotly\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week6/tutorial.html#exercises",
    "href": "week6/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 6",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: Olympics\nWe have seen from the lecture that the Athletics category has too many different types of athletics in it for it to be a useful group for studying height and weight. There is another variable called Event which contains more specific information.\n\n# Read-in data\ndata(oly12, package = \"VGAMdata\")\n\n\nTabulate Event for just the Sport category Athletics, and decide which new categories to create.\n\n\nCreate the new categories, in steps, creating a new binary variable for each. The function str_detect is useful for searching for text patterns in a string. It also helps to know about regular expressions to work with strings like this. And there are two sites, which are great for learning: Regex puzzles, Information and testing board\n\n\nMake several plots to explore the association between height and weight for the different athletic categories, eg scatterplots faceted by sex and event type, with/without free scales, linear models for the different subsets, overlaid on the same plot, 2D density plots faceted by sex and event type, with free scales.\n\n\nList what you learned about body types across the different athletics types and sexes.\n\n\nIf one were use visual inference to check for a different relationship between height and weight across sports how would you generate null data? Do it, and test your lineup with others in the class.\n\n\n\nExercise 2: Exploring associations\nDownload the bike.rda file from the geomnet software site. This network is a summary of the bike trips taken by customers of the bike sharing company Capital Bikeshare () during the second quarter of 2015. Only trips between stations in the vicinity of Rockville, MD, are included. The data is organized as a list of two datasets, vertices (stations) and edges (trips between stations), as follows:\nA list of two data frames:\n\ntrips: the trips data set consists of four variables of length 53:\n\nStart.station: Station where bike trip starts\nEnd.station: Station where bike trip ends\nn: Number of trips between the two stations\nminlength: Duration of shortest trip between the two stations (in seconds). Only those stations are included, if the shortest trip between them lasted not more than 15 minutes.\n\nstations: the vertices data set consists of five variables with information on 21 stations:\n\nid: Station ID number\nname: Station name\nlat: Latitude of station location\nlong: Longitude of station location\nnbDocks: Number of bike docks at the station\n\n\nImagine you are the bike company, and you are interested in learning how to manage keeping bikes in places where people will use them.\n\na. Make some summaries of the bike station data.\n\n\nb. Make some summary of the trips data.\n\n\nc. Make an interactive heatmap\nThis is to understand the number of trips from one station to another. This will be examining where bikes typically are rented from and where they are left. You need to make sure you have a complete association matrix in order to do this.\n\n\nd. Represent the association as an interactive network\nYou can generate a layout based on the number of trips, or you can use the geographic location of the bike stations."
  },
  {
    "objectID": "week6/tutorial.html#finishing-up",
    "href": "week6/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 6",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week7/index.html",
    "href": "week7/index.html",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "",
    "text": "Wilke (2019) Ch 9, 10.2-4, 11.2"
  },
  {
    "objectID": "week7/index.html#main-reference",
    "href": "week7/index.html#main-reference",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "",
    "text": "Wilke (2019) Ch 9, 10.2-4, 11.2"
  },
  {
    "objectID": "week7/index.html#what-you-will-learn-this-week",
    "href": "week7/index.html#what-you-will-learn-this-week",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nWhy making comparisons is important\nHow to decide on what comparison to make\nComparing between strata, relative to a baseline\nComparing the same data, in many ways\nUsing normalising to compare different distributions\nInference using bootstrap and lineups"
  },
  {
    "objectID": "week7/index.html#lecture-slides",
    "href": "week7/index.html#lecture-slides",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week7/index.html#worksheet-instructions",
    "href": "week7/index.html#worksheet-instructions",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Worksheet instructions",
    "text": "Worksheet instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week7/index.html#tutorial-instructions",
    "href": "week7/index.html#tutorial-instructions",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week7/index.html#assignments",
    "href": "week7/index.html#assignments",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Assignments",
    "text": "Assignments\n\nQuiz 6 is due on Thursday 11 September.\nQuiz 7 is due on Thursday 18 September."
  },
  {
    "objectID": "week7/tutorialsol.html",
    "href": "week7/tutorialsol.html",
    "title": "ETC5521 Tutorial 7",
    "section": "",
    "text": "These are exercises so that you can make some numerical and graphical comparisons for various datasets and help to think about the comparisons being made."
  },
  {
    "objectID": "week7/tutorialsol.html#objectives",
    "href": "week7/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 7",
    "section": "",
    "text": "These are exercises so that you can make some numerical and graphical comparisons for various datasets and help to think about the comparisons being made."
  },
  {
    "objectID": "week7/tutorialsol.html#preparation",
    "href": "week7/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 7",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nThe reading for this week is Wilke (2019) Chapters 9, 10.2-4, 11.2.\nComplete the weekly quiz, before the deadline!\nInstall the following R-packages if you do not have them already:\n\n\ninstall.packages(c(\"colorspace\", \"lvplot\", \"patchwork\", \"janitor\", \"lubridate\", \"vcd\", \"ggbeeswarm\", \"kableExtra\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week7/tutorialsol.html#exercises",
    "href": "week7/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 7",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: Melbourne daily maximum temperature\nThe csv file melb_temp_2023-09-08.csv contains data on the daily maximum temperature from 1970 to 2023 collected from the weather station at Melbourne Airport. Use this to answer the following questions, with the additional information that in Australia:\n\nSummer is from the beginning of December to the end of February,\nAutumn is from the beginning of March to the end of May,\nWinter is from the beginning of June to the end of August, and\nSpring is from the beginning of September to the end of November.\n\n\nThere are four plots below. Write the code to make them yourself. Then think about the three questions (i), (ii) or (iii) below.\n\n\n\nAre there any winters where the daily maximum temperature is different to winter in other years?\n\n\nWhat is the general pattern of maximum daily temperatures in winter?\n\n\nIs there evidence that winters in Melbourne are getting warmer?\n\n\nWhich plot best matches each question? If none of them work, for any particular question, make an alternative plot. Also, if any of the plots don’t help answer any of the questions, think about a question that they might answer.\n\nMake a transformation of the data and a new plot with this variable, that will allow a more direct comparison to answer question (iii).\n\nThe data can be read and processed using this code:\n\nmelb_df &lt;- read_csv(\"https://raw.githubusercontent.com/numbats/ddde/main/data/melb_temp_2023-09-08.csv\") |&gt;\n  clean_names() |&gt;\n  rename(temp = maximum_temperature_degree_c) |&gt;\n  dplyr::filter(!is.na(temp)) |&gt;\n  dplyr::select(year, month, day, temp) |&gt;\n  mutate(\n    date = as.Date(paste(year, month, day, sep = \"-\")))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nCode for the plots is:\n\n\n# Plot (A)\nwinter_df &lt;- melb_df |&gt;\n  dplyr::filter(month %in% c(\"06\", \"07\", \"08\")) |&gt;\n  mutate(winter_day = as.numeric(\n    difftime(date, \n             ymd(paste0(year, \"-06\", \"-01\")), \n             units=\"days\")))\np_a &lt;- ggplot(winter_df, aes(x=winter_day, y=temp)) +\n  geom_point(\n    data = dplyr::select(winter_df, -year),\n    color = \"gray\", size = 0.1\n  ) +\n  geom_line() +\n  facet_wrap(~year) +\n  labs(\n    tag = \"(A)\", x = \"\",\n    y = \"Temperature (°C)\"\n  ) + \n  scale_x_continuous(\"\", breaks = c(0, 31, 62, 92), labels=c(\"J\", \"J\", \"A\", \"S\"))\n\n# Plot (B)\np_b &lt;- ggplot(winter_df, aes(x=year, y=temp)) +\n  #geom_boxplot() +\n  geom_jitter(width=0.1, height=0, alpha=0.2) +\n  stat_summary(fun=\"mean\", geom=\"point\", colour=\"orangered\", size=3) +\n  geom_smooth(colour=\"red\") +\n  labs(\n    tag = \"(B)\", x = \"Year\",\n    y = \"Temperature (°C)\"\n  ) \n\n# Plot (C)\np_c &lt;- winter_df |&gt;\n  mutate(year = fct_reorder(as.factor(year), temp)) |&gt;\n  ggplot(aes(temp, year)) +\n  # geom_boxplot(aes(color = as.numeric(as.character(year)))) +\n  geom_quasirandom(aes(x=as.factor(year), y=temp,\n                       colour = as.numeric(as.character(year)))) +\n  labs(\n    tag = \"(C)\", x = \"Year\",\n    y = \"Temperature (°C)\",\n    color = \"Year\"\n  ) +\n  scale_color_continuous_divergingx(mid = 1995) +\n  coord_flip()\n\n# Plot (D)\np_d &lt;- winter_df |&gt;\n  mutate(pre1995 = ifelse(year &lt; 1995, \"&lt; 1995\", \"&gt; 1995\")) |&gt;\n  ggplot(aes(x=pre1995, y=temp)) +\n  geom_lv(aes(fill=after_stat(LV))) +\n  labs(\n    tag = \"(D)\", y = \"Temperature (°C)\",\n    x = \"Time Period\"\n  ) +\n  scale_fill_discrete_divergingx(palette = \"Fall\")\n\np_a + p_b + p_c + p_d + \n  plot_layout(ncol=1, heights=c(5,3,5,3))\n\nPlot A is the only one that allows examining the pattern for winter each year, which helps to address questions (i) and (ii).\nIn answer to (i) I don’t see any year that is especially different from any other year.\nThere is a lot of variability in the patterns for winter. For most years it appears to be fairly flat with an increase in August. However, some years temperature remain cool in August.\nTo better study the winter pattern it might be better to use a smoother model instead of connecting the day-to-day measurements with a line.\n\nggplot(winter_df, aes(x=winter_day, y=temp)) +\n  geom_point(\n    data = dplyr::select(winter_df, -year),\n    color = \"gray\", size = 0.1\n  ) +\n  geom_smooth(se=F, colour=\"black\") +\n  facet_wrap(~year) +\n  labs(\n    tag = \"(E)\", x = \"\",\n    y = \"Temperature (°C)\"\n  ) + \n  scale_x_continuous(\"\", breaks = c(0, 31, 62, 92), labels=c(\"J\", \"J\", \"A\", \"S\"))\n\n\n\n\n\n\n\n\nFrom plot E, it’s a bit easier to see that overall the years (grey points) there is the expected pattern of a dip to the coolest part of winter and then a warmup. However, this pattern is very rare in any year. Variability is more typical, some years the temperature is flat across June, July, August, and in some years there is a dramatic warming in August. Some years it is even warmer in July than August.\nBoth plots C, D address question (iii). Plot C is a bit awkward! Reading the colour trend is easy. The top of the diagram is more red, and the bottom more green. To understand what this means, and how it relates to the question requires thinking about the colour mapping, in relation to the sorting of the y axis. If there was no relationship between median temperature and year, the colouring would be very mixed. That’s not what is seen - the later years (more orange) occur with higher medians. Thus this plot would help us answer question (iii), with there is evidence that winters have been warmer in recent years.\nPlot D is much more compact and simple to interpret. The colder winter temperatures seen before 1995 have not been observed after 1995. Also the median is higher in the after 1995 years. It gives evidence for “winters are warmer in recent years” too, but it is too coarse a view with only these two subsets of years. Interestingly, the maximums of the maximum temperatures don’t appear to have changed.\n\nUsing the median or a median of an early period of measurements as a baseline, compute the relative change in temperature. Plot these against year, using yearly median as points with a smoother, or possibly using bars (above and below zero) to display the yearly median.\n\n\nwinter_df &lt;- winter_df |&gt;\n  mutate(rel_change = (temp - median(temp))/median(temp))\n\nwinter_df |&gt;\n  ggplot(aes(x=year, y=rel_change)) +\n  geom_hline(yintercept=0, linewidth=3, colour=\"grey90\") +\n  stat_summary(fun=\"mean\", geom=\"point\") +\n  geom_smooth(colour=\"red\", se=F) +\n  labs(\n    tag = \"(F)\", x = \"Year\",\n    y = \"Change in temperature\",\n    color = \"Year\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Evidence of Simpson’s paradox?\nCheck the following data set for evidence of Simpsons Paradox, in the sense that if group2 == \"X\" the pass rate is higher.\n\ndf &lt;- tribble(\n  ~group1, ~group2, ~result, ~count,\n  \"A\", \"X\", \"pass\", 100,\n  \"B\", \"X\", \"pass\", 50,\n  \"C\", \"X\", \"pass\", 25,\n  \"A\", \"X\", \"fail\", 10,\n  \"B\", \"X\", \"fail\", 20,\n  \"C\", \"X\", \"fail\", 20,\n  \"A\", \"Y\", \"pass\", 10,\n  \"B\", \"Y\", \"pass\", 70,\n  \"C\", \"Y\", \"pass\", 15,\n  \"A\", \"Y\", \"fail\", 20,\n  \"B\", \"Y\", \"fail\", 40,\n  \"C\", \"Y\", \"fail\", 30)\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nOverall, if group2 is “X” the pass rate is higher. This is true if the data is also examined separately for each of A, B, C of group1. The rates are slightly different though, with A having a big difference in pass rate, and B having a small difference. So there is evidence of Simpsons Paradox because the proportions differ.\n\ndf |&gt; group_by(group1) |&gt; summarise(sum(count))\n\n# A tibble: 3 × 2\n  group1 `sum(count)`\n  &lt;chr&gt;         &lt;dbl&gt;\n1 A               140\n2 B               180\n3 C                90\n\ndf |&gt; group_by(group2) |&gt; summarise(sum(count))\n\n# A tibble: 2 × 2\n  group2 `sum(count)`\n  &lt;chr&gt;         &lt;dbl&gt;\n1 X               225\n2 Y               185\n\nggplot(df, aes(x=group2, y=count, fill=result)) +\n  geom_col(position=\"fill\") +\n  scale_fill_discrete_divergingx()\n\n\n\n\n\n\n\ndoubledecker(xtabs(count ~ group1 + group2 + result, data = df),\n  gp = gpar(fill = c(\"grey90\", \"orangered\"))\n)"
  },
  {
    "objectID": "week7/tutorialsol.html#finishing-up",
    "href": "week7/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 7",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week7/tutorial.html",
    "href": "week7/tutorial.html",
    "title": "ETC5521 Tutorial 7",
    "section": "",
    "text": "These are exercises so that you can make some numerical and graphical comparisons for various datasets and help to think about the comparisons being made."
  },
  {
    "objectID": "week7/tutorial.html#objectives",
    "href": "week7/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 7",
    "section": "",
    "text": "These are exercises so that you can make some numerical and graphical comparisons for various datasets and help to think about the comparisons being made."
  },
  {
    "objectID": "week7/tutorial.html#preparation",
    "href": "week7/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 7",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nThe reading for this week is Wilke (2019) Chapters 9, 10.2-4, 11.2.\nComplete the weekly quiz, before the deadline!\nInstall the following R-packages if you do not have them already:\n\n\ninstall.packages(c(\"colorspace\", \"lvplot\", \"patchwork\", \"janitor\", \"lubridate\", \"vcd\", \"ggbeeswarm\", \"kableExtra\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week7/tutorial.html#exercises",
    "href": "week7/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 7",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: Melbourne daily maximum temperature\nThe csv file melb_temp_2023-09-08.csv contains data on the daily maximum temperature from 1970 to 2023 collected from the weather station at Melbourne Airport. Use this to answer the following questions, with the additional information that in Australia:\n\nSummer is from the beginning of December to the end of February,\nAutumn is from the beginning of March to the end of May,\nWinter is from the beginning of June to the end of August, and\nSpring is from the beginning of September to the end of November.\n\n\nThere are four plots below. Write the code to make them yourself. Then think about the three questions (i), (ii) or (iii) below.\n\n\n\nAre there any winters where the daily maximum temperature is different to winter in other years?\n\n\nWhat is the general pattern of maximum daily temperatures in winter?\n\n\nIs there evidence that winters in Melbourne are getting warmer?\n\n\nWhich plot best matches each question? If none of them work, for any particular question, make an alternative plot. Also, if any of the plots don’t help answer any of the questions, think about a question that they might answer.\n\nMake a transformation of the data and a new plot with this variable, that will allow a more direct comparison to answer question (iii).\n\nThe data can be read and processed using this code:\n\nmelb_df &lt;- read_csv(\"https://raw.githubusercontent.com/numbats/ddde/main/data/melb_temp_2023-09-08.csv\") |&gt;\n  clean_names() |&gt;\n  rename(temp = maximum_temperature_degree_c) |&gt;\n  dplyr::filter(!is.na(temp)) |&gt;\n  dplyr::select(year, month, day, temp) |&gt;\n  mutate(\n    date = as.Date(paste(year, month, day, sep = \"-\")))\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Evidence of Simpson’s paradox?\nCheck the following data set for evidence of Simpsons Paradox, in the sense that if group2 == \"X\" the pass rate is higher.\n\ndf &lt;- tribble(\n  ~group1, ~group2, ~result, ~count,\n  \"A\", \"X\", \"pass\", 100,\n  \"B\", \"X\", \"pass\", 50,\n  \"C\", \"X\", \"pass\", 25,\n  \"A\", \"X\", \"fail\", 10,\n  \"B\", \"X\", \"fail\", 20,\n  \"C\", \"X\", \"fail\", 20,\n  \"A\", \"Y\", \"pass\", 10,\n  \"B\", \"Y\", \"pass\", 70,\n  \"C\", \"Y\", \"pass\", 15,\n  \"A\", \"Y\", \"fail\", 20,\n  \"B\", \"Y\", \"fail\", 40,\n  \"C\", \"Y\", \"fail\", 30)"
  },
  {
    "objectID": "week7/tutorial.html#finishing-up",
    "href": "week7/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 7",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week8/index.html",
    "href": "week8/index.html",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "",
    "text": "Cook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1"
  },
  {
    "objectID": "week8/index.html#main-reference",
    "href": "week8/index.html#main-reference",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "",
    "text": "Cook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1"
  },
  {
    "objectID": "week8/index.html#what-you-will-learn-this-week",
    "href": "week8/index.html#what-you-will-learn-this-week",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nWhat is high-dimensional data? (If all variables are quantitative)\nExploring relationships between more than two variables\n\nTours - scatterplots of combinations of variables\nMatrix of plots\nParallel coordinates\n\nWhat can be hidden\nAutomating the search for pairwise relationships using scagnostics\nLinking elements of multiple plots\nExploring multiple categorical variables"
  },
  {
    "objectID": "week8/index.html#lecture-slides",
    "href": "week8/index.html#lecture-slides",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week8/index.html#worksheet-instructions",
    "href": "week8/index.html#worksheet-instructions",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Worksheet instructions",
    "text": "Worksheet instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week8/index.html#tutorial-instructions",
    "href": "week8/index.html#tutorial-instructions",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week8/index.html#assignments",
    "href": "week8/index.html#assignments",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Assignments",
    "text": "Assignments\n\nExercises 3 is due on Monday 22 September.\nQuiz 7 is due on Thursday 18 September.\nQuiz 8 is due on Thursday 25 September."
  },
  {
    "objectID": "week8/tutorialsol.html",
    "href": "week8/tutorialsol.html",
    "title": "ETC5521 Tutorial 8",
    "section": "",
    "text": "These are exercises in plots to make to explore relationships between multiple variables. You will use interactive scatterplot matrices, interactive parallel coordinate plots and tours to explore the world beyond 2D."
  },
  {
    "objectID": "week8/tutorialsol.html#objectives",
    "href": "week8/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 8",
    "section": "",
    "text": "These are exercises in plots to make to explore relationships between multiple variables. You will use interactive scatterplot matrices, interactive parallel coordinate plots and tours to explore the world beyond 2D."
  },
  {
    "objectID": "week8/tutorialsol.html#preparation",
    "href": "week8/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 8",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nThe reading for this week is Cook and Laa (2023) “Interactively exploring high-dimensional data and models in R” Chapter 1.\nComplete the weekly quiz, before the deadline!\nInstall the following R-packages if you do not have them already:\n\n\ninstall.packages(c(\"tidyverse\", \"tourr\", \"GGally\", \"plotly\", \"colorspace\", \"mulgar\", \"simputation\", \"naniar\", \"crosstalk\", \"sf\", \"ozmaps\", \"ggthemes\"))\n\n\nOpen your RStudio Project for this unit (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week8/tutorialsol.html#exercises",
    "href": "week8/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 8",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: Melbourne housing\n\nRead in a copy of the Melbourne housing data from Nick Tierney’s github repo which is a collation from the version at kaggle. It’s fairly large, so let’s start simply, and choose two suburbs to focus on. I recommend “South Yarra” and “Brighton”.\n\n\nmel_houses &lt;- read_csv(\"https://raw.githubusercontent.com/njtierney/melb-housing-data/master/data/housing.csv\") |&gt;\n  dplyr::filter(suburb %in% c(\"South Yarra\", \"Brighton\")) \n\n\nThere are a substantial number of missing values. These need to be handled first because examining multiple variables, with almost every method, requires complete data. Examine the missing value distribution using naniar, and strategise on how to handle the missings.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThis is an awful data set to wrangle into a form that can be analysed! First step is to get a sense of the missing values in each of the suburbs chosen. It is always a good idea to break it into these groups, if there aren’t too many, in case there is a difference in data handling between locations.\n\nm1_sy &lt;- mel_houses |&gt; filter(suburb == \"South Yarra\") |&gt; gg_miss_var(show_pct = TRUE) + ggtitle(\"South Yarra\")\nm1_b &lt;- mel_houses |&gt; filter(suburb == \"Brighton\") |&gt; gg_miss_var(show_pct = TRUE) + ggtitle(\"Brighton\")\nm2_sy &lt;- mel_houses |&gt; filter(suburb == \"South Yarra\") |&gt; gg_miss_case(show_pct = TRUE) + ggtitle(\"South Yarra\")\nm2_b &lt;- mel_houses |&gt; filter(suburb == \"Brighton\") |&gt; gg_miss_case(show_pct = TRUE) + ggtitle(\"Brighton\")\nm1_sy + m1_b + m2_sy + m2_b + plot_layout(ncol=2)\n\n\n\n\n\n\n\nFigure 1: Plots of missing by variable and case.\n\n\n\n\n\nSummary:\n\nSimilar missingness in both suburbs. Although there are some slight differences, the same variables have missings and a similar number of cases have substantial missings.\nThe main information about variables to note is that building_area and year_built both have more than 50% missing.\n\n\nm3 &lt;- vis_miss(mel_houses, cluster = TRUE, sort_miss = TRUE)\nm3\n\n\n\n\n\n\n\nFigure 2: Overall summary of missing values, with variables and cases grouped.\n\n\n\n\n\nSummary:\nThere are co-occurring missings.\n\nIf bedroom2 is missing then likely bathroom, landsize, car, latitude and longitude are missing.\nWhen price is missing, most other variables are available - this is good!\nBut often price and landsize are both missing.\nThere is a small amount of sporadic missing values other than these.\n\nStrategy:\n\nremove/ignore the two worst variables, building_area and year_built which have more missings than present data.\nremove observations that have missings on all of these variables: landsize, car, bathroom, bedroom2, bathroom, latitude, longitude. There is not enough information in the remaining variables to be able to impute the missing values.\n\nTo impute missing values the choice is to fit a regression model on complete records. (We have previously noted in lecture that to impute price we need to take into account other variables because missingness appears to depend on other variables.) This will be done in pieces, to impute price when observations on all other variables are available, we will use these.\n\n\n\n\n\nNow implement your strategy for removing, and imputing missing values, until you have a complete data set. This can involve some iteration on the strategy, as we find that some other approach is needed.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nWe will separate date into year and quarter, as these might be useful for the imputation - prices will change from year to year, and, from experience, we know that there are some seasonal differences in property prices.\nNotice that, the filtering and imputation should be done in small steps, checking the resulting data after each.\n\nmel_houses_na &lt;- mel_houses |&gt;\n  mutate(year = lubridate::year(date),\n         quarter = lubridate::quarter(date)) |&gt;\n  select(-building_area, -year_built) |&gt;\n  filter(!(is.na(car) & \n             is.na(bathroom) &\n             is.na(bedroom2) &\n             is.na(latitude) &\n             is.na(longitude) &\n             is.na(landsize)))\n\n# check data again\n(nrow(mel_houses) - nrow(mel_houses_na))/nrow(mel_houses)\n\n[1] 0.22\n\n# 21% of observations removed - that's a lot but \n# realistically it is not possible to impute \n# price without information on \n# bedrooms, baths, landsize and location\nvis_miss(mel_houses_na, cluster = TRUE, sort_miss = TRUE)\n\n\n\n\n\n\n\n\nNotice that 21% of observations removed - that’s a lot but realistically it is not possible to impute price without information on bedrooms, baths, landsize and location.\nNext, handling landsize might be done better with some logic. Check landsize and price and type of unit. It may be that landsize is missing for units, which would be ok, and then we would use 0 as the imputed value.\n\nggplot(mel_houses_na, aes(x=landsize, y=price)) +\n  geom_miss_point() +\n  facet_wrap(~type, ncol=3, scales=\"free\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\nHmm, no but maybe for units and townhouses it is reasonable to set landsize missings to be 0, because technically there is little land owned by owners of units and townhouses.\n\nmel_houses_na &lt;- mel_houses_na |&gt;\n  mutate(landsize = if_else(type %in% c(\"u\", \"t\") & is.na(landsize), 0, landsize))\nvis_miss(mel_houses_na, cluster = TRUE, sort_miss = TRUE)\n\n\n\n\n\n\n\n\nNow remove observations missing on both landsize and price, because houses can’t really be priced without knowing landsize.\n\nmel_houses_na &lt;- mel_houses_na |&gt;\n  filter(!(is.na(landsize) & is.na(price)))\nvis_miss(mel_houses_na, cluster = TRUE, sort_miss = TRUE)\n\n\n\n\n\n\n\n\nHere is the imputation using regression, and the simputation function impute_lm made available from naniar. I recommend running each part of the pipe, and making a couple of plots to examine the result. I’ve put the piped code together after doing this, to make it more readable, but it took some work checking the results, and fixing a few things each time, to confidently be able to make it all one big imputation.\nNote also, that imputed values for bedroom2, bathroom and car have been rounded to integer. The regression model will impute these with decimal places, which is not realistic in the later analysis. This is one place, where I iterated in the handling os missings after noticing that bedrooms and bathrooms looked odd.\n\n# Conduct imputation, carefully!\nmel_houses_imputed &lt;- mel_houses_na |&gt; \n  nabular() |&gt;\n  as.data.frame() |&gt; \n  # bedroom2\n  impute_lm(bedroom2 ~ type + quarter + year +  \n              latitude + longitude +\n              postcode + distance) |&gt;\n  mutate(bedroom2 = round(bedroom2, 0)) |&gt;\n  # bathroom\n  impute_lm(bathroom ~ type + quarter + year + \n              latitude + longitude +\n              postcode + distance) |&gt;\n  mutate(bathroom = round(bathroom, 0)) |&gt;\n  # car\n  impute_lm(car ~ type + quarter + year +  \n              bedroom2 + bathroom + latitude +\n              longitude + postcode + distance) |&gt;\n  mutate(car = round(car, 0)) |&gt;\n  # landsize\n  impute_lm(landsize ~ type + quarter + year + \n              bedroom2 + bathroom + \n              latitude + longitude +\n              postcode + distance)\n\n# check again\nvis_miss(mel_houses_imputed, cluster = TRUE,\n         sort_miss = TRUE)\n\n\n\n\n\n\n\nggplot(mel_houses_na, aes(x=landsize, y=price)) +\n  geom_miss_point() +\n  facet_wrap(~type, ncol=3, scales=\"free\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\nggplot(mel_houses_imputed, aes(x=landsize, y=price)) +\n  geom_miss_point() +\n  facet_wrap(~type, ncol=3, scales=\"free\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n# Also check bedroom2, bathroom and car\n\n# price\nmel_houses_imputed &lt;- mel_houses_imputed |&gt;\n  impute_lm(price ~ type + quarter + year + landsize + \n              bedroom2 + bathroom + latitude +\n              longitude + postcode + distance) \n# check again\nvis_miss(mel_houses_imputed, cluster = TRUE, sort_miss = TRUE)\n\n\n\n\n\n\n\nggplot(mel_houses_imputed, aes(x=landsize, y=price)) +\n  geom_point() +\n  facet_wrap(~type, ncol=3, scales=\"free\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\nggplot(mel_houses_imputed, aes(x=bedroom2, y=price)) +\n  geom_point() +\n  facet_wrap(~type, ncol=3, scales=\"free\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\nggplot(mel_houses_imputed, aes(x=bathroom, y=price)) +\n  geom_point() +\n  facet_wrap(~type, ncol=3, scales=\"free\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n# Now add label\nmel_houses_imputed &lt;- mel_houses_imputed |&gt;\n  add_label_shadow()\n# Note: method not used because it causes problems with missing factor levels. Ideally it is included.\n\nOne more check of resulting data.\n\nggplot(mel_houses, aes(x=bedroom2, y=price)) +\n         geom_miss_point() +\n         facet_grid(type~suburb)\n\n\n\n\n\n\n\nggplot(mel_houses_imputed, aes(x=bedroom2, y=price, colour=price_NA)) +\n         geom_point() +\n         facet_grid(type~suburb) +\n  scale_colour_discrete_divergingx(palette=\"Zissou 1\")\n\n\n\n\n\n\n\n\nOk, quite confident now that we have a reasonable set of data to work with. There are some odd values in the data still that will need handling at some point, like negative landsize. But for now these can be part of the analysis, as is.\nHaving the additional variables _NA will allow identifying observations that have been imputed as we work through the analysis.\n\n\n\n\n\nMake a scatterplot matrix of price, rooms, bedroom2, bathroom, landsize, latitude and longitude, suburb. The order of variables can affect the readability. I advise that the plot will be easier to read if you order the variables a little, at least put price first. Think about what associations can be seen?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nggpairs(mel_houses_imputed,\n        columns=c(4,2,10,11,13,15,16,1,3))\n\n\n\n\n\n\n\n\n\nSome continuous variables are discrete. We can still examine the associations. It could be useful to use a jittered scatterplot, but that would require making a special plot function to use in the ggpairs function.\nThere is positive linear association between price, rooms, bedroom2, bathroom, which indicates the bigger the house the higher the price\nFrom the boxplots: houses in Brighton tend to be higher priced and bigger than South Yarra, and houses tend to be worth more than townhouses and units.\nThere are lots of outliers.\n\n\n# To add jitter\nggpairs(mel_houses_imputed,\n        columns=c(4,2,10,11,13,15,16),\n        lower=list(continuous=wrap(\"points\",\n                  position=position_jitter(height=0.3, width=0.3))))\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s where we will start using interactive plots to explore the multivariate relationships. Subset the data to Brighton only. This will make the analysis easier. Make an interactive scatterplot matrix of price, rooms, bedroom2, bathroom and landsize, coloured by type of property. There are some high price properties. Select these cases, and determine what’s special about them – if anything.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nbrighton &lt;- mel_houses_imputed |&gt; \n  dplyr::filter(suburb==\"Brighton\") \nhighlight_key(brighton) |&gt;\n  ggpairs(aes(shape = price_NA, colour=type), \n          columns = c(4,2,10,11,13),\n                  upper=list(continuous=\"points\")) |&gt;\n  ggplotly(900, 900) |&gt;\n  highlight(\"plotly_selected\")\n\n\n\n\n\n\n# To add jittering\n# which makes it a little easier to see all points\n# in the discrete variables\nhighlight_key(brighton) |&gt;\n  ggpairs(aes(shape = price_NA, colour=type), \n          columns = c(4,2,10,11,13),\n          lower=list(continuous=wrap(\"points\",\n                  position=position_jitter(height=0.3, width=0.3))),\n          upper=list(continuous=wrap(\"points\",\n                  position=position_jitter(height=0.3, width=0.3)))) |&gt;\n  ggplotly(900, 900) |&gt;\n  highlight(\"plotly_selected\") \n\n\n\n\n\n\nThere is one very high price, which is a modest size of 4 bedrooms and 3 bathrooms.\nThere is one property with an extraordinarily large number of rooms, but this is an imputed value for price. Something may have gone wrong with the imputation, because it has a low price despite being a big property. Despite the large number of rooms it only has 3 bedrooms and 2 bathrooms.\n\n\n\n\n\n\nThe realtors mantra is location, location, location! Next check the location of properties relative to price and size, using two linked plots. One should have the longitude and latitude, and the other price by rooms. Ideally, you can make a map underneath the spatial coordinates, to better put these in context.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe ozmaps package has quick maps of Australia that can be used. The code below checks that the map can be made.\n\n# Check location\nlibrary(sf)\nlibrary(ozmaps)\nlibrary(ggthemes)\nlibrary(crosstalk)\nlga_sf &lt;- ozmap_data(\"abs_lga\")\nbrighton_map &lt;- lga_sf |&gt;\n  filter(NAME %in% c(\"Bayside (C)\", \n                     \"Glen Eira (C)\",\n                     \"Stonnington (C)\",\n                     \"Port Phillip (C)\"))\nbasic_map &lt;- ggplot(brighton_map) + geom_sf() +\n  geom_point(data=brighton, \n             aes(x=longitude, y=latitude,\n                 colour=type, shape=price_NA)) +\n  theme_map() +\n  theme(legend.position = \"none\")\n\nThis code can be used to make the interactive linked plots.\n\n# Now make interactive linked plots\nshared_brighton &lt;- SharedData$new(brighton)\n\nmap &lt;- ggplot(brighton_map) + \n  geom_sf() +\n  geom_point(data=shared_brighton, \n             aes(x=longitude, y=latitude,\n                 colour=type, shape=price_NA)) +\n  theme_map() +\n  theme(legend.position = \"none\")\n\nsp &lt;- ggplot(shared_brighton, \n             aes(x=rooms, y=price, \n                 colour=type, \n                 shape=price_NA)) +\n  geom_point() +\n  theme(legend.position=\"none\")\n\nmapi &lt;- ggplotly(map) |&gt;\n  highlight(on = \"plotly_selected\", \n              off = \"plotly_deselect\")\nspi &lt;- ggplotly(sp) |&gt;\n  highlight(on = \"plotly_selected\", \n              off = \"plotly_deselect\")\n\nbscols(\n  widths = c(5, 5), \n  mapi, \n  spi \n )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe high priced house is right against the bay.\nThe property with the large number of rooms is also close to the bay.\nOtherwise, if you select the properties near the bay, they don’t seem to be especially higher in price.\n\n\n\n\n\n\n\n\nExercise 2: Challenges\nFor each of the data sets, c1, …, c7 from the mulgar package, use the grand tour to view and try to identify structure (outliers, clusters, non-linear relationships).\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nlibrary(mulgar)\nanimate_xy(c1)\n# four small clusters, two big clusters\n# linear dependence\nanimate_xy(c2) \n# Six spherical clusters\nanimate_xy(c3)\n# tetrahedron with lots of smaller triangles,\n# barriers, linear dependence\nanimate_xy(c4) \n# Four linear connected pieces\nanimate_xy(c5)\n# Spiral in lower dimensional space\n# Non-linear and linear dependence\nanimate_xy(c6)\n# Two curved clusters\nanimate_xy(c7)\n# spherical cluster, curve cluster and a lot of noise points"
  },
  {
    "objectID": "week8/tutorialsol.html#finishing-up",
    "href": "week8/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 8",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week8/tutorial.html",
    "href": "week8/tutorial.html",
    "title": "ETC5521 Tutorial 8",
    "section": "",
    "text": "These are exercises in plots to make to explore relationships between multiple variables. You will use interactive scatterplot matrices, interactive parallel coordinate plots and tours to explore the world beyond 2D."
  },
  {
    "objectID": "week8/tutorial.html#objectives",
    "href": "week8/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 8",
    "section": "",
    "text": "These are exercises in plots to make to explore relationships between multiple variables. You will use interactive scatterplot matrices, interactive parallel coordinate plots and tours to explore the world beyond 2D."
  },
  {
    "objectID": "week8/tutorial.html#preparation",
    "href": "week8/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 8",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nThe reading for this week is Cook and Laa (2023) “Interactively exploring high-dimensional data and models in R” Chapter 1.\nComplete the weekly quiz, before the deadline!\nInstall the following R-packages if you do not have them already:\n\n\ninstall.packages(c(\"tidyverse\", \"tourr\", \"GGally\", \"plotly\", \"colorspace\", \"mulgar\", \"simputation\", \"naniar\", \"crosstalk\", \"sf\", \"ozmaps\", \"ggthemes\"))\n\n\nOpen your RStudio Project for this unit (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week8/tutorial.html#exercises",
    "href": "week8/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 8",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: Melbourne housing\n\nRead in a copy of the Melbourne housing data from Nick Tierney’s github repo which is a collation from the version at kaggle. It’s fairly large, so let’s start simply, and choose two suburbs to focus on. I recommend “South Yarra” and “Brighton”.\n\n\nmel_houses &lt;- read_csv(\"https://raw.githubusercontent.com/njtierney/melb-housing-data/master/data/housing.csv\") |&gt;\n  dplyr::filter(suburb %in% c(\"South Yarra\", \"Brighton\")) \n\n\nThere are a substantial number of missing values. These need to be handled first because examining multiple variables, with almost every method, requires complete data. Examine the missing value distribution using naniar, and strategise on how to handle the missings.\n\n\nNow implement your strategy for removing, and imputing missing values, until you have a complete data set. This can involve some iteration on the strategy, as we find that some other approach is needed.\n\n\nMake a scatterplot matrix of price, rooms, bedroom2, bathroom, landsize, latitude and longitude, suburb. The order of variables can affect the readability. I advise that the plot will be easier to read if you order the variables a little, at least put price first. Think about what associations can be seen?\n\n\nHere’s where we will start using interactive plots to explore the multivariate relationships. Subset the data to Brighton only. This will make the analysis easier. Make an interactive scatterplot matrix of price, rooms, bedroom2, bathroom and landsize, coloured by type of property. There are some high price properties. Select these cases, and determine what’s special about them – if anything.\n\n\nThe realtors mantra is location, location, location! Next check the location of properties relative to price and size, using two linked plots. One should have the longitude and latitude, and the other price by rooms. Ideally, you can make a map underneath the spatial coordinates, to better put these in context.\n\n\n\nExercise 2: Challenges\nFor each of the data sets, c1, …, c7 from the mulgar package, use the grand tour to view and try to identify structure (outliers, clusters, non-linear relationships)."
  },
  {
    "objectID": "week8/tutorial.html#finishing-up",
    "href": "week8/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 8",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week9/index.html",
    "href": "week9/index.html",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "",
    "text": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R\nAlso see tsibble documentation."
  },
  {
    "objectID": "week9/index.html#main-reference",
    "href": "week9/index.html#main-reference",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "",
    "text": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R\nAlso see tsibble documentation."
  },
  {
    "objectID": "week9/index.html#what-you-will-learn-this-week",
    "href": "week9/index.html#what-you-will-learn-this-week",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nWhat is temporal data?\nWhat is exploratory temporal data analysis?\nUsing temporal objects in R: tsibble\nData wrangling: aggregation, creating temporal components, missing values\nPlotting conventions: connect the dots; aspect ratio, landscape or portrait\nCalendar plots: arranging daily records into a calendar format\nVisual inference for temporal data\ntignostics: cognostics for temporal data\nInteractive graphics for temporal data\nExploring longitudinal data, with the brolgar package"
  },
  {
    "objectID": "week9/index.html#lecture-slides",
    "href": "week9/index.html#lecture-slides",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week9/index.html#worksheet-instructions",
    "href": "week9/index.html#worksheet-instructions",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Worksheet instructions",
    "text": "Worksheet instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week9/index.html#tutorial-instructions",
    "href": "week9/index.html#tutorial-instructions",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week9/index.html#assignments",
    "href": "week9/index.html#assignments",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Assignments",
    "text": "Assignments\n\nExercises 3 is due on Monday 22 September.\nQuiz 8 is due on Thursday 25 September."
  },
  {
    "objectID": "week9/tutorialsol.html",
    "href": "week9/tutorialsol.html",
    "title": "ETC5521 Tutorial 9",
    "section": "",
    "text": "These exercise are to do some exploratory analysis with graphics and statistical models, focusing on temporal data analysis."
  },
  {
    "objectID": "week9/tutorialsol.html#objectives",
    "href": "week9/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 9",
    "section": "",
    "text": "These exercise are to do some exploratory analysis with graphics and statistical models, focusing on temporal data analysis."
  },
  {
    "objectID": "week9/tutorialsol.html#preparation",
    "href": "week9/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 9",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nThe reading for this week is Reintroducing tsibble: data tools that melt the clock and brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R.\nComplete the weekly quiz, before the deadline!\nInstall the following R-packages if you do not have them already:\n\n\ninstall.packages(c(\"tidyverse\", \"here\", \"tsibble\", \"lubridate\", \"DAAG\", \"broom\", \"patchwork\", \"colorspace\", \"GGally\", \"tsibbledata\", \"forcats\", \"chron\", \"sugrrants\", \"brolgar\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week9/tutorialsol.html#exercises",
    "href": "week9/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 9",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: Australian rain\nThis exercise is based on one from Unwin (2015), and uses the bomregions data from the DAAG package. The data contains regional rainfall for the years 1900-2008. The regional rainfall numbers are area-weighted averages for the respective regions. Extract just the rainfall columns from the data, along with year.\n\nWhat do you think area-weighted averages are, and how would these be calculated?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe total rainfall is divided by geographic area to get the rainfall on a scale that can be compared aross different sized regions.\n\n\n\n\n\nMake line plots of the rainfall for each of the regions, the states and the Australian averages. What do you learn about rainfall patterns across the years and regions?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nWhen viewed on independent scales, there are some small differences in rainfall patterns across regions. Regions sw, se, east, mdb and states tas, vic have some decline, but mostly in last few years of the data. Region north and states nt, wa appear to have some increasing rainfall, particularly in the last few years of this data.\n\ndata(bomregions)\ntrend_plots &lt;- list()\nfor (i in 16:29) {\n  p &lt;- ggplot(bomregions, aes_string(x=\"Year\", y=colnames(bomregions)[i])) +\n  geom_point() + \n  geom_smooth(se=F) +\n  theme(aspect.ratio = 0.8)\n  trend_plots[[paste(i-15)]] &lt;- p\n}\nwrap_plots(trend_plots, ncol = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt can be difficult to assess correlation between multiple series using line plots, and the best way to check correlation between multiple series is to make a scatterplot. Make a splom for this data, ignoring year. What regions have strong positive correlation between their rainfall averages?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nIt is mostly positive linear association. Some pairs - eastRain, seRain, mdbRain, qldRain, vicRain - are strongly correlated. There are a few outliers (high values) in several regions, particularly the north.\n\nggscatmat(bomregions[, c(16:29)])\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the consequences of climate change for Australia is that some regions are likely getting drier. Make a transformation of the data to compute the difference between rainfall average in the year, and the mean over all years. Using a bar for each year, make a barchart that examines the differences in the yearly rainfall over time. (Hint: you will need to pivot the data into tidy long form to make this easier.) Are there some regions who have negative differences in recent years? What else do you notice?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nSubtracting the mean, and plotting the differences exaggerates the change in rainfall for each year. There are some regions that seem to have some increase and some with a decrease, particularly nt, north, wa, and in decline sw, vic, tas. Generally the pattern is a few wet years then a few dry years.\n\nmed_ctr &lt;- function(x, na.rm = TRUE) {\n  x-mean(x, na.rm = TRUE)\n}\nbomrain &lt;- bomregions |&gt; as_tibble() |&gt;\n  select(Year, contains(\"Rain\")) |&gt;\n  mutate_at(vars(-Year), med_ctr) |&gt;\n  pivot_longer(cols = seRain:ausRain, \n               names_to = \"area\", \n               values_to = \"rain\")\nggplot(bomrain, aes(x=Year, y=rain)) +\n  geom_col() +\n  facet_wrap(~area, ncol=5, scales=\"free_y\") +\n  theme(aspect.ratio = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Imputing missings for pedestrian sensor using a model\nSometimes imputing by a simple method such as mean or moving average doesn’t work well with multiple seasonality in a time series. Here we will use a linear model to capture the seasonality and produce better imputations for the pedestrian sensor data (from the tsibble package). This data has counts for four sensors, for two years 2015-2016.\n\nWhat are the multiple seasons of the pedestrian sensor data, for QV Market-Elizabeth St (West)? (Hint: Make a plot to check. You might filter to a single month to make it easier to see seasonality. You might also want to check when Queen Victoria Market is open.)\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThere is a daily seasonality, and an open/closed market day seasonality (Tue, Thu, Fri, Sat, Sun), and there is even a summer winter seasonality (Wednesday night market, see the double-peak in Feb/Mar).\n\nped_QV &lt;-  pedestrian |&gt;\n  mutate(year = year(Date),\n         month = month(Date)) |&gt;\n  dplyr::filter(Sensor == \"QV Market-Elizabeth St (West)\",\n                year == 2016, month %in% c(2:5))\nggplot(ped_QV) +   \n    geom_line(aes(x=Time, y=Count)) +\n    facet_calendar(~Date) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_blank(),\n        aspect.ratio = 0.5) \n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck temporal gaps for all the pedestrian sensor data. Subset to just the QV market sensor for the two years. Where are the missing values? Fill these with NA. (Note that fill_gaps doesn’t fill in the additional variables, Date, Time, and year, month so these will need to be computed after filling.)\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nhas_gaps(pedestrian, .full = TRUE)\n\n# A tibble: 4 × 2\n  Sensor                        .gaps\n  &lt;chr&gt;                         &lt;lgl&gt;\n1 Birrarung Marr                TRUE \n2 Bourke Street Mall (North)    TRUE \n3 QV Market-Elizabeth St (West) TRUE \n4 Southern Cross Station        TRUE \n\nped_gaps &lt;- pedestrian |&gt; \n  dplyr::filter(Sensor == \"QV Market-Elizabeth St (West)\") |&gt;\n  count_gaps(.full = TRUE)\nped_gaps\n\n# A tibble: 3 × 4\n  Sensor                        .from               .to                    .n\n  &lt;chr&gt;                         &lt;dttm&gt;              &lt;dttm&gt;              &lt;int&gt;\n1 QV Market-Elizabeth St (West) 2015-04-05 02:00:00 2015-04-05 02:00:00     1\n2 QV Market-Elizabeth St (West) 2015-12-31 00:00:00 2015-12-31 23:00:00    24\n3 QV Market-Elizabeth St (West) 2016-04-03 02:00:00 2016-04-03 02:00:00     1\n\nped_qvm_filled &lt;- pedestrian |&gt; \n  dplyr::filter(Sensor == \"QV Market-Elizabeth St (West)\") |&gt;\n  fill_gaps(.full = TRUE) |&gt;\n  mutate(Date = as.Date(Date_Time, tz=\"Australia/Melbourne\"),\n         Time = hour(Date_Time)) |&gt;\n  mutate(year = year(Date),\n    month = month(Date)) \n\nThere is a block of missing values on the last day of 2015. The other two missings correspond to an hour in April, when the clocks change from summer to regular time.\n\n\n\n\n\nCreate a new variable to indicate if a day is a non-working day, called hol. We need this to accurately model the differences between pedestrian patterns on working vs not working days. Make hour a factor - this helps to make a simple model for a non-standard daily pattern.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nhol &lt;- holiday_aus(2015:2016, state = \"VIC\")\nped_qvm_filled &lt;- ped_qvm_filled |&gt; \n  mutate(hol = is.weekend(Date)) |&gt;\n  mutate(hol = if_else(Date %in% hol, TRUE, hol)) |&gt;\n  mutate(Time = factor(Time))\n\n\n\n\n\n\nFit a linear model with Count as the response on predictors Time and hol interacted.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nped_qvm_lm &lt;- lm(Count~Time*hol, data=ped_qvm_filled)\n\n\n\n\n\n\nPredict the count for all the data at the sensor.\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nped_qvm_filled &lt;- ped_qvm_filled |&gt;\n  mutate(pCount = predict(ped_qvm_lm, ped_qvm_filled))\n\n\n\n\n\n\nMake a line plot focusing on the last two weeks in 2015, where there was a day of missings, where the missing counts are substituted by the model predictions. Do you think that these imputed values match the rest of the series, nicely?\n\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThis makes a much better imputed value. There’s still room for improvement but its better than a nearest neighbour, or mean or moving average imputation.\n\nped_qvm_sub &lt;- ped_qvm_filled |&gt;\n  filter(Date &gt; ymd(\"2015-12-17\"), Date &lt; ymd(\"2016-01-01\")) \nggplot(ped_qvm_sub) +   \n    geom_line(aes(x=Date_Time, y=Count)) +\n    geom_line(data=filter(ped_qvm_sub, is.na(Count)), \n                      aes(x=Date_Time, \n                          y=pCount), \n                      colour=\"seagreen3\") +\n  scale_x_datetime(\"\", date_breaks = \"1 day\", \n                   date_labels = \"%a %d\") +\n  theme(aspect.ratio = 0.15)\n\n\n\n\n\n\n\n\nCheck the daily pattern, also.\n\nggplot() +   \n    geom_line(data=filter(ped_qvm_sub), \n              aes(x=Time, y=Count, group=Date)) +\n    geom_line(data=filter(ped_qvm_sub, is.na(Count)), \n                      aes(x=Time, \n                          y=pCount, \n                          group=Date), \n                      colour=\"seagreen3\",\n                      linewidth=2) +\n  facet_wrap(~hol) +\n  theme(aspect.ratio = 1)"
  },
  {
    "objectID": "week9/tutorialsol.html#finishing-up",
    "href": "week9/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 9",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week9/tutorial.html",
    "href": "week9/tutorial.html",
    "title": "ETC5521 Tutorial 9",
    "section": "",
    "text": "These exercise are to do some exploratory analysis with graphics and statistical models, focusing on temporal data analysis."
  },
  {
    "objectID": "week9/tutorial.html#objectives",
    "href": "week9/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 9",
    "section": "",
    "text": "These exercise are to do some exploratory analysis with graphics and statistical models, focusing on temporal data analysis."
  },
  {
    "objectID": "week9/tutorial.html#preparation",
    "href": "week9/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 9",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nThe reading for this week is Reintroducing tsibble: data tools that melt the clock and brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R.\nComplete the weekly quiz, before the deadline!\nInstall the following R-packages if you do not have them already:\n\n\ninstall.packages(c(\"tidyverse\", \"here\", \"tsibble\", \"lubridate\", \"DAAG\", \"broom\", \"patchwork\", \"colorspace\", \"GGally\", \"tsibbledata\", \"forcats\", \"chron\", \"sugrrants\", \"brolgar\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week9/tutorial.html#exercises",
    "href": "week9/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 9",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: Australian rain\nThis exercise is based on one from Unwin (2015), and uses the bomregions data from the DAAG package. The data contains regional rainfall for the years 1900-2008. The regional rainfall numbers are area-weighted averages for the respective regions. Extract just the rainfall columns from the data, along with year.\n\nWhat do you think area-weighted averages are, and how would these be calculated?\n\n\nMake line plots of the rainfall for each of the regions, the states and the Australian averages. What do you learn about rainfall patterns across the years and regions?\n\n\nIt can be difficult to assess correlation between multiple series using line plots, and the best way to check correlation between multiple series is to make a scatterplot. Make a splom for this data, ignoring year. What regions have strong positive correlation between their rainfall averages?\n\n\nOne of the consequences of climate change for Australia is that some regions are likely getting drier. Make a transformation of the data to compute the difference between rainfall average in the year, and the mean over all years. Using a bar for each year, make a barchart that examines the differences in the yearly rainfall over time. (Hint: you will need to pivot the data into tidy long form to make this easier.) Are there some regions who have negative differences in recent years? What else do you notice?\n\n\n\nExercise 2: Imputing missings for pedestrian sensor using a model\nSometimes imputing by a simple method such as mean or moving average doesn’t work well with multiple seasonality in a time series. Here we will use a linear model to capture the seasonality and produce better imputations for the pedestrian sensor data (from the tsibble package). This data has counts for four sensors, for two years 2015-2016.\n\nWhat are the multiple seasons of the pedestrian sensor data, for QV Market-Elizabeth St (West)? (Hint: Make a plot to check. You might filter to a single month to make it easier to see seasonality. You might also want to check when Queen Victoria Market is open.)\n\n\nCheck temporal gaps for all the pedestrian sensor data. Subset to just the QV market sensor for the two years. Where are the missing values? Fill these with NA. (Note that fill_gaps doesn’t fill in the additional variables, Date, Time, and year, month so these will need to be computed after filling.)\n\n\nCreate a new variable to indicate if a day is a non-working day, called hol. We need this to accurately model the differences between pedestrian patterns on working vs not working days. Make hour a factor - this helps to make a simple model for a non-standard daily pattern.\n\n\nFit a linear model with Count as the response on predictors Time and hol interacted.\n\n\nPredict the count for all the data at the sensor.\n\n\nMake a line plot focusing on the last two weeks in 2015, where there was a day of missings, where the missing counts are substituted by the model predictions. Do you think that these imputed values match the rest of the series, nicely?"
  },
  {
    "objectID": "week9/tutorial.html#finishing-up",
    "href": "week9/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 9",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week9/slides.html",
    "href": "week9/slides.html",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "What is temporal data?\nWhat is exploratory temporal data analysis?\nUsing temporal objects in R: tsibble\nData wrangling: aggregation, creating temporal components, missing values\nPlotting conventions: connect the dots; aspect ratio, landscape or portrait\nCalendar plots: arranging daily records into a calendar format\nVisual inference for temporal data\ntignostics: cognostics for temporal data\nInteractive graphics for temporal data\nExploring longitudinal data, with the brolgar package"
  },
  {
    "objectID": "week10/slides.html#section",
    "href": "week10/slides.html#section",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "You show me continents, I see the islands,\nYou count the centuries, I blink my eyes\n~Björk"
  },
  {
    "objectID": "week10/slides.html#outline",
    "href": "week10/slides.html#outline",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Outline",
    "text": "Outline\n\nBreaking up data by time, and by space\nChanging focus:\n\nMaps of space over time\nExploring time over space with glyph maps\n\nInference for spatial trends\nA flash back to the 1970s: Tukey’s median polish\nWorking with spatial polygon data\n\nMaking a choropleth map\nBending the choropleth into a cartogram\nTiling spatial regions"
  },
  {
    "objectID": "week10/slides.html#approach",
    "href": "week10/slides.html#approach",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Approach",
    "text": "Approach\nWith spatiotemporaral data, you need to be able to pivot to focus on (1) time, (2) space, and (3) both together, although this latter task is harder.\nThe cubble object in the cubble R package, makes these operations easier."
  },
  {
    "objectID": "week10/slides.html#example-temperature-change",
    "href": "week10/slides.html#example-temperature-change",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Temperature change",
    "text": "Example: Temperature change\n\n\n6 years of monthly measurements of a 24x24 spatial grid from Central America collated by Paul Murrell, U. Auckland.\n\n\nRows: 41,472\nColumns: 15\n$ time        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ id          &lt;chr&gt; \"1-1\", \"1-2\", \"1-3\", \"1-4\", \"1-5\", \"1-…\n$ lat         &lt;dbl&gt; -21, -21, -21, -21, -21, -21, -21, -21…\n$ long        &lt;dbl&gt; -114, -111, -109, -106, -104, -101, -9…\n$ elevation   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ date        &lt;dttm&gt; 1995-01-01, 1995-01-01, 1995-01-01, 1…\n$ cloudlow    &lt;dbl&gt; 31, 32, 32, 39, 48, 50, 51, 52, 54, 56…\n$ cloudmid    &lt;dbl&gt; 2.0, 2.5, 3.5, 4.0, 4.5, 2.5, 4.5, 5.0…\n$ cloudhigh   &lt;dbl&gt; 0.5, 1.5, 1.5, 1.0, 0.5, 0.0, 0.0, 0.0…\n$ ozone       &lt;int&gt; 260, 260, 260, 258, 258, 258, 256, 258…\n$ pressure    &lt;int&gt; 1000, 1000, 1000, 1000, 1000, 1000, 10…\n$ surftemp    &lt;dbl&gt; 297, 297, 297, 297, 296, 296, 296, 296…\n$ temperature &lt;dbl&gt; 297, 296, 296, 296, 296, 295, 296, 295…\n$ month       &lt;ord&gt; Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan…\n$ year        &lt;int&gt; 1995, 1995, 1995, 1995, 1995, 1995, 19…\n\n\n\n\n\n\nCode\nnasa_cb &lt;- as_cubble(as_tibble(nasa), \n                     key=id, \n                     index=time, \n                     coords=c(long, lat))\nnasa_cb\n\n\n# cubble:   key: id [576], index: time, nested form\n# spatial:  [-113.75, -21.25, -56.25, 36.25], Missing CRS!\n# temporal: time [int], date [dttm], cloudlow [dbl],\n#   cloudmid [dbl], cloudhigh [dbl], ozone [int], pressure\n#   [int], surftemp [dbl], temperature [dbl], month [ord],\n#   year [int]\n   id      lat   long elevation ts                \n   &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;list&gt;            \n 1 1-1   -21.2 -114.          0 &lt;tibble [72 × 11]&gt;\n 2 1-2   -21.2 -111.          0 &lt;tibble [72 × 11]&gt;\n 3 1-3   -21.2 -109.          0 &lt;tibble [72 × 11]&gt;\n 4 1-4   -21.2 -106.          0 &lt;tibble [72 × 11]&gt;\n 5 1-5   -21.2 -104.          0 &lt;tibble [72 × 11]&gt;\n 6 1-6   -21.2 -101.          0 &lt;tibble [72 × 11]&gt;\n 7 1-7   -21.2  -98.8         0 &lt;tibble [72 × 11]&gt;\n 8 1-8   -21.2  -96.2         0 &lt;tibble [72 × 11]&gt;\n 9 1-9   -21.2  -93.8         0 &lt;tibble [72 × 11]&gt;\n10 1-10  -21.2  -91.2         0 &lt;tibble [72 × 11]&gt;\n# ℹ 566 more rows"
  },
  {
    "objectID": "week10/slides.html#spatial-and-temporal",
    "href": "week10/slides.html#spatial-and-temporal",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Spatial and temporal",
    "text": "Spatial and temporal\n\n\n\n\nCode\nggplot() + \n  geom_point(data=nasa_cb, aes(x=long, y=lat)) +\n  geom_point(data=dplyr::filter(nasa_cb, \n       id == \"5-20\"),\n       aes(x=long, y=lat),\n       colour=\"orange\", size=4) +\n  geom_point(data=dplyr::filter(nasa_cb, \n       id == \"20-2\"),\n       aes(x=long, y=lat),\n       colour=\"turquoise\", size=4)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nnasa_cb_f &lt;- nasa_cb |&gt; \n  face_temporal() \nggplot(nasa_cb_f) + \n  geom_line(aes(x=date, \n                 y=surftemp, \n                 group=id), alpha=0.2) +\n  geom_line(data=filter(nasa_cb_f , \n       id==\"5-20\"),\n       aes(x=date, \n                 y=surftemp, \n                 group=id),\n       colour=\"orange\", linewidth=2) +\n  geom_line(data=filter(nasa_cb_f , \n       id==\"20-2\"),\n       aes(x=date, \n                 y=surftemp, \n                 group=id),\n       colour=\"turquoise\", linewidth=2) +\n  theme(aspect.ratio = 0.5)"
  },
  {
    "objectID": "week10/slides.html#pre-processing-of-time-and-space",
    "href": "week10/slides.html#pre-processing-of-time-and-space",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Pre-processing of time and space",
    "text": "Pre-processing of time and space\n\n\n\nThink of time and space as ordered categorical variables.\n\n\n\nTime may need to be converted to categories.\nSpatial variable might need to be discretised, or gridded.\n\n This data is already gridded. Time is an integer from 1 to 72 (6 years of 12 months), as well as a date, and month and year. Space is a 24x24 grid of longitude and latitude, and also provided as an integer 1 to 24 in both x and y."
  },
  {
    "objectID": "week10/slides.html#focus-on-spatial-12",
    "href": "week10/slides.html#focus-on-spatial-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Focus on spatial (1/2)",
    "text": "Focus on spatial (1/2)\n\n🖼️LearnCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlice one month, and show gridded temperatures as a tiled display on spatial coordinates.\n\n\n\nIn January 2005, temperatures are\n\ncool over land in the north\ncool over the Andes in south america\nwarm on the equator, and along the coastline\n\nThere are 12*6=72 maps to make!! No problem.\n\n\n\n# Get the map\nsth_america &lt;- map_data(\"world\") |&gt;\n  filter(between(long, -115, -53), between(lat, -20.5, 41))\n\nnasa_cb |&gt; \n  face_temporal() |&gt;\n  filter(month == \"Jan\", year == 1995) |&gt;\n  select(id, time, surftemp) |&gt;\n  unfold(long, lat) |&gt;\n  ggplot() + \n  geom_tile(aes(x=long, y=lat, fill=surftemp)) +\n  geom_path(data=sth_america, \n            aes(x=long, y=lat, group=group), \n            colour=\"white\", linewidth=1) +\n  scale_fill_viridis_c(\"\", option = \"magma\") +\n  ggtitle(\"January 1995\") +\n  theme_map() +\n  theme(legend.position = \"bottom\", \n        plot.title = element_text(size = 24))"
  },
  {
    "objectID": "week10/slides.html#focus-on-spatial-22",
    "href": "week10/slides.html#focus-on-spatial-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Focus on spatial (2/2)",
    "text": "Focus on spatial (2/2)\n\n🖼️LearnCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring spatial trend over time is obtained by .monash-blue2[faceting the maps by time].\nCan you see El Nino in 1997? Can you see the summer vs winter in the different hemispheres?\n\n\n\n\nnasa_cb |&gt; face_temporal() |&gt;\n  select(id, time, month, year, surftemp) |&gt;\n  unfold(long, lat) |&gt;\n  ggplot() + \n  geom_tile(aes(x=long, y=lat, fill=surftemp)) +\n  facet_grid(year~month) +\n  scale_fill_viridis_c(\"\", option = \"magma\") +\n  theme_map() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "week10/slides.html#focus-on-temporal-14",
    "href": "week10/slides.html#focus-on-temporal-14",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Focus on temporal (1/4)",
    "text": "Focus on temporal (1/4)\n\n\n\n\nCode\nnasa_cb |&gt; face_temporal() |&gt;\n  select(id, time, month, year, surftemp) |&gt;\n  unfold(long, lat) |&gt;\n  ggplot() +\n    geom_polygon(data=sth_america, \n            aes(x=long, y=lat, group=group), \n            fill=\"#014221\", alpha=0.2, colour=\"#ffffff\") +\n    cubble::geom_glyph_box(data=nasa, \n                           aes(x_major = long, \n                               x_minor = date,\n                               y_major = lat, \n                               y_minor = surftemp), fill=NA) +\n    cubble::geom_glyph(data=nasa, \n                       aes(x_major = long, \n                           x_minor = date,\n                           y_major = lat, \n                           y_minor = surftemp)) +\n    theme_map() \n\n\n\n\n\n\n\n\n\n\n\nA glyphmap shows a (small) time series at each spatial location.\n\n\nSeveral scaling choices:\n\nglobal: overall min and max used to scale all locations\nlocal: each location scaled on it’s own min/max\n\n\n\n\nGlobal scale used here\nCan see differences in the overall magnitude, particularly north to south."
  },
  {
    "objectID": "week10/slides.html#focus-on-temporal-24",
    "href": "week10/slides.html#focus-on-temporal-24",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Focus on temporal (2/4)",
    "text": "Focus on temporal (2/4)\n\n\n\n\nCode\nnasa_cb |&gt; face_temporal() |&gt;\n  select(id, time, month, year, surftemp) |&gt;\n  unfold(long, lat) |&gt;\n  ggplot() +\n    geom_polygon(data=sth_america, \n            aes(x=long, y=lat, group=group), \n            fill=\"#014221\", alpha=0.2, colour=\"#ffffff\") +\n    cubble::geom_glyph_box(data=nasa, \n                           aes(x_major = long, \n                               x_minor = date,\n                               y_major = lat, \n                               y_minor = surftemp), fill=NA) +\n    cubble::geom_glyph(data=nasa, \n                       aes(x_major = long, \n                           x_minor = date,\n                           y_major = lat, \n                           y_minor = surftemp), \n                       global_rescale = FALSE) +\n    theme_map() \n\n\n\n\n\n\n\n\n\n\n\nNote: Local scale used, min/max for each spatial location\nEl Nino year in equatorial region may be visible.\nNotice also odd patterns on the west (Andes mountains) of South America."
  },
  {
    "objectID": "week10/slides.html#focus-on-temporal-34",
    "href": "week10/slides.html#focus-on-temporal-34",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Focus on temporal (3/4)",
    "text": "Focus on temporal (3/4)\n\n\n\n\nCode\nnasa_cb |&gt; face_temporal() |&gt;\n  select(id, time, month, year, surftemp) |&gt;\n  unfold(long, lat) |&gt;\n  ggplot() +\n    geom_polygon(data=sth_america, \n            aes(x=long, y=lat, group=group), \n            fill=\"#014221\", alpha=0.2, colour=\"#ffffff\") +\n    cubble::geom_glyph(data=nasa, \n                       aes(x_major = long, \n                           x_minor = date,\n                           y_major = lat, \n                           y_minor = surftemp), \n                       global_rescale = FALSE,\n                       polar = TRUE) +\n    theme_map() \n\n\n\n\n\n\n\n\n\n\n\nNote: Local scale used, min/max for each spatial location, and polar coordinates used"
  },
  {
    "objectID": "week10/slides.html#focus-on-temporal-44",
    "href": "week10/slides.html#focus-on-temporal-44",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Focus on temporal (4/4)",
    "text": "Focus on temporal (4/4)\n\n\n\n\nCode\nnasa_mth &lt;- nasa_cb |&gt; \n  face_temporal() |&gt;\n  select(id, time, month, year, surftemp) |&gt;\n  unfold(long, lat) |&gt;\n  as_tibble() |&gt;\n  group_by(id, month) |&gt;\n  dplyr::summarise(tmin = min(surftemp),\n            tmax = max(surftemp), \n            long = min(long),\n            lat = min(lat)) |&gt;\n  ungroup() |&gt;\n  mutate(month = as.numeric(month))\nggplot() +\n    geom_polygon(data=sth_america, \n            aes(x=long, y=lat, group=group), \n            fill=\"#014221\", alpha=0.2, colour=\"#ffffff\") +\n    geom_glyph_ribbon(data = nasa_mth, \n                      aes(x_major = long, \n                          x_minor = month,\n                          y_major = lat, \n                          ymin_minor = tmin,\n                          ymax_minor = tmax), \n                          width = 2) +\n    theme_map() \n\n\n\n\n\n\n\n\n\n\nSeasonality can be the focus in the glyphs.\nHere monthly min and max temperature over the 6 years is shown, as ribbon glyphs."
  },
  {
    "objectID": "week10/slides.html#adding-interaction",
    "href": "week10/slides.html#adding-interaction",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Adding interaction",
    "text": "Adding interaction\n\n\nDEMO\nlibrary(tsibble)\nlibrary(tsibbletalk)\nlibrary(lubridate)\nlibrary(plotly)\nnasa_shared &lt;- nasa |&gt; \n  mutate(date = ymd(date)) |&gt;\n  select(long, lat, date, surftemp, id) |&gt;\n  as_tsibble(index=date, key=id) |&gt;\n  as_shared_tsibble()\np1 &lt;- ggplot() +\n  geom_polygon(data=sth_america, \n            aes(x=long, y=lat, group=group), \n            colour=\"#ffffff\", alpha=0.2, fill=\"#014221\") +\n  geom_point(data=nasa_shared, aes(x = long, \n         y = lat, group = id)) \np2 &lt;- nasa_shared |&gt;\n  ggplot(aes(x = date, y = surftemp)) +\n  geom_line(aes(group = id), alpha = 0.5) \nsubplot(\n    ggplotly(p1, tooltip = \"Region\"),\n    ggplotly(p2, tooltip = \"Region\"),\n    nrows = 1, widths=c(0.3, 0.7)) |&gt;\n  highlight(dynamic = TRUE)"
  },
  {
    "objectID": "week10/slides.html#inference-for-spatial-trend",
    "href": "week10/slides.html#inference-for-spatial-trend",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Inference for spatial trend",
    "text": "Inference for spatial trend\n\n\n\n\ngenerate-data\n# Set up a simple example\nset.seed(945)\nx &lt;- 1:24\ny &lt;- 1:24\nxy &lt;- expand.grid(x, y)\nd &lt;- tibble(x=xy$Var1, y=xy$Var2) |&gt;\n  mutate(v = x+2*y) \nd_sf &lt;- SpatialPointsDataFrame(d[,1:2],\n                   data.frame(d[,3]))\nvgm_mod &lt;- vgm(psill=5, model = \"Sph\", range=20, nmax=30)\nd_dummy &lt;- gstat(formula = v~1, dummy=TRUE, beta=0,\n           model=vgm_mod)\nd_err &lt;- predict(d_dummy, d_sf, nsim=1)\nd &lt;- d |&gt;\n  mutate(e = d_err@data$sim1*3) |&gt;\n  mutate(ve = v+e)\n\n\n\n\nplot\nobs &lt;- ggplot(d, aes(x, y, fill = ve)) +\n  geom_tile() +\n  scale_fill_viridis_c(\"\") +\n  theme(aspect.ratio = 1) +\n  ggtitle(\"Observed\") +\n  theme(legend.position = \"none\",\n              axis.text = element_blank(),\n              axis.title = element_blank())\ntrend &lt;- ggplot(d, aes(x, y, fill = v)) +\n  geom_tile() +\n  scale_fill_viridis_c(\"\", option = \"magma\") +\n  theme(aspect.ratio = 1) +\n  ggtitle(\"Trend\") +\n  theme(legend.position = \"none\",\n              axis.text = element_blank(),\n              axis.title = element_blank())\nerr &lt;- ggplot(d, aes(x, y, fill = e)) +\n  geom_tile() +\n  scale_fill_distiller(\"\", palette = \"PRGn\") +\n  theme(aspect.ratio = 1) +\n  ggtitle(\"Residual\") +\n  theme(legend.position = \"none\",\n              axis.text = element_blank(),\n              axis.title = element_blank())\nobs + trend + err + plot_layout(ncol=3)\n\n\n\n\n\n\n\n\n\n\n\nGenerate nulls by simulating from the spatial dependence model\n\n\ngenerate-nulls\nset.seed(953)\nd_null &lt;- predict(d_dummy, d_sf, nsim=5)\npos &lt;- sample(1:6, 1)\nlineup_plots &lt;- list()\nj &lt;- 1\nfor (i in 1:6) {\n  if (pos == i) { # plot data\n    p &lt;- ggplot(d, aes(x, y, fill = scale(ve))) +\n           geom_tile() \n  } \n  else { # plot nulls\n    null_df &lt;- tibble(x=d$x, y=d$y, v=d_null@data[,j])\n    p &lt;- ggplot(null_df, aes(x, y, fill = scale(v))) +\n           geom_tile() \n   j &lt;- j + 1\n  }\n  p &lt;- p +\n        scale_fill_viridis_c(\"\", option = \"magma\") +\n        theme(legend.position = \"none\",\n              axis.text = element_blank(),\n              axis.title = element_blank())\n    \n  lineup_plots[[paste(i)]] &lt;- p\n}\nwrap_plots(lineup_plots, ncol = 3)"
  },
  {
    "objectID": "week10/slides.html#extracting-spatial-trends",
    "href": "week10/slides.html#extracting-spatial-trends",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Extracting spatial trends",
    "text": "Extracting spatial trends\n\n\n\n\nplot\nlong &lt;- ggplot(d, aes(x, y=ve)) +\n  geom_point() +\n  geom_smooth(se=F) +\n  ylab(\"obs\") +\n  theme(aspect.ratio = 1)\nlat &lt;- ggplot(d, aes(y, y=ve)) +\n  geom_point() +\n  ylab(\"obs\") +\n  geom_smooth(se=F) +\n  theme(aspect.ratio = 1)\nlong + lat + plot_layout(ncol=2)\n\n\n\n\n\n\n\n\n\n\nA flash back to the 1970s: Tukey’s median polish\nThis is a useful data scratching technique, particularly for spatial data, to remove complicated trends, as long as they are in spatial marginals.\nThe median polish is designed for two-way tables. Gridded spatial data is a form of two-way table."
  },
  {
    "objectID": "week10/slides.html#median-polish-12",
    "href": "week10/slides.html#median-polish-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Median polish (1/2)",
    "text": "Median polish (1/2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompute overall median and residual table.\nCompute the row medians.\nCreate a new residual table from the row medians.\nCompute the column medians.\nCreate a new residual table from the column medians.\nSecond iteration – row effects.\nSecond iteration – column effects\nIterate through steps 2-5 until row and column effect medians are close to 0."
  },
  {
    "objectID": "week10/slides.html#median-polish-22",
    "href": "week10/slides.html#median-polish-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Median polish (2/2)",
    "text": "Median polish (2/2)\n\n\nColumn and row effects from median polish\n\n\nCode\nlibrary(tukeyedar)\nd_pol &lt;- eda_pol(d, row = x, col = y, val = ve, plot=FALSE)\nlong &lt;- ggplot(d, aes(x, y=ve)) +\n  geom_point() +\n  geom_smooth(se=F) +\n  geom_point(data = d_pol$row, aes(x=x, \n                       y=effect + d_pol$global), \n    colour = \"#D93F00\", size=3) +\n  ylab(\"obs\") +\n  theme(aspect.ratio = 1)\nlat &lt;- ggplot(d, aes(y, y=ve)) +\n  geom_point() +\n  ylab(\"obs\") +\n  geom_smooth(se=F) +\n  geom_point(data = d_pol$col, aes(x=y, \n                      y=effect + d_pol$global), \n    colour = \"#D93F00\", size=3) +\n  theme(aspect.ratio = 1)\nlong + lat + plot_layout(ncol=2)\n\n\n\n\n\n\n\n\n\n\nResulting in the residuals as:\n\n\nCode\npol_res &lt;- ggplot(d_pol$long, aes(x, y, fill = ve)) +\n  geom_tile() +\n  scale_fill_distiller(\"\", palette = \"PRGn\") +\n  theme(aspect.ratio = 1) +\n  ggtitle(\"Polish Residuals\") +\n  theme(legend.position = \"none\",\n              axis.text = element_blank(),\n              axis.title = element_blank())\nerr &lt;- err +\n  theme(legend.position = \"none\",\n              axis.text = element_blank(),\n              axis.title = element_blank()) \nerr + pol_res + plot_layout(ncol=2)"
  },
  {
    "objectID": "week10/slides.html#spatial-data-needs-maps",
    "href": "week10/slides.html#spatial-data-needs-maps",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Spatial data needs maps",
    "text": "Spatial data needs maps"
  },
  {
    "objectID": "week10/slides.html#spatial-polygon-data",
    "href": "week10/slides.html#spatial-polygon-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Spatial polygon data",
    "text": "Spatial polygon data"
  },
  {
    "objectID": "week10/slides.html#spatial-polygon-data-1",
    "href": "week10/slides.html#spatial-polygon-data-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Spatial polygon data",
    "text": "Spatial polygon data\n\n\nCode\noz &lt;- world_map |&gt; \n  filter(region == \"Australia\") |&gt;\n  filter(lat &gt; -50)\nm1 &lt;- ggplot(oz, aes(x = long, y = lat)) + \n  geom_point(size=0.2) + #&lt;&lt;\n  coord_map() +\n  ggtitle(\"Points\")\nm2 &lt;- ggplot(oz, aes(x = long, y = lat, \n               group = group)) + #&lt;&lt;\n  geom_path() + #&lt;&lt;\n  coord_map() +\n  ggtitle(\"Path\")\nm3 &lt;- ggplot(oz, aes(x = long, y = lat, \n               group = group)) + #&lt;&lt;\n  geom_polygon(fill = \"#607848\", colour = \"#184848\") + #&lt;&lt;\n  coord_map() +\n  ggtitle(\"Filled polygon\")\nm1 + m2 + m3\n\n\n\n\nSpatial polygon data, includes measured values (variables) associated with a spatial polygon."
  },
  {
    "objectID": "week10/slides.html#spatial-polygon-data-2",
    "href": "week10/slides.html#spatial-polygon-data-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Spatial polygon data",
    "text": "Spatial polygon data\n\nSTEP 1: Thin your map!\n\nMost spatial polygon data is large, with high resolution on the polygons.\nThis makes them SLOW to plot.\nFor data analysis needs fast plotting, and resolution can be smaller.\nrmapshaper::ms_simplify() is the best function."
  },
  {
    "objectID": "week10/slides.html#sf-simple-spatial-polygon-objects-in-r",
    "href": "week10/slides.html#sf-simple-spatial-polygon-objects-in-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "sf: Simple spatial polygon objects in R",
    "text": "sf: Simple spatial polygon objects in R\n\n\n\n Has a coordinate system (projection), and bounding box. Supports technically accurate distance calculations between coordinates (on a sphere)."
  },
  {
    "objectID": "week10/slides.html#choropleth-maps",
    "href": "week10/slides.html#choropleth-maps",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Choropleth maps",
    "text": "Choropleth maps\n\n\nA choropleth map is used to show a measured variable associated with a political or geographic region. Polygons for the region are filled with colour.\nThe purpose is to examine the spatial distribution of a variable.\n\n\nCode\nsa2 &lt;- strayr::read_absmap(\"sa22011\") |&gt; \n  filter(!st_is_empty(geometry)) |&gt; \n  filter(!state_name_2011 == \"Other Territories\") |&gt; \n  filter(!sa2_name_2011 == \"Lord Howe Island\")\nsa2 &lt;- sa2 |&gt; rmapshaper::ms_simplify(keep = 0.5, keep_shapes = TRUE) # Simplify the map!!!\nSIR &lt;- read_csv(here::here(\"data/SIR Downloadable Data.csv\")) |&gt; \n  filter(SA2_name %in% sa2$sa2_name_2011) |&gt; \n  dplyr::select(Cancer_name, SA2_name, Sex_name, p50) |&gt; \n  filter(Cancer_name == \"Thyroid\", Sex_name == \"Females\")\nERP &lt;- read_csv(here::here(\"data/ERP.csv\")) |&gt;\n  filter(REGIONTYPE == \"SA2\", Time == 2011, Region %in% SIR$SA2_name) |&gt; \n  dplyr::select(Region, Value)\n# Alternative maps\n# Join with sa2 sf object\nsa2thyroid_ERP &lt;- SIR |&gt; \n  left_join(sa2, ., by = c(\"sa2_name_2011\" = \"SA2_name\")) |&gt;\n  left_join(., ERP |&gt; \n              dplyr::select(Region, \n              Population = Value), by = c(\"sa2_name_2011\"= \"Region\")) |&gt; \n  filter(!st_is_empty(geometry))\nsa2thyroid_ERP &lt;- sa2thyroid_ERP |&gt; \n  #filter(!is.na(Population)) |&gt; \n  filter(!sa2_name_2011 == \"Lord Howe Island\") |&gt; \n  mutate(SIR = map_chr(p50, aus_colours)) |&gt; \n  st_as_sf() \nsave(sa2, file=\"data/sa2.rda\")\nsave(sa2thyroid_ERP, file=\"data/sa2thyroid_ERP.rda\")\n\n\n\n\n\nCode\n# Plot the choropleth\nload(\"../data/sa2thyroid_ERP.rda\")\naus_ggchoro &lt;- ggplot(sa2thyroid_ERP) + \n  geom_sf(aes(fill = SIR), size = 0.1) + \n  scale_fill_identity() + invthm\naus_ggchoro"
  },
  {
    "objectID": "week10/slides.html#the-problem-with-choropleth-maps",
    "href": "week10/slides.html#the-problem-with-choropleth-maps",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "The problem with choropleth maps",
    "text": "The problem with choropleth maps\nThe problem is that high density population areas may be very small geographically. They can disappear in a choropleth map, which means that we get a biased sense of the spatial distribution of a variable."
  },
  {
    "objectID": "week10/slides.html#cartograms",
    "href": "week10/slides.html#cartograms",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Cartograms",
    "text": "Cartograms\n\n\nA cartogram transforms the geographic shape to match the value of a statistic or the population. Its a useful exploratory technique for examining the spatial distribution of a measured variable.\n  BUT they don’t work for Australia.\n\n\n\nCode\n# transform to NAD83 / UTM zone 16N\nnc &lt;- nc |&gt;\n  mutate(lBIR79 = log(BIR79))\nnc_utm &lt;- st_transform(nc, 26916)\n\norig &lt;- ggplot(nc) + \n  geom_sf(aes(fill = lBIR79)) +\n  ggtitle(\"original\") +\n  theme_map() +\n  theme(legend.position = \"none\")\n\nnc_utm_carto &lt;- cartogram_cont(nc_utm, weight = \"BIR74\", itermax = 5)\n\ncarto &lt;- ggplot(nc_utm_carto) + \n  geom_sf(aes(fill = lBIR79)) +\n  ggtitle(\"cartogram\") +\n  theme_map() +\n  theme(legend.position = \"none\")\n\nnc_utm_dorl &lt;- cartogram_dorling(nc_utm, weight = \"BIR74\")\n\ndorl &lt;- ggplot(nc_utm_dorl) + \n  geom_sf(aes(fill = lBIR79)) +\n  ggtitle(\"dorling\") +\n  theme_map() +\n  theme(legend.position = \"none\")\n\norig + carto + dorl + plot_layout(ncol=1)"
  },
  {
    "objectID": "week10/slides.html#hexagon-tile",
    "href": "week10/slides.html#hexagon-tile",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hexagon tile",
    "text": "Hexagon tile\n\n\nA hexagon tile map represents every spatial polygon with an equal sized hexagon. In dense areas these will be tesselated, but separated hexagons are placed at centroids of the remote spatial regions.\n\nIt’s not perfect, but now the higher incidence in Perth suburbs, some melbourne suburbs, and Sydney are more visible.\n\n\n\nCode\nif (!file.exists(here::here(\"data/aus_hexmap.rda\"))) {\n  \n## Create centroids set\ncentroids &lt;- sa2 |&gt; \n  create_centroids(., \"sa2_name_2011\")\n## Create hexagon grid\ngrid &lt;- create_grid(centroids = centroids,\n                    hex_size = 0.2,\n                    buffer_dist = 5)\n## Allocate polygon centroids to hexagon grid points\naus_hexmap &lt;- allocate(\n  centroids = centroids,\n  hex_grid = grid,\n  sf_id = \"sa2_name_2011\",\n  ## same column used in create_centroids\n  hex_size = 0.2,\n  ## same size used in create_grid\n  hex_filter = 10,\n  focal_points = capital_cities,\n  width = 35,\n  verbose = FALSE\n)\nsave(aus_hexmap, \n     file = here::here(\"data/aus_hexmap.rda\")) \n}\n\nload(here::here(\"data/aus_hexmap.rda\"))\n## Prepare to plot\nfort_hex &lt;- fortify_hexagon(data = aus_hexmap,\n                            sf_id = \"sa2_name_2011\",\n                            hex_size = 0.2) |&gt; \n            left_join(sa2thyroid_ERP |&gt; select(sa2_name_2011, SIR, p50))\n## Make a plot\naus_hexmap_plot &lt;- ggplot() +\n  geom_sf(data=sa2thyroid_ERP, fill=NA, colour=\"grey60\", size=0.1) +\n  geom_polygon(data = fort_hex, aes(x = long, y = lat, group = hex_id, fill = SIR)) +\n  scale_fill_identity() +\n  invthm \naus_hexmap_plot"
  },
  {
    "objectID": "week10/slides.html#resources",
    "href": "week10/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\ncubble: A Vector Spatio-Temporal Data Structure for Data Analysis\nsf: Simple Features for R\nHealy (2018) Data Visualization\nVisualising spatial data using R\nKobakian et al Hexagon tile map\nWikle, Zammit-Mangion, Cressie (2018) Spatio-Temporal Statistics with R\nMoraga, Paula. (2019). Geospatial Health Data"
  },
  {
    "objectID": "week10/index.html#worksheet-instructions",
    "href": "week10/index.html#worksheet-instructions",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Worksheet instructions",
    "text": "Worksheet instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week10/tutorial.html",
    "href": "week10/tutorial.html",
    "title": "ETC5521 Tutorial 10",
    "section": "",
    "text": "This tutorial practices rearranging spatiotemporal data to focus on spatial or temporal patterns, and constructing choropleth maps and cartograms."
  },
  {
    "objectID": "week10/tutorial.html#objectives",
    "href": "week10/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 10",
    "section": "",
    "text": "This tutorial practices rearranging spatiotemporal data to focus on spatial or temporal patterns, and constructing choropleth maps and cartograms."
  },
  {
    "objectID": "week10/tutorial.html#preparation",
    "href": "week10/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 10",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nResources for this week is Moraga (2019) Spatial data and R packages for mapping; cubble: A Vector Spatio-Temporal Data Structure for Data Analysis; Making maps plot faster Simplify spatial polygons; sf: Simple Features for R.\nComplete the weekly quiz, before the deadline!\nInstall the following R-packages if you do not have them already:\n\n\ninstall.packages(c(\"tidyverse\",\"here\",\"lubridate\",\"GGally\",\"tsibble\",\"cubble\",\"forcats\",\"cartogram\",\"sf\",\"cartogram\",\"patchwork\",\"ggthemes\", \"sugarbag\", \"viridis\", \"rmapshaper\"))\nremotes::install_github(\"runapp-aus/strayr\")\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week10/tutorial.html#exercises",
    "href": "week10/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 10",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: Melbourne Covid-19 outbreak\nIn Melbourne we were in a strict lockdown for much of 2020, and large chunks of 2021. Each week we got our hopes up that restrictions might be eased, and once again these hopes were dashed by announcements each week, keeping the restrictions a little longer. The data we have collected here are the case counts by Victorian local government area (LGA) since the beginning of July, 2020. We will examine the spatiotemporal distribution of these counts.\nWorking with spatial data is always painful! It almost always requires some ugly code.\n\nPart of the reason for the difficulty is the use of special data objects, that describe maps. There are several different choices, and some packages and tools use one, and others use another, so not all tools work together. The sf package helps enormously, but when you run into errors it can be hard to debug.\nAnother reason is that map objects can be very large, which makes sense for accurate mapping, but for data analysis and visualisation, we’d rather have smaller, even if slightly inaccurate, spatial objects. It is virtually always necessary to thin out map data before doing further analysis - you need special tools for this, eg mapshapr. We don’t really need this for the exercises here, because the strayr version of the LGAs is already thinned.\nAnother problem commonly encountered is that there are numerous coordinate systems, and types of projections of the 3D globe into a 2D canvas. We have become accustomed to lat/long but like time its an awkward scale to compute on because a translation from E/W and N/S to positive and negative values is needed. More commonly a Universal Transverse Mercator (UTM) is the standard but its far less intuitive to use.\n\nAnd yet another reason is that keys linking data tables and spatial tables may not match perfectly because there are often synonyms or slightly different name preferences between different data collectors.\n\nThe code for all the analysis is provided for you in the solution. We recommend that you run the code in steps to see what it is doing, why the mutating and text manipulations are necessary. Talk about the code with each other to help you understand it.\n\na. Read case counts for 2020\nThe file melb_lga_covid.csv contains the cases by LGA. Read the data in and inspect result. You should find that some variables are type chr because “null” has been used to code entries on some days. This needs fixing, and also missings should be converted to 0. Why does it make sense to substitute missings with 0, here?\n\n\nb. Check the data\nCheck the case counts to learn whether they are daily or cumulative. The best way to do this is select one suburb where there were substantial cases, and make a time series. If the counts are cumulative, calculate the daily counts, and re-check the temporal trend for your chosen LGA. Describe the temporal trend, and any visible artifacts.\n\n\nc. Spatial polygons size\nNow let’s get polygon data of Victorian LGAs using the strayr package. The map is already fairly small, so it doesn’t need any more thinning, but we’ll look at how thinning works.\nGet a copy of the lga2018 using strayr::read_absmap(). Save the resulting data as an .rda file, and plot the map.\nNow run rmapshaper::ms_simplify(), saving it as a different object. Save the object as an .rda file, and plot the map.\nWhat is the difference in file size before and after thinning. Can you see a difference in the map?\n\n\nc. Spatial polygons matching\nNow let’s match polygon data of Victorian LGAs to the COVID counts. The cubble::check_key() can be used to check if the keys match between spatial and temporal data sets.\nYou will find that we need to fix some names of LGAs, even though cubble does a pretty good job working out which are supposed to match.\n\n\ne. Choropleth map\nSum the counts over the time period for each LGA, merge the COVID data with the map polygons (LGA) and create a choropleth map. The LGA data is an sf object so the geom_sf will automatically grab the geometry from the object to make the spatial polygons. Where was the highest COVID incidence?\n\n\nf. Cartogram\nTo make a population-transformed polygon we need to get population data for each LGA. The file VIF2019_Population_Service_Ages_LGA_2036.xlsx has been extracted from the Vic Gov web site. It is a complicated xlsx file, with the data in sheet 3, and starting 13 rows down. The readxl package is handy here to extract the population data needed. You’ll need to join the population counts to the map data to make a cartogram. Once you have the transformed polygon data, the same plotting code can be used, as created the choropleth map.\n\n\ng. Hexagon tile map\nUse the provided code to make a hexgon tile map, with functions from the sugarbag package. Is it easier to see the spatial distribution of incidence from the hexagon tile map, or the choropleth or the cartogram?"
  },
  {
    "objectID": "week10/tutorial.html#finishing-up",
    "href": "week10/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 10",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week10/tutorialsol.html",
    "href": "week10/tutorialsol.html",
    "title": "ETC5521 Tutorial 10",
    "section": "",
    "text": "This tutorial practices rearranging spatiotemporal data to focus on spatial or temporal patterns, and constructing choropleth maps and cartograms."
  },
  {
    "objectID": "week10/tutorialsol.html#objectives",
    "href": "week10/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 10",
    "section": "",
    "text": "This tutorial practices rearranging spatiotemporal data to focus on spatial or temporal patterns, and constructing choropleth maps and cartograms."
  },
  {
    "objectID": "week10/tutorialsol.html#preparation",
    "href": "week10/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 10",
    "section": "🔧 Preparation",
    "text": "🔧 Preparation\n\nResources for this week is Moraga (2019) Spatial data and R packages for mapping; cubble: A Vector Spatio-Temporal Data Structure for Data Analysis; Making maps plot faster Simplify spatial polygons; sf: Simple Features for R.\nComplete the weekly quiz, before the deadline!\nInstall the following R-packages if you do not have them already:\n\n\ninstall.packages(c(\"tidyverse\",\"here\",\"lubridate\",\"GGally\",\"tsibble\",\"cubble\",\"forcats\",\"cartogram\",\"sf\",\"cartogram\",\"patchwork\",\"ggthemes\", \"sugarbag\", \"viridis\", \"rmapshaper\"))\nremotes::install_github(\"runapp-aus/strayr\")\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week10/tutorialsol.html#exercises",
    "href": "week10/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 10",
    "section": "📥 Exercises",
    "text": "📥 Exercises\n\nExercise 1: Melbourne Covid-19 outbreak\nIn Melbourne we were in a strict lockdown for much of 2020, and large chunks of 2021. Each week we got our hopes up that restrictions might be eased, and once again these hopes were dashed by announcements each week, keeping the restrictions a little longer. The data we have collected here are the case counts by Victorian local government area (LGA) since the beginning of July, 2020. We will examine the spatiotemporal distribution of these counts.\nWorking with spatial data is always painful! It almost always requires some ugly code.\n\nPart of the reason for the difficulty is the use of special data objects, that describe maps. There are several different choices, and some packages and tools use one, and others use another, so not all tools work together. The sf package helps enormously, but when you run into errors it can be hard to debug.\nAnother reason is that map objects can be very large, which makes sense for accurate mapping, but for data analysis and visualisation, we’d rather have smaller, even if slightly inaccurate, spatial objects. It is virtually always necessary to thin out map data before doing further analysis - you need special tools for this, eg mapshapr. We don’t really need this for the exercises here, because the strayr version of the LGAs is already thinned.\nAnother problem commonly encountered is that there are numerous coordinate systems, and types of projections of the 3D globe into a 2D canvas. We have become accustomed to lat/long but like time its an awkward scale to compute on because a translation from E/W and N/S to positive and negative values is needed. More commonly a Universal Transverse Mercator (UTM) is the standard but its far less intuitive to use.\n\nAnd yet another reason is that keys linking data tables and spatial tables may not match perfectly because there are often synonyms or slightly different name preferences between different data collectors.\n\nThe code for all the analysis is provided for you in the solution. We recommend that you run the code in steps to see what it is doing, why the mutating and text manipulations are necessary. Talk about the code with each other to help you understand it.\n\na. Read case counts for 2020\nThe file melb_lga_covid.csv contains the cases by LGA. Read the data in and inspect result. You should find that some variables are type chr because “null” has been used to code entries on some days. This needs fixing, and also missings should be converted to 0. Why does it make sense to substitute missings with 0, here?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nNAs really have to be 0s. Its likely that the cells were left blank when numbers were recorded, left blank because there were no cases that day.\n\n# Read the data\n# Replace null with 0, for three LGAs\n# Convert to long form to join with polygons\n# Make the date variables a proper date\n# Set NAs to 0, this is a reasonable assumption\ncovid &lt;- read_csv(\"https://raw.githubusercontent.com/numbats/ddde/master/data/melb_lga_covid.csv\") |&gt;\n  mutate(Buloke = as.numeric(ifelse(Buloke == \"null\", \"0\", Buloke))) |&gt;\n   mutate(Hindmarsh = as.numeric(ifelse(Hindmarsh == \"null\", \"0\", Hindmarsh))) |&gt;\n   mutate(Towong = as.numeric(ifelse(Towong == \"null\", \"0\", Towong))) |&gt;\n  pivot_longer(cols = Alpine:Yarriambiack, names_to=\"NAME\", values_to=\"cases\") |&gt;\n  mutate(Date = ydm(paste0(\"2020/\",Date))) |&gt;\n  mutate(cases=replace_na(cases, 0))\n\n\n\n\n\n\n\nb. Check the data\nCheck the case counts to learn whether they are daily or cumulative. The best way to do this is select one suburb where there were substantial cases, and make a time series. If the counts are cumulative, calculate the daily counts, and re-check the temporal trend for your chosen LGA. Describe the temporal trend, and any visible artifacts.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThis is cumulative data. Take the most recent case count as the value to use. If you wanted to explore the temporal trend, you would need to take lags to compute the differences between days. This is interesting because it also generated some negative values!\n\n# Check the case counts\ncovid |&gt; filter(NAME == \"Brimbank\") |&gt;\n  ggplot(aes(x=Date, y=cases)) +\n    geom_point()\n\n\n\n\n\n\n\n# Case counts are cumulative, so take lags to get daily case counts\ncovid &lt;- covid |&gt;\n  group_by(NAME) |&gt;\n  mutate(new_cases = cases - dplyr::lag(cases)) |&gt;\n  na.omit()\n\n# Check the case counts\ncovid |&gt; filter(NAME == \"Brimbank\") |&gt;\n  ggplot(aes(x=Date, y=new_cases)) +\n    geom_col() \n\n\n\n\n\n\n\n# Only keep the latest date information\ncovid_tot &lt;- covid |&gt;\n  filter(Date == max(Date))\n# Double-check we have all LGAs\n# covid |&gt; count(NAME)\n\n\n\n\n\n\n\nc. Spatial polygons size\nNow let’s get polygon data of Victorian LGAs using the strayr package. The map is already fairly small, so it doesn’t need any more thinning, but we’ll look at how thinning works.\nGet a copy of the lga2018 using strayr::read_absmap(). Save the resulting data as an .rda file, and plot the map.\nNow run rmapshaper::ms_simplify(), saving it as a different object. Save the object as an .rda file, and plot the map.\nWhat is the difference in file size before and after thinning. Can you see a difference in the map?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nBefore thinning the file is 523 KB, and after thinning it is just 33 KB.\nThe thinning has removed some small islands, and smoother the boundaries between LGAs.\n\n# Read the LGA data from strayr package. \n# This has LGAs for all of Australia. \n# Need to filter out Victoria LGAs, avoiding LGAs \n# from other states with same name, and make the names\n# match covid data names. The regex equation is\n# removing () state and LGA type text strings\n# Good reference: https://r-spatial.github.io/sf/articles/sf1.html\nlga &lt;- strayr::read_absmap(\"lga2018\") |&gt;\n  rename(lga = lga_name_2018) |&gt;\n  filter(state_name_2016 == \"Victoria\") \nsave(lga, file=\"data/lga.rda\")\nggplot(lga) + geom_sf() + theme_map()\n\nlga_sm &lt;- ms_simplify(lga)\nsave(lga_sm, file=\"data/lga_sm.rda\")\nggplot(lga_sm) + geom_sf() + theme_map()\n\n\n\n\n\n\n\nc. Spatial polygons matching\nNow let’s match polygon data of Victorian LGAs to the COVID counts. The cubble::check_key() can be used to check if the keys match between spatial and temporal data sets.\nYou will find that we need to fix some names of LGAs, even though cubble does a pretty good job working out which are supposed to match.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\n\nload(\"../data/lga.rda\")\n\n# Turn the full covid data into a tsibble\n\ncovid &lt;- covid |&gt;\n  select(-cases) |&gt;\n  rename(lga = NAME, date=Date, cases = new_cases) \ncovid_ts &lt;- as_tsibble(covid, key=lga, index=date)\n\n# Check the name matching using cubble function, check_key\ncovid_matching &lt;- check_key(spatial = lga, temporal = covid_ts)\ncovid_matching\n\n$paired\n# A tibble: 0 × 0\n\n$potential_pairs\n# A tibble: 78 × 2\n   spatial        temporal  \n   &lt;chr&gt;          &lt;chr&gt;     \n 1 Alpine (S)     Alpine    \n 2 Ararat (RC)    Ararat    \n 3 Ballarat (C)   Ballarat  \n 4 Banyule (C)    Banyule   \n 5 Bass Coast (S) Bass Coast\n 6 Baw Baw (S)    Baw Baw   \n 7 Bayside (C)    Bayside   \n 8 Benalla (RC)   Benalla   \n 9 Boroondara (C) Boroondara\n10 Brimbank (C)   Brimbank  \n# ℹ 68 more rows\n\n$others\n$others$spatial\n[1] \"Colac-Otway (S)\"                       \n[2] \"Unincorporated Vic\"                    \n[3] \"No usual address (Vic.)\"               \n[4] \"Migratory - Offshore - Shipping (Vic.)\"\n\n$others$temporal\n[1] \"Colac Otway\"\n\n\nattr(,\"class\")\n[1] \"key_tbl\" \"list\"   \n\n# Fix matching\nlga &lt;- lga |&gt; \n  mutate(lga = ifelse(lga == \"Colac-Otway (S)\", \"Colac Otway (S)\", lga)) |&gt;\n  filter(!(lga %in% covid_matching$others$spatial)) |&gt;\n  mutate(lga = str_replace(lga, \" \\\\(.+\\\\)\", \"\")) # Remove (.)\n\n# Re-do the name matching\ncovid_matching &lt;- check_key(spatial = lga, temporal = covid_ts)\ncovid_matching\n\n$paired\n# A tibble: 79 × 2\n   spatial    temporal  \n   &lt;chr&gt;      &lt;chr&gt;     \n 1 Alpine     Alpine    \n 2 Ararat     Ararat    \n 3 Ballarat   Ballarat  \n 4 Banyule    Banyule   \n 5 Bass Coast Bass Coast\n 6 Baw Baw    Baw Baw   \n 7 Bayside    Bayside   \n 8 Benalla    Benalla   \n 9 Boroondara Boroondara\n10 Brimbank   Brimbank  \n# ℹ 69 more rows\n\n$potential_pairs\n# A tibble: 0 × 0\n\n$others\n$others$temporal\ncharacter(0)\n\n$others$spatial\ncharacter(0)\n\n\nattr(,\"class\")\n[1] \"key_tbl\" \"list\"   \n\n# Join covid cases to spatial polygons\ncovid_tot &lt;- covid_tot |&gt;\n  rename(lga = NAME)\ncovid_lga &lt;- left_join(lga, covid_tot) |&gt;\n  st_as_sf()\n\n\n\n\n\n\n\ne. Choropleth map\nSum the counts over the time period for each LGA, merge the COVID data with the map polygons (LGA) and create a choropleth map. The LGA data is an sf object so the geom_sf will automatically grab the geometry from the object to make the spatial polygons. Where was the highest COVID incidence?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe high count LGAs are all in Melbourne, mostly in the western suburbs.\n\n# Make choropleth map, with appropriate colour palette\nggplot(covid_lga) + \n  geom_sf(aes(fill = cases, label=lga), colour=\"white\") + \n  scale_fill_distiller(\"Cases\", palette = \"YlOrRd\",\n                       direction=1) + \n  theme_map() +\n  theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n# Make it interactive\n# plotly::ggplotly() \n\n\n\n\n\n\n\nf. Cartogram\nTo make a population-transformed polygon we need to get population data for each LGA. The file VIF2019_Population_Service_Ages_LGA_2036.xlsx has been extracted from the Vic Gov web site. It is a complicated xlsx file, with the data in sheet 3, and starting 13 rows down. The readxl package is handy here to extract the population data needed. You’ll need to join the population counts to the map data to make a cartogram. Once you have the transformed polygon data, the same plotting code can be used, as created the choropleth map.\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nInterestingly, the population of the LGAs is quite different, with densely populated LGAs in Melbourne. These get greatly enlarged by the algorithm, and LGA polygons from the rural areas are much smaller. It makes it easier to see the LGAs with high case counts, and also all of the LGAs in the city with low counts. (Note: the white inner city polygons are not actually LGAs, just unfortunate artifacts of the cartogram transformation.\n\n# Incorporate population data to make cartogram\n# Population from https://www.planning.vic.gov.au/land-use-and-population-research/victoria-in-future/tab-pages/victoria-in-future-data-tables\n# Data can be downloaded from https://github.com/numbats/eda/blob/master/data/VIF2019_Population_Service_Ages_LGA_2036.xlsx\npop &lt;- read_xlsx(\"../data/VIF2019_Population_Service_Ages_LGA_2036.xlsx\", sheet=3, skip=13, col_names = FALSE) |&gt;\n  select(`...4`, `...22`) |&gt;\n  rename(lga = `...4`, pop=`...22`) |&gt;\n  filter(lga != \"Unincorporated Vic\") |&gt; \n  mutate(lga = str_replace(lga, \" \\\\(.+\\\\)\", \"\")) |&gt;\n  mutate(lga = ifelse(lga == \"Colac-Otway\", \"Colac Otway\", lga)) \n\ncovid_lga &lt;- covid_lga |&gt;\n  left_join(pop) \n\n# Compute additional statistics\ncovid_lga &lt;- covid_lga |&gt;\n  mutate(cases_per10k = cases/pop*10000,\n         lcases = log10(cases + 1)) \n\n# Make a contiguous cartogram\n# The sf object is in lat/long (WGS84) which is \n# an angle on the globe, but the cartogram \n# needs spatial locations in metres/numeric \n# as given by EPSG:3395.\n# So we convert to metres and then back to \n# lat/long with st_transform\ncovid_carto &lt;- covid_lga |&gt; \n  st_transform(3395) |&gt; \n  cartogram_cont(\"pop\") |&gt;\n  st_transform(\"WGS84\") \n# The cartogram object contains a mix of MULTIPOLYGON\n# and POLYGON - yes, amazing! - st_cast() forces all \n# to be MULTIPOLYGON and is necessary for plotly \ncovid_carto &lt;- st_cast(covid_carto, \"MULTIPOLYGON\") \n# st_geometry() is a good function for checking\n# the projection (lat/long vs metres) and POLYGON\n\nggplot(covid_carto) + \n  geom_sf(aes(fill = cases, label=lga), colour=\"white\") + \n  scale_fill_distiller(\"Cases\", palette = \"YlOrRd\",\n                       direction=1) + \n  theme_map() +\n  theme(legend.position=\"bottom\") \n\n\n\n\n\n\n\n# ggplotly()\n\n\n\n\n\n\n\ng. Hexagon tile map\nUse the provided code to make a hexgon tile map, with functions from the sugarbag package. Is it easier to see the spatial distribution of incidence from the hexagon tile map, or the choropleth or the cartogram?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe hexagon tile map leaves the georgraphy more recognisable than the cartogram, and it is easier to see the high incidence LGAs, and that they are in the city.\n\n# Placement of hexmaps depends on position relative to\n# Melbourne central\ndata(capital_cities)\ncovid_hexmap &lt;- create_hexmap(\n  shp = covid_lga,\n  sf_id = \"lga\",\n  focal_points = capital_cities, verbose = TRUE)\n# This shows the centroids of the hexagons\nggplot(covid_hexmap, aes(x=hex_long, y=hex_lat)) +\n  geom_point()\n\n\n\n\n\n\n\n# Hexagons are made with the `fortify_hexagon` function\ncovid_hexmap_poly &lt;- covid_hexmap |&gt;\n  fortify_hexagon(sf_id = \"lga\", hex_size = 0.1869) |&gt;\n  left_join(covid_tot, by=\"lga\") # hexmap code removed cases!\nggplot() +\n  geom_sf(data=covid_lga, \n          fill = \"grey95\", colour = \"white\", linewidth=0.1) +\n  geom_polygon(data=covid_hexmap_poly, \n               aes(x=long, y=lat, group=hex_id, \n                   fill = cases, \n                   colour = cases,\n                   label=lga), size=0.2) +\n  scale_fill_distiller(\"Cases\", palette = \"YlOrRd\",\n                       direction=1) +\n  scale_colour_distiller(\"Cases\", palette = \"YlOrRd\",\n                       direction=1) +\n  theme_map() +\n  theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n# ggplotly()"
  },
  {
    "objectID": "week10/tutorialsol.html#finishing-up",
    "href": "week10/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 10",
    "section": "👌 Finishing up",
    "text": "👌 Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week10/worksheet.html",
    "href": "week10/worksheet.html",
    "title": "ETC5521 Worksheet Week 10",
    "section": "",
    "text": "Exercise 1: Gridded spatiotemporal data\nConduct a spatiotemporal analysis of ozone measurements over central America, following the analysis of temperature provided in the class lecture notes.\n\na. Make a single map\nLoad the nasa data from the GGally package, and make a map of ozone for January 2015, overlaid on a map of the geographic area. What do you learn about the spatial distribution of ozone?\n\n\nb. Display the map over time\nGenerate the maps of ozone for all of the time period, by facetting on month and year. Why was the plot organised so that months were in columns and years in rows, do you think? What do you learn about the temporal changes in the spatial distribution of ozone?\n\n\nc. Glyphmap\nMake two glyphmaps of ozone, one with time series at each spatial grid point, scaled globally, and the other using polar coordinates at each spatial grid point, scaled individually. What do you learn about the temporal trend, and seasonal patterns of ozone over this geographic region?"
  },
  {
    "objectID": "week10/worksheetsol.html",
    "href": "week10/worksheetsol.html",
    "title": "ETC5521 Worksheet Week 10",
    "section": "",
    "text": "Exercise 1: Gridded spatiotemporal data\nConduct a spatiotemporal analysis of ozone measurements over central America, following the analysis of temperature provided in the class lecture notes.\n\na. Make a single map\nLoad the nasa data from the GGally package, and make a map of ozone for January 2015, overlaid on a map of the geographic area. What do you learn about the spatial distribution of ozone?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe high concentrations of ozone are at the highest latitude. The lowest are close to the equator, and there is a small increase in values in the southern hemisphere. The trend is primarily north-south, and doesn’t change between land and sea.\n\n\nCode\ndata(nasa) # from GGally package\n\nnasa_cb &lt;- as_cubble(as_tibble(nasa), \n                     key=id, \n                     index=time, \n                     coords=c(long, lat))\n\nsth_america &lt;- map_data(\"world\") |&gt;\n  filter(between(long, -115, -53), between(lat, -20.5, 41))\nnasa_cb |&gt; face_temporal() |&gt;\n  filter(month == 1, year == 1995) |&gt; \n  select(id, time, ozone) |&gt;\n  unfold(long, lat) |&gt;\n  ggplot() + \n  geom_tile(aes(x=long, y=lat, fill=ozone)) +\n  geom_path(data=sth_america, \n            aes(x=long, y=lat, group=group), \n            colour=\"white\", linewidth=1) +\n  scale_fill_viridis_c(\"ozone\", option = \"magma\") +\n  theme_map() +\n  theme(aspect.ratio = 0.8, legend.position = \"bottom\") +\n  ggtitle(\"January 1995\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb. Display the map over time\nGenerate the maps of ozone for all of the time period, by facetting on month and year. Why was the plot organised so that months were in columns and years in rows, do you think? What do you learn about the temporal changes in the spatial distribution of ozone?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe primary comparison is same month each year, which we might expect to be fairly similar. Reading down columns is easier for making this comparison. Reading across the row, allows comparison of seasonal patterns within a year.\nThere is a small seasonal pattern, in that there is a decrease in values in the northern hemisphere in the late northern hemisphere summer (July, Aug, Sep). There is an increase during these months around the equator also. Because the latitude does not go as far south as north, we cannot see whether the ozone values are similarly high in the south as in the north, for corresponding distance from the equator. The pattern remains that it is mostly north-south trend rather than east-west trend or land-sea trend. There is not a lot of difference across years: perhaps slightly increased values extending further towards the equator from the northern latitudes in the summer months.\n\n\nCode\nnasa_cb |&gt; face_temporal() |&gt;\n  select(id, time, month, year, ozone) |&gt;\n  unfold(long, lat) |&gt;\n  ggplot() + \n  geom_tile(aes(x=long, y=lat, fill=ozone)) +\n  facet_grid(year~month) +\n  scale_fill_viridis_c(\"ozone\",\n                       option = \"magma\") +\n  #coord_map() +\n  theme_map() +\n  theme(aspect.ratio = 0.8, legend.position=\"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nc. Glyphmap\nMake two glyphmaps of ozone, one with time series at each spatial grid point, scaled globally, and the other using polar coordinates at each spatial grid point, scaled individually. What do you learn about the temporal trend, and seasonal patterns of ozone over this geographic region?\n\n\n\n\n\n\nSolutionSolution\n\n\n\n\n\n\nThe dominant pattern from the (time series) glyph maps of high values in the north, and decreasing values going south, and then a slight increase at the most southern values. There may be some seasonality because the series have the up-down pattern repeated 6 times for the 6 years, visible at all locations.\nIn the seasonal glyphmaps, the seasonality is the strongest pattern at all locations. Some years tend to have more ozone than others, because the 6 “petals” are different sizes in some locations. There is possibly a slight land-sea difference in seasonality. It’s also possible to see the shift in seasons, that the peaks in the south are at different times of the year than the peaks in the north.\n\n\nCode\nnasa_cb |&gt; face_temporal() |&gt;\n  select(id, time, month, year, ozone) |&gt;\n  unfold(long, lat) |&gt;\n  ggplot() +\n  geom_polygon(data=sth_america, \n            aes(x=long, y=lat, group=group), \n            fill=\"#014221\", alpha=0.5, colour=\"#ffffff\") +\n  cubble::geom_glyph_box(data=nasa, aes(x_major = long, x_minor = date,\n            y_major = lat, y_minor = ozone), fill=NA) +\n  cubble::geom_glyph(data=nasa, aes(x_major = long, x_minor = date,\n            y_major = lat, y_minor = ozone)) +\n  theme_map() +\n  theme(aspect.ratio = 0.8)\n\n\n\n\n\n\n\n\n\n\n\nCode\nnasa_cb |&gt; face_temporal() |&gt;\n  select(id, time, month, year, ozone) |&gt;\n  unfold(long, lat) |&gt;\n  ggplot() + \n  geom_polygon(data=sth_america, \n           aes(x=long, y=lat, group=group), \n          fill=\"#014221\", alpha=0.5, colour=\"#ffffff\") +\n  cubble::geom_glyph_box(data=nasa, \n                 aes(x_major = long, x_minor = date,\n            y_major = lat, y_minor = ozone), fill=NA) +\n  cubble::geom_glyph(data=nasa, aes(x_major = long, x_minor = date,\n            y_major = lat, y_minor = ozone), \n            polar = TRUE) +\n  theme_map() +\n  theme(aspect.ratio = 0.8)"
  }
]